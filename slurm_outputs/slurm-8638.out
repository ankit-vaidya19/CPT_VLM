[2024-07-10 10:40:51,471] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-10 10:40:54,339] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1: setting --include=localhost:0,1
[2024-07-10 10:40:54,339] [INFO] [runner.py:571:main] cmd = /home/avaidya7/.conda/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /home/avaidya7/CPT_VLM/llava/train/train_mem.py --lora_enable True --nola_enable False --nola_num_basis 512 --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed /home/avaidya7/CPT_VLM/scripts/zero3.json --model_name_or_path /home/avaidya7/checkpoints/llava-v1.5-7b-tv100_lora_merged --version v1 --data_path /home/avaidya7/FloodNet_train.json --image_folder /home/avaidya7/Sat_data/FloodNet/Images/Train_Image --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /home/avaidya7/checkpoints/llava-v1.5-7b-c_lora_floodnet --num_train_epochs 1 --per_device_train_batch_size 4 --per_device_eval_batch_size 2 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name llava1.5_c_lora_floodnet
[2024-07-10 10:40:56,555] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-10 10:40:57,964] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2024-07-10 10:40:57,964] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-07-10 10:40:57,964] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-07-10 10:40:57,964] [INFO] [launch.py:163:main] dist_world_size=2
[2024-07-10 10:40:57,964] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2024-07-10 10:41:02,335] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-10 10:41:02,335] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-10 10:41:04,892] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-10 10:41:04,892] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-07-10 10:41:04,892] [INFO] [comm.py:637:init_distributed] cdb=None
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2024-07-10 10:41:07,084] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 295, num_elems = 6.76B
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  3.55it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:12,  6.43s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:12<00:07,  7.16s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:13<00:06,  6.62s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  6.15s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.74s/it]
Some weights of the model checkpoint at /home/avaidya7/checkpoints/llava-v1.5-7b-tv100_lora_merged were not used when initializing LlavaLlamaForCausalLM: ['model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight']
- This IS expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.77s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.98s/it]
Some weights of the model checkpoint at /home/avaidya7/checkpoints/llava-v1.5-7b-tv100_lora_merged were not used when initializing LlavaLlamaForCausalLM: ['model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight']
- This IS expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Adding LoRA adapters...
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[2024-07-10 10:43:36,456] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 686, num_elems = 7.06B
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
Formatting inputs...Skip in lazy mode
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Parameter Offload: Total persistent parameters: 599040 in 312 params
wandb: Currently logged in as: ankxanity19. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/avaidya7/CPT_VLM/wandb/run-20240710_104345-3nriek0x
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llava1.5_c_lora_floodnet
wandb: ⭐️ View project at https://wandb.ai/ankxanity19/CPT_VLM
wandb: 🚀 View run at https://wandb.ai/ankxanity19/CPT_VLM/runs/3nriek0x
  0%|          | 0/395 [00:00<?, ?it/s]/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  0%|          | 1/395 [00:24<2:42:23, 24.73s/it]                                                 {'loss': 11.8219, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.0}
  0%|          | 1/395 [00:24<2:42:23, 24.73s/it]  1%|          | 2/395 [00:33<1:38:44, 15.07s/it]                                                 {'loss': 10.7612, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.01}
  1%|          | 2/395 [00:33<1:38:44, 15.07s/it]  1%|          | 3/395 [00:36<1:03:40,  9.75s/it]                                                 {'loss': 8.2685, 'learning_rate': 5e-05, 'epoch': 0.01}
  1%|          | 3/395 [00:36<1:03:40,  9.75s/it]  1%|          | 4/395 [00:40<47:47,  7.33s/it]                                                 {'loss': 9.288, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.01}
  1%|          | 4/395 [00:40<47:47,  7.33s/it]  1%|▏         | 5/395 [00:43<39:07,  6.02s/it]                                               {'loss': 8.4192, 'learning_rate': 8.333333333333334e-05, 'epoch': 0.01}
  1%|▏         | 5/395 [00:43<39:07,  6.02s/it]  2%|▏         | 6/395 [00:47<33:46,  5.21s/it]                                               {'loss': 9.5883, 'learning_rate': 0.0001, 'epoch': 0.02}
  2%|▏         | 6/395 [00:47<33:46,  5.21s/it]  2%|▏         | 7/395 [00:51<30:21,  4.69s/it]                                               {'loss': 11.0082, 'learning_rate': 0.00011666666666666668, 'epoch': 0.02}
  2%|▏         | 7/395 [00:51<30:21,  4.69s/it]  2%|▏         | 8/395 [00:54<27:57,  4.33s/it]                                               {'loss': 8.3751, 'learning_rate': 0.00013333333333333334, 'epoch': 0.02}
  2%|▏         | 8/395 [00:54<27:57,  4.33s/it]  2%|▏         | 9/395 [00:58<26:47,  4.16s/it]                                               {'loss': 8.2921, 'learning_rate': 0.00015000000000000001, 'epoch': 0.02}
  2%|▏         | 9/395 [00:58<26:47,  4.16s/it]  3%|▎         | 10/395 [01:02<25:37,  3.99s/it]                                                {'loss': 8.1673, 'learning_rate': 0.0001666666666666667, 'epoch': 0.03}
  3%|▎         | 10/395 [01:02<25:37,  3.99s/it]  3%|▎         | 11/395 [01:05<24:59,  3.90s/it]                                                {'loss': 7.9471, 'learning_rate': 0.00018333333333333334, 'epoch': 0.03}
  3%|▎         | 11/395 [01:05<24:59,  3.90s/it]  3%|▎         | 12/395 [01:09<24:31,  3.84s/it]                                                {'loss': 5.4899, 'learning_rate': 0.0002, 'epoch': 0.03}
  3%|▎         | 12/395 [01:09<24:31,  3.84s/it]  3%|▎         | 13/395 [01:13<24:03,  3.78s/it]                                                {'loss': 5.6481, 'learning_rate': 0.0001999966358932628, 'epoch': 0.03}
  3%|▎         | 13/395 [01:13<24:03,  3.78s/it]  4%|▎         | 14/395 [01:16<23:51,  3.76s/it]                                                {'loss': 3.6699, 'learning_rate': 0.00019998654379939533, 'epoch': 0.04}
  4%|▎         | 14/395 [01:16<23:51,  3.76s/it]  4%|▍         | 15/395 [01:20<23:23,  3.69s/it]                                                {'loss': 5.3458, 'learning_rate': 0.00019996972439741538, 'epoch': 0.04}
  4%|▍         | 15/395 [01:20<23:23,  3.69s/it]  4%|▍         | 16/395 [01:23<23:15,  3.68s/it]                                                {'loss': 4.5808, 'learning_rate': 0.0001999461788189681, 'epoch': 0.04}
  4%|▍         | 16/395 [01:23<23:15,  3.68s/it]  4%|▍         | 17/395 [01:27<22:55,  3.64s/it]                                                {'loss': 3.5382, 'learning_rate': 0.00019991590864825028, 'epoch': 0.04}
  4%|▍         | 17/395 [01:27<22:55,  3.64s/it]  5%|▍         | 18/395 [01:31<22:56,  3.65s/it]                                                {'loss': 4.0779, 'learning_rate': 0.00019987891592190366, 'epoch': 0.05}
  5%|▍         | 18/395 [01:31<22:56,  3.65s/it]  5%|▍         | 19/395 [01:34<22:39,  3.62s/it]                                                {'loss': 2.8535, 'learning_rate': 0.00019983520312887785, 'epoch': 0.05}
  5%|▍         | 19/395 [01:34<22:39,  3.62s/it]  5%|▌         | 20/395 [01:38<22:24,  3.58s/it]                                                {'loss': 3.1114, 'learning_rate': 0.00019978477321026282, 'epoch': 0.05}
  5%|▌         | 20/395 [01:38<22:24,  3.58s/it]  5%|▌         | 21/395 [01:41<22:12,  3.56s/it]                                                {'loss': 3.0033, 'learning_rate': 0.0001997276295590912, 'epoch': 0.05}
  5%|▌         | 21/395 [01:41<22:12,  3.56s/it]  6%|▌         | 22/395 [01:45<22:26,  3.61s/it]                                                {'loss': 1.3942, 'learning_rate': 0.00019966377602010984, 'epoch': 0.06}
  6%|▌         | 22/395 [01:45<22:26,  3.61s/it]  6%|▌         | 23/395 [01:49<22:29,  3.63s/it]                                                {'loss': 1.8077, 'learning_rate': 0.0001995932168895211, 'epoch': 0.06}
  6%|▌         | 23/395 [01:49<22:29,  3.63s/it]  6%|▌         | 24/395 [01:52<22:20,  3.61s/it]                                                {'loss': 2.7662, 'learning_rate': 0.00019951595691469396, 'epoch': 0.06}
  6%|▌         | 24/395 [01:52<22:20,  3.61s/it]  6%|▋         | 25/395 [01:56<22:16,  3.61s/it]                                                {'loss': 3.349, 'learning_rate': 0.00019943200129384444, 'epoch': 0.06}
  6%|▋         | 25/395 [01:56<22:16,  3.61s/it]  7%|▋         | 26/395 [02:00<22:28,  3.65s/it]                                                {'loss': 1.2451, 'learning_rate': 0.0001993413556756859, 'epoch': 0.07}
  7%|▋         | 26/395 [02:00<22:28,  3.65s/it]  7%|▋         | 27/395 [02:03<22:21,  3.64s/it]                                                {'loss': 2.0477, 'learning_rate': 0.0001992440261590491, 'epoch': 0.07}
  7%|▋         | 27/395 [02:03<22:21,  3.64s/it]  7%|▋         | 28/395 [02:07<22:16,  3.64s/it]                                                {'loss': 3.7066, 'learning_rate': 0.00019914001929247167, 'epoch': 0.07}
  7%|▋         | 28/395 [02:07<22:16,  3.64s/it]  7%|▋         | 29/395 [02:10<22:07,  3.63s/it]                                                {'loss': 1.1201, 'learning_rate': 0.0001990293420737576, 'epoch': 0.07}
  7%|▋         | 29/395 [02:10<22:07,  3.63s/it]  8%|▊         | 30/395 [02:14<22:01,  3.62s/it]                                                {'loss': 1.91, 'learning_rate': 0.00019891200194950643, 'epoch': 0.08}
  8%|▊         | 30/395 [02:14<22:01,  3.62s/it]  8%|▊         | 31/395 [02:18<21:53,  3.61s/it]                                                {'loss': 1.5611, 'learning_rate': 0.00019878800681461222, 'epoch': 0.08}
  8%|▊         | 31/395 [02:18<21:53,  3.61s/it]  8%|▊         | 32/395 [02:21<21:43,  3.59s/it]                                                {'loss': 1.7951, 'learning_rate': 0.00019865736501173238, 'epoch': 0.08}
  8%|▊         | 32/395 [02:21<21:43,  3.59s/it]  8%|▊         | 33/395 [02:24<21:09,  3.51s/it]                                                {'loss': 1.5037, 'learning_rate': 0.00019852008533072625, 'epoch': 0.08}
  8%|▊         | 33/395 [02:24<21:09,  3.51s/it]  9%|▊         | 34/395 [02:28<21:00,  3.49s/it]                                                {'loss': 1.3325, 'learning_rate': 0.00019837617700806383, 'epoch': 0.09}
  9%|▊         | 34/395 [02:28<21:00,  3.49s/it]  9%|▉         | 35/395 [02:32<21:20,  3.56s/it]                                                {'loss': 0.8309, 'learning_rate': 0.00019822564972620427, 'epoch': 0.09}
  9%|▉         | 35/395 [02:32<21:20,  3.56s/it]  9%|▉         | 36/395 [02:35<21:30,  3.59s/it]                                                {'loss': 1.0501, 'learning_rate': 0.0001980685136129445, 'epoch': 0.09}
  9%|▉         | 36/395 [02:35<21:30,  3.59s/it]  9%|▉         | 37/395 [02:39<21:24,  3.59s/it]                                                {'loss': 0.8528, 'learning_rate': 0.00019790477924073755, 'epoch': 0.09}
  9%|▉         | 37/395 [02:39<21:24,  3.59s/it] 10%|▉         | 38/395 [02:42<21:23,  3.59s/it]                                                {'loss': 1.1678, 'learning_rate': 0.00019773445762598163, 'epoch': 0.1}
 10%|▉         | 38/395 [02:42<21:23,  3.59s/it] 10%|▉         | 39/395 [02:46<21:21,  3.60s/it]                                                {'loss': 0.9132, 'learning_rate': 0.00019755756022827846, 'epoch': 0.1}
 10%|▉         | 39/395 [02:46<21:21,  3.60s/it] 10%|█         | 40/395 [02:50<21:25,  3.62s/it]                                                {'loss': 1.7274, 'learning_rate': 0.00019737409894966267, 'epoch': 0.1}
 10%|█         | 40/395 [02:50<21:25,  3.62s/it] 10%|█         | 41/395 [02:53<21:17,  3.61s/it]                                                {'loss': 0.7672, 'learning_rate': 0.00019718408613380074, 'epoch': 0.1}
 10%|█         | 41/395 [02:53<21:17,  3.61s/it] 11%|█         | 42/395 [02:57<21:10,  3.60s/it]                                                {'loss': 0.8806, 'learning_rate': 0.00019698753456516048, 'epoch': 0.11}
 11%|█         | 42/395 [02:57<21:10,  3.60s/it] 11%|█         | 43/395 [03:01<21:27,  3.66s/it]                                                {'loss': 1.2192, 'learning_rate': 0.00019678445746815107, 'epoch': 0.11}
 11%|█         | 43/395 [03:01<21:27,  3.66s/it] 11%|█         | 44/395 [03:04<21:15,  3.64s/it]                                                {'loss': 0.7694, 'learning_rate': 0.00019657486850623306, 'epoch': 0.11}
 11%|█         | 44/395 [03:04<21:15,  3.64s/it] 11%|█▏        | 45/395 [03:08<21:08,  3.63s/it]                                                {'loss': 0.903, 'learning_rate': 0.00019635878178099928, 'epoch': 0.11}
 11%|█▏        | 45/395 [03:08<21:08,  3.63s/it] 12%|█▏        | 46/395 [03:11<21:02,  3.62s/it]                                                {'loss': 1.0106, 'learning_rate': 0.0001961362118312259, 'epoch': 0.12}
 12%|█▏        | 46/395 [03:11<21:02,  3.62s/it] 12%|█▏        | 47/395 [03:15<21:03,  3.63s/it]                                                {'loss': 0.42, 'learning_rate': 0.0001959071736318942, 'epoch': 0.12}
 12%|█▏        | 47/395 [03:15<21:03,  3.63s/it] 12%|█▏        | 48/395 [03:19<20:59,  3.63s/it]                                                {'loss': 0.7029, 'learning_rate': 0.00019567168259318325, 'epoch': 0.12}
 12%|█▏        | 48/395 [03:19<20:59,  3.63s/it] 12%|█▏        | 49/395 [03:22<20:47,  3.61s/it]                                                {'loss': 1.0346, 'learning_rate': 0.00019542975455943281, 'epoch': 0.12}
 12%|█▏        | 49/395 [03:22<20:47,  3.61s/it] 13%|█▎        | 50/395 [03:26<20:45,  3.61s/it]                                                {'loss': 1.2472, 'learning_rate': 0.00019518140580807744, 'epoch': 0.13}
 13%|█▎        | 50/395 [03:26<20:45,  3.61s/it] 13%|█▎        | 51/395 [03:30<20:53,  3.64s/it]                                                {'loss': 0.5392, 'learning_rate': 0.00019492665304855132, 'epoch': 0.13}
 13%|█▎        | 51/395 [03:30<20:53,  3.64s/it] 13%|█▎        | 52/395 [03:33<20:56,  3.66s/it]                                                {'loss': 0.7421, 'learning_rate': 0.0001946655134211639, 'epoch': 0.13}
 13%|█▎        | 52/395 [03:33<20:56,  3.66s/it] 13%|█▎        | 53/395 [03:37<20:51,  3.66s/it]                                                {'loss': 0.6913, 'learning_rate': 0.0001943980044959468, 'epoch': 0.13}
 13%|█▎        | 53/395 [03:37<20:51,  3.66s/it] 14%|█▎        | 54/395 [03:41<20:48,  3.66s/it]                                                {'loss': 0.6727, 'learning_rate': 0.0001941241442714716, 'epoch': 0.14}
 14%|█▎        | 54/395 [03:41<20:48,  3.66s/it] 14%|█▍        | 55/395 [03:44<20:42,  3.65s/it]                                                {'loss': 0.6894, 'learning_rate': 0.0001938439511736388, 'epoch': 0.14}
 14%|█▍        | 55/395 [03:44<20:42,  3.65s/it] 14%|█▍        | 56/395 [03:48<20:07,  3.56s/it]                                                {'loss': 0.6839, 'learning_rate': 0.0001935574440544381, 'epoch': 0.14}
 14%|█▍        | 56/395 [03:48<20:07,  3.56s/it] 14%|█▍        | 57/395 [03:51<19:58,  3.55s/it]                                                {'loss': 0.7302, 'learning_rate': 0.0001932646421906802, 'epoch': 0.14}
 14%|█▍        | 57/395 [03:51<19:58,  3.55s/it] 15%|█▍        | 58/395 [03:55<19:58,  3.56s/it]                                                {'loss': 0.5345, 'learning_rate': 0.00019296556528269952, 'epoch': 0.15}
 15%|█▍        | 58/395 [03:55<19:58,  3.56s/it] 15%|█▍        | 59/395 [03:58<19:57,  3.56s/it]                                                {'loss': 0.5577, 'learning_rate': 0.00019266023345302887, 'epoch': 0.15}
 15%|█▍        | 59/395 [03:58<19:57,  3.56s/it] 15%|█▌        | 60/395 [04:02<19:34,  3.51s/it]                                                {'loss': 0.5822, 'learning_rate': 0.00019234866724504555, 'epoch': 0.15}
 15%|█▌        | 60/395 [04:02<19:34,  3.51s/it] 15%|█▌        | 61/395 [04:06<19:57,  3.59s/it]                                                {'loss': 0.7653, 'learning_rate': 0.00019203088762158915, 'epoch': 0.15}
 15%|█▌        | 61/395 [04:06<19:57,  3.59s/it] 16%|█▌        | 62/395 [04:09<19:52,  3.58s/it]                                                {'loss': 0.6279, 'learning_rate': 0.00019170691596355114, 'epoch': 0.16}
 16%|█▌        | 62/395 [04:09<19:52,  3.58s/it] 16%|█▌        | 63/395 [04:13<19:52,  3.59s/it]                                                {'loss': 0.3631, 'learning_rate': 0.00019137677406843619, 'epoch': 0.16}
 16%|█▌        | 63/395 [04:13<19:52,  3.59s/it] 16%|█▌        | 64/395 [04:16<19:42,  3.57s/it]                                                {'loss': 1.0247, 'learning_rate': 0.00019104048414889587, 'epoch': 0.16}
 16%|█▌        | 64/395 [04:16<19:42,  3.57s/it] 16%|█▋        | 65/395 [04:20<19:42,  3.58s/it]                                                {'loss': 0.7475, 'learning_rate': 0.00019069806883123387, 'epoch': 0.16}
 16%|█▋        | 65/395 [04:20<19:42,  3.58s/it] 17%|█▋        | 66/395 [04:23<19:35,  3.57s/it]                                                {'loss': 0.5643, 'learning_rate': 0.00019034955115388363, 'epoch': 0.17}
 17%|█▋        | 66/395 [04:23<19:35,  3.57s/it] 17%|█▋        | 67/395 [04:27<19:26,  3.56s/it]                                                {'loss': 0.5252, 'learning_rate': 0.00018999495456585854, 'epoch': 0.17}
 17%|█▋        | 67/395 [04:27<19:26,  3.56s/it] 17%|█▋        | 68/395 [04:31<19:30,  3.58s/it]                                                {'loss': 0.5102, 'learning_rate': 0.00018963430292517398, 'epoch': 0.17}
 17%|█▋        | 68/395 [04:31<19:30,  3.58s/it] 17%|█▋        | 69/395 [04:34<19:52,  3.66s/it]                                                {'loss': 0.3781, 'learning_rate': 0.00018926762049724228, 'epoch': 0.17}
 17%|█▋        | 69/395 [04:34<19:52,  3.66s/it] 18%|█▊        | 70/395 [04:38<19:42,  3.64s/it]                                                {'loss': 0.8887, 'learning_rate': 0.00018889493195323997, 'epoch': 0.18}
 18%|█▊        | 70/395 [04:38<19:42,  3.64s/it] 18%|█▊        | 71/395 [04:42<19:36,  3.63s/it]                                                {'loss': 0.6992, 'learning_rate': 0.00018851626236844786, 'epoch': 0.18}
 18%|█▊        | 71/395 [04:42<19:36,  3.63s/it] 18%|█▊        | 72/395 [04:45<19:21,  3.60s/it]                                                {'loss': 0.746, 'learning_rate': 0.00018813163722056396, 'epoch': 0.18}
 18%|█▊        | 72/395 [04:45<19:21,  3.60s/it] 18%|█▊        | 73/395 [04:49<19:19,  3.60s/it]                                                {'loss': 0.1835, 'learning_rate': 0.0001877410823879893, 'epoch': 0.18}
 18%|█▊        | 73/395 [04:49<19:19,  3.60s/it] 19%|█▊        | 74/395 [04:52<19:16,  3.60s/it]                                                {'loss': 0.275, 'learning_rate': 0.0001873446241480868, 'epoch': 0.19}
 19%|█▊        | 74/395 [04:52<19:16,  3.60s/it] 19%|█▉        | 75/395 [04:56<19:07,  3.59s/it]                                                {'loss': 0.4543, 'learning_rate': 0.00018694228917541313, 'epoch': 0.19}
 19%|█▉        | 75/395 [04:56<19:07,  3.59s/it] 19%|█▉        | 76/395 [04:59<19:00,  3.58s/it]                                                {'loss': 0.5234, 'learning_rate': 0.00018653410453992413, 'epoch': 0.19}
 19%|█▉        | 76/395 [04:59<19:00,  3.58s/it] 19%|█▉        | 77/395 [05:03<18:52,  3.56s/it]                                                {'loss': 0.8509, 'learning_rate': 0.0001861200977051535, 'epoch': 0.19}
 19%|█▉        | 77/395 [05:03<18:52,  3.56s/it] 20%|█▉        | 78/395 [05:07<19:03,  3.61s/it]                                                {'loss': 0.5076, 'learning_rate': 0.0001857002965263648, 'epoch': 0.2}
 20%|█▉        | 78/395 [05:07<19:03,  3.61s/it] 20%|██        | 79/395 [05:10<18:52,  3.59s/it]                                                {'loss': 0.6023, 'learning_rate': 0.00018527472924867756, 'epoch': 0.2}
 20%|██        | 79/395 [05:10<18:52,  3.59s/it] 20%|██        | 80/395 [05:14<18:35,  3.54s/it]                                                {'loss': 0.4354, 'learning_rate': 0.00018484342450516671, 'epoch': 0.2}
 20%|██        | 80/395 [05:14<18:35,  3.54s/it] 21%|██        | 81/395 [05:17<18:28,  3.53s/it]                                                {'loss': 0.4711, 'learning_rate': 0.0001844064113149361, 'epoch': 0.21}
 21%|██        | 81/395 [05:17<18:28,  3.53s/it] 21%|██        | 82/395 [05:21<18:27,  3.54s/it]                                                {'loss': 0.5175, 'learning_rate': 0.0001839637190811661, 'epoch': 0.21}
 21%|██        | 82/395 [05:21<18:27,  3.54s/it] 21%|██        | 83/395 [05:24<18:26,  3.55s/it]                                                {'loss': 0.5292, 'learning_rate': 0.00018351537758913518, 'epoch': 0.21}
 21%|██        | 83/395 [05:24<18:26,  3.55s/it] 21%|██▏       | 84/395 [05:28<18:25,  3.56s/it]                                                {'loss': 0.3438, 'learning_rate': 0.00018306141700421606, 'epoch': 0.21}
 21%|██▏       | 84/395 [05:28<18:25,  3.56s/it] 22%|██▏       | 85/395 [05:31<18:20,  3.55s/it]                                                {'loss': 0.4629, 'learning_rate': 0.000182601867869846, 'epoch': 0.22}
 22%|██▏       | 85/395 [05:31<18:20,  3.55s/it] 22%|██▏       | 86/395 [05:35<18:08,  3.52s/it]                                                {'loss': 0.644, 'learning_rate': 0.00018213676110547176, 'epoch': 0.22}
 22%|██▏       | 86/395 [05:35<18:08,  3.52s/it] 22%|██▏       | 87/395 [05:39<18:21,  3.58s/it]                                                {'loss': 0.5784, 'learning_rate': 0.0001816661280044693, 'epoch': 0.22}
 22%|██▏       | 87/395 [05:39<18:21,  3.58s/it] 22%|██▏       | 88/395 [05:42<18:22,  3.59s/it]                                                {'loss': 0.3576, 'learning_rate': 0.00018119000023203837, 'epoch': 0.22}
 22%|██▏       | 88/395 [05:42<18:22,  3.59s/it] 23%|██▎       | 89/395 [05:46<18:18,  3.59s/it]                                                {'loss': 0.3411, 'learning_rate': 0.0001807084098230719, 'epoch': 0.23}
 23%|██▎       | 89/395 [05:46<18:18,  3.59s/it] 23%|██▎       | 90/395 [05:49<18:18,  3.60s/it]                                                {'loss': 0.4324, 'learning_rate': 0.0001802213891800007, 'epoch': 0.23}
 23%|██▎       | 90/395 [05:49<18:18,  3.60s/it] 23%|██▎       | 91/395 [05:53<18:15,  3.60s/it]                                                {'loss': 0.4926, 'learning_rate': 0.00017972897107061328, 'epoch': 0.23}
 23%|██▎       | 91/395 [05:53<18:15,  3.60s/it] 23%|██▎       | 92/395 [05:57<18:15,  3.62s/it]                                                {'loss': 0.3783, 'learning_rate': 0.00017923118862585123, 'epoch': 0.23}
 23%|██▎       | 92/395 [05:57<18:15,  3.62s/it] 24%|██▎       | 93/395 [06:00<18:14,  3.62s/it]                                                {'loss': 0.4879, 'learning_rate': 0.00017872807533758007, 'epoch': 0.24}
 24%|██▎       | 93/395 [06:00<18:14,  3.62s/it] 24%|██▍       | 94/395 [06:04<17:50,  3.56s/it]                                                {'loss': 0.7944, 'learning_rate': 0.00017821966505633587, 'epoch': 0.24}
 24%|██▍       | 94/395 [06:04<17:50,  3.56s/it] 24%|██▍       | 95/395 [06:07<18:00,  3.60s/it]                                                {'loss': 0.5304, 'learning_rate': 0.00017770599198904763, 'epoch': 0.24}
 24%|██▍       | 95/395 [06:07<18:00,  3.60s/it] 24%|██▍       | 96/395 [06:11<17:55,  3.60s/it]                                                {'loss': 0.3877, 'learning_rate': 0.00017718709069673594, 'epoch': 0.24}
 24%|██▍       | 96/395 [06:11<17:55,  3.60s/it] 25%|██▍       | 97/395 [06:15<18:02,  3.63s/it]                                                {'loss': 0.5999, 'learning_rate': 0.00017666299609218745, 'epoch': 0.25}
 25%|██▍       | 97/395 [06:15<18:02,  3.63s/it] 25%|██▍       | 98/395 [06:18<17:50,  3.60s/it]                                                {'loss': 0.5451, 'learning_rate': 0.00017613374343760594, 'epoch': 0.25}
 25%|██▍       | 98/395 [06:18<17:50,  3.60s/it] 25%|██▌       | 99/395 [06:22<17:39,  3.58s/it]                                                {'loss': 0.5307, 'learning_rate': 0.00017559936834223982, 'epoch': 0.25}
 25%|██▌       | 99/395 [06:22<17:39,  3.58s/it] 25%|██▌       | 100/395 [06:25<17:47,  3.62s/it]                                                 {'loss': 0.2985, 'learning_rate': 0.0001750599067599863, 'epoch': 0.25}
 25%|██▌       | 100/395 [06:25<17:47,  3.62s/it] 26%|██▌       | 101/395 [06:29<17:38,  3.60s/it]                                                 {'loss': 0.541, 'learning_rate': 0.00017451539498697225, 'epoch': 0.26}
 26%|██▌       | 101/395 [06:29<17:38,  3.60s/it] 26%|██▌       | 102/395 [06:33<17:33,  3.60s/it]                                                 {'loss': 0.5311, 'learning_rate': 0.0001739658696591121, 'epoch': 0.26}
 26%|██▌       | 102/395 [06:33<17:33,  3.60s/it] 26%|██▌       | 103/395 [06:36<17:25,  3.58s/it]                                                 {'loss': 0.4758, 'learning_rate': 0.00017341136774964307, 'epoch': 0.26}
 26%|██▌       | 103/395 [06:36<17:25,  3.58s/it] 26%|██▋       | 104/395 [06:40<17:40,  3.64s/it]                                                 {'loss': 0.617, 'learning_rate': 0.0001728519265666373, 'epoch': 0.26}
 26%|██▋       | 104/395 [06:40<17:40,  3.64s/it] 27%|██▋       | 105/395 [06:44<17:35,  3.64s/it]                                                 {'loss': 0.3703, 'learning_rate': 0.00017228758375049185, 'epoch': 0.27}
 27%|██▋       | 105/395 [06:44<17:35,  3.64s/it] 27%|██▋       | 106/395 [06:47<17:30,  3.63s/it]                                                 {'loss': 0.6463, 'learning_rate': 0.00017171837727139613, 'epoch': 0.27}
 27%|██▋       | 106/395 [06:47<17:30,  3.63s/it] 27%|██▋       | 107/395 [06:51<17:33,  3.66s/it]                                                 {'loss': 0.331, 'learning_rate': 0.0001711443454267772, 'epoch': 0.27}
 27%|██▋       | 107/395 [06:51<17:33,  3.66s/it] 27%|██▋       | 108/395 [06:54<17:22,  3.63s/it]                                                 {'loss': 0.8073, 'learning_rate': 0.00017056552683872292, 'epoch': 0.27}
 27%|██▋       | 108/395 [06:54<17:22,  3.63s/it] 28%|██▊       | 109/395 [06:58<17:12,  3.61s/it]                                                 {'loss': 0.5418, 'learning_rate': 0.00016998196045138353, 'epoch': 0.28}
 28%|██▊       | 109/395 [06:58<17:12,  3.61s/it] 28%|██▊       | 110/395 [07:02<17:07,  3.61s/it]                                                 {'loss': 0.508, 'learning_rate': 0.00016939368552835137, 'epoch': 0.28}
 28%|██▊       | 110/395 [07:02<17:07,  3.61s/it] 28%|██▊       | 111/395 [07:05<17:01,  3.60s/it]                                                 {'loss': 0.5849, 'learning_rate': 0.00016880074165001905, 'epoch': 0.28}
 28%|██▊       | 111/395 [07:05<17:01,  3.60s/it] 28%|██▊       | 112/395 [07:09<17:15,  3.66s/it]                                                 {'loss': 0.3169, 'learning_rate': 0.0001682031687109165, 'epoch': 0.28}
 28%|██▊       | 112/395 [07:09<17:15,  3.66s/it] 29%|██▊       | 113/395 [07:13<17:17,  3.68s/it]                                                 {'loss': 0.4936, 'learning_rate': 0.00016760100691702674, 'epoch': 0.29}
 29%|██▊       | 113/395 [07:13<17:17,  3.68s/it] 29%|██▉       | 114/395 [07:16<16:42,  3.57s/it]                                                 {'loss': 0.2307, 'learning_rate': 0.0001669942967830807, 'epoch': 0.29}
 29%|██▉       | 114/395 [07:16<16:42,  3.57s/it] 29%|██▉       | 115/395 [07:20<16:34,  3.55s/it]                                                 {'loss': 0.5581, 'learning_rate': 0.00016638307912983136, 'epoch': 0.29}
 29%|██▉       | 115/395 [07:20<16:34,  3.55s/it] 29%|██▉       | 116/395 [07:23<16:37,  3.57s/it]                                                 {'loss': 0.266, 'learning_rate': 0.00016576739508130726, 'epoch': 0.29}
 29%|██▉       | 116/395 [07:23<16:37,  3.57s/it] 30%|██▉       | 117/395 [07:27<16:34,  3.58s/it]                                                 {'loss': 0.3488, 'learning_rate': 0.0001651472860620455, 'epoch': 0.3}
 30%|██▉       | 117/395 [07:27<16:34,  3.58s/it] 30%|██▉       | 118/395 [07:30<16:32,  3.58s/it]                                                 {'loss': 0.3474, 'learning_rate': 0.00016452279379430463, 'epoch': 0.3}
 30%|██▉       | 118/395 [07:30<16:32,  3.58s/it] 30%|███       | 119/395 [07:34<16:29,  3.58s/it]                                                 {'loss': 1.0212, 'learning_rate': 0.00016389396029525762, 'epoch': 0.3}
 30%|███       | 119/395 [07:34<16:29,  3.58s/it] 30%|███       | 120/395 [07:37<16:22,  3.57s/it]                                                 {'loss': 0.5644, 'learning_rate': 0.0001632608278741646, 'epoch': 0.3}
 30%|███       | 120/395 [07:37<16:22,  3.57s/it] 31%|███       | 121/395 [07:41<16:27,  3.60s/it]                                                 {'loss': 0.4574, 'learning_rate': 0.00016262343912952656, 'epoch': 0.31}
 31%|███       | 121/395 [07:41<16:27,  3.60s/it] 31%|███       | 122/395 [07:45<16:19,  3.59s/it]                                                 {'loss': 0.3473, 'learning_rate': 0.0001619818369462188, 'epoch': 0.31}
 31%|███       | 122/395 [07:45<16:19,  3.59s/it] 31%|███       | 123/395 [07:48<16:20,  3.60s/it]                                                 {'loss': 0.5637, 'learning_rate': 0.0001613360644926059, 'epoch': 0.31}
 31%|███       | 123/395 [07:48<16:20,  3.60s/it] 31%|███▏      | 124/395 [07:52<16:23,  3.63s/it]                                                 {'loss': 0.3429, 'learning_rate': 0.00016068616521763707, 'epoch': 0.31}
 31%|███▏      | 124/395 [07:52<16:23,  3.63s/it] 32%|███▏      | 125/395 [07:56<16:15,  3.61s/it]                                                 {'loss': 0.4388, 'learning_rate': 0.00016003218284792298, 'epoch': 0.32}
 32%|███▏      | 125/395 [07:56<16:15,  3.61s/it] 32%|███▏      | 126/395 [07:59<16:11,  3.61s/it]                                                 {'loss': 0.1369, 'learning_rate': 0.00015937416138479344, 'epoch': 0.32}
 32%|███▏      | 126/395 [07:59<16:11,  3.61s/it] 32%|███▏      | 127/395 [08:03<16:08,  3.62s/it]                                                 {'loss': 0.4479, 'learning_rate': 0.0001587121451013373, 'epoch': 0.32}
 32%|███▏      | 127/395 [08:03<16:08,  3.62s/it] 32%|███▏      | 128/395 [08:06<16:05,  3.62s/it]                                                 {'loss': 0.2867, 'learning_rate': 0.0001580461785394233, 'epoch': 0.32}
 32%|███▏      | 128/395 [08:06<16:05,  3.62s/it] 33%|███▎      | 129/395 [08:10<16:21,  3.69s/it]                                                 {'loss': 0.082, 'learning_rate': 0.00015737630650670335, 'epoch': 0.33}
 33%|███▎      | 129/395 [08:10<16:21,  3.69s/it] 33%|███▎      | 130/395 [08:14<16:14,  3.68s/it]                                                 {'loss': 0.7197, 'learning_rate': 0.00015670257407359792, 'epoch': 0.33}
 33%|███▎      | 130/395 [08:14<16:14,  3.68s/it] 33%|███▎      | 131/395 [08:18<16:00,  3.64s/it]                                                 {'loss': 0.4076, 'learning_rate': 0.00015602502657026328, 'epoch': 0.33}
 33%|███▎      | 131/395 [08:18<16:00,  3.64s/it] 33%|███▎      | 132/395 [08:21<15:50,  3.61s/it]                                                 {'loss': 0.4633, 'learning_rate': 0.00015534370958354186, 'epoch': 0.33}
 33%|███▎      | 132/395 [08:21<15:50,  3.61s/it] 34%|███▎      | 133/395 [08:25<15:34,  3.57s/it]                                                 {'loss': 0.6093, 'learning_rate': 0.00015465866895389495, 'epoch': 0.34}
 34%|███▎      | 133/395 [08:25<15:34,  3.57s/it] 34%|███▍      | 134/395 [08:28<15:30,  3.57s/it]                                                 {'loss': 0.2022, 'learning_rate': 0.00015396995077231854, 'epoch': 0.34}
 34%|███▍      | 134/395 [08:28<15:30,  3.57s/it] 34%|███▍      | 135/395 [08:32<15:27,  3.57s/it]                                                 {'loss': 0.8531, 'learning_rate': 0.00015327760137724212, 'epoch': 0.34}
 34%|███▍      | 135/395 [08:32<15:27,  3.57s/it] 34%|███▍      | 136/395 [08:35<15:33,  3.60s/it]                                                 {'loss': 0.5158, 'learning_rate': 0.00015258166735141092, 'epoch': 0.34}
 34%|███▍      | 136/395 [08:35<15:33,  3.60s/it] 35%|███▍      | 137/395 [08:39<15:30,  3.61s/it]                                                 {'loss': 0.5014, 'learning_rate': 0.0001518821955187519, 'epoch': 0.35}
 35%|███▍      | 137/395 [08:39<15:30,  3.61s/it] 35%|███▍      | 138/395 [08:43<15:32,  3.63s/it]                                                 {'loss': 0.5741, 'learning_rate': 0.00015117923294122312, 'epoch': 0.35}
 35%|███▍      | 138/395 [08:43<15:32,  3.63s/it] 35%|███▌      | 139/395 [08:46<15:24,  3.61s/it]                                                 {'loss': 0.7654, 'learning_rate': 0.0001504728269156475, 'epoch': 0.35}
 35%|███▌      | 139/395 [08:46<15:24,  3.61s/it] 35%|███▌      | 140/395 [08:50<15:08,  3.56s/it]                                                 {'loss': 0.2445, 'learning_rate': 0.00014976302497053036, 'epoch': 0.35}
 35%|███▌      | 140/395 [08:50<15:08,  3.56s/it] 36%|███▌      | 141/395 [08:53<15:15,  3.61s/it]                                                 {'loss': 0.4802, 'learning_rate': 0.00014904987486286184, 'epoch': 0.36}
 36%|███▌      | 141/395 [08:53<15:15,  3.61s/it] 36%|███▌      | 142/395 [08:57<15:00,  3.56s/it]                                                 {'loss': 0.5103, 'learning_rate': 0.00014833342457490361, 'epoch': 0.36}
 36%|███▌      | 142/395 [08:57<15:00,  3.56s/it] 36%|███▌      | 143/395 [09:00<14:58,  3.57s/it]                                                 {'loss': 0.2423, 'learning_rate': 0.00014761372231096047, 'epoch': 0.36}
 36%|███▌      | 143/395 [09:00<14:58,  3.57s/it] 36%|███▋      | 144/395 [09:04<14:52,  3.56s/it]                                                 {'loss': 0.1981, 'learning_rate': 0.00014689081649413708, 'epoch': 0.36}
 36%|███▋      | 144/395 [09:04<14:52,  3.56s/it] 37%|███▋      | 145/395 [09:07<14:48,  3.55s/it]                                                 {'loss': 0.4968, 'learning_rate': 0.00014616475576308005, 'epoch': 0.37}
 37%|███▋      | 145/395 [09:07<14:48,  3.55s/it] 37%|███▋      | 146/395 [09:11<14:48,  3.57s/it]                                                 {'loss': 0.453, 'learning_rate': 0.00014543558896870531, 'epoch': 0.37}
 37%|███▋      | 146/395 [09:11<14:48,  3.57s/it] 37%|███▋      | 147/395 [09:15<14:55,  3.61s/it]                                                 {'loss': 0.3404, 'learning_rate': 0.0001447033651709114, 'epoch': 0.37}
 37%|███▋      | 147/395 [09:15<14:55,  3.61s/it] 37%|███▋      | 148/395 [09:18<14:49,  3.60s/it]                                                 {'loss': 0.3024, 'learning_rate': 0.0001439681336352785, 'epoch': 0.37}
 37%|███▋      | 148/395 [09:18<14:49,  3.60s/it] 38%|███▊      | 149/395 [09:22<14:40,  3.58s/it]                                                 {'loss': 0.2747, 'learning_rate': 0.00014322994382975386, 'epoch': 0.38}
 38%|███▊      | 149/395 [09:22<14:40,  3.58s/it] 38%|███▊      | 150/395 [09:26<14:42,  3.60s/it]                                                 {'loss': 2.7422, 'learning_rate': 0.0001424888454213235, 'epoch': 0.38}
 38%|███▊      | 150/395 [09:26<14:42,  3.60s/it] 38%|███▊      | 151/395 [09:29<14:36,  3.59s/it]                                                 {'loss': 0.5433, 'learning_rate': 0.0001417448882726703, 'epoch': 0.38}
 38%|███▊      | 151/395 [09:29<14:36,  3.59s/it] 38%|███▊      | 152/395 [09:33<14:34,  3.60s/it]                                                 {'loss': 0.4579, 'learning_rate': 0.00014099812243881948, 'epoch': 0.38}
 38%|███▊      | 152/395 [09:33<14:34,  3.60s/it] 39%|███▊      | 153/395 [09:36<14:25,  3.58s/it]                                                 {'loss': 0.4416, 'learning_rate': 0.00014024859816377046, 'epoch': 0.39}
 39%|███▊      | 153/395 [09:36<14:25,  3.58s/it] 39%|███▉      | 154/395 [09:40<14:23,  3.58s/it]                                                 {'loss': 0.3986, 'learning_rate': 0.00013949636587711644, 'epoch': 0.39}
 39%|███▉      | 154/395 [09:40<14:23,  3.58s/it] 39%|███▉      | 155/395 [09:44<14:25,  3.61s/it]                                                 {'loss': 0.6157, 'learning_rate': 0.0001387414761906516, 'epoch': 0.39}
 39%|███▉      | 155/395 [09:44<14:25,  3.61s/it] 39%|███▉      | 156/395 [09:47<14:18,  3.59s/it]                                                 {'loss': 0.2816, 'learning_rate': 0.00013798397989496549, 'epoch': 0.39}
 39%|███▉      | 156/395 [09:47<14:18,  3.59s/it] 40%|███▉      | 157/395 [09:51<14:22,  3.63s/it]                                                 {'loss': 0.4972, 'learning_rate': 0.00013722392795602594, 'epoch': 0.4}
 40%|███▉      | 157/395 [09:51<14:22,  3.63s/it] 40%|████      | 158/395 [09:54<14:19,  3.63s/it]                                                 {'loss': 0.4594, 'learning_rate': 0.0001364613715117499, 'epoch': 0.4}
 40%|████      | 158/395 [09:54<14:19,  3.63s/it] 40%|████      | 159/395 [09:58<14:08,  3.60s/it]                                                 {'loss': 0.232, 'learning_rate': 0.00013569636186856288, 'epoch': 0.4}
 40%|████      | 159/395 [09:58<14:08,  3.60s/it] 41%|████      | 160/395 [10:02<14:03,  3.59s/it]                                                 {'loss': 0.4814, 'learning_rate': 0.0001349289504979467, 'epoch': 0.41}
 41%|████      | 160/395 [10:02<14:03,  3.59s/it] 41%|████      | 161/395 [10:05<13:58,  3.58s/it]                                                 {'loss': 0.7279, 'learning_rate': 0.0001341591890329766, 'epoch': 0.41}
 41%|████      | 161/395 [10:05<13:58,  3.58s/it] 41%|████      | 162/395 [10:09<14:00,  3.61s/it]                                                 {'loss': 0.2896, 'learning_rate': 0.00013338712926484725, 'epoch': 0.41}
 41%|████      | 162/395 [10:09<14:00,  3.61s/it] 41%|████▏     | 163/395 [10:12<13:53,  3.59s/it]                                                 {'loss': 0.47, 'learning_rate': 0.00013261282313938795, 'epoch': 0.41}
 41%|████▏     | 163/395 [10:12<13:53,  3.59s/it] 42%|████▏     | 164/395 [10:16<14:06,  3.66s/it]                                                 {'loss': 0.2406, 'learning_rate': 0.00013183632275356777, 'epoch': 0.42}
 42%|████▏     | 164/395 [10:16<14:06,  3.66s/it] 42%|████▏     | 165/395 [10:20<13:49,  3.61s/it]                                                 {'loss': 0.4108, 'learning_rate': 0.00013105768035199034, 'epoch': 0.42}
 42%|████▏     | 165/395 [10:20<13:49,  3.61s/it] 42%|████▏     | 166/395 [10:23<13:44,  3.60s/it]                                                 {'loss': 1.2708, 'learning_rate': 0.0001302769483233786, 'epoch': 0.42}
 42%|████▏     | 166/395 [10:23<13:44,  3.60s/it] 42%|████▏     | 167/395 [10:27<13:23,  3.52s/it]                                                 {'loss': 0.3599, 'learning_rate': 0.00012949417919705007, 'epoch': 0.42}
 42%|████▏     | 167/395 [10:27<13:23,  3.52s/it] 43%|████▎     | 168/395 [10:30<13:12,  3.49s/it]                                                 {'loss': 0.1192, 'learning_rate': 0.00012870942563938264, 'epoch': 0.43}
 43%|████▎     | 168/395 [10:30<13:12,  3.49s/it] 43%|████▎     | 169/395 [10:34<13:20,  3.54s/it]                                                 {'loss': 0.6148, 'learning_rate': 0.0001279227404502709, 'epoch': 0.43}
 43%|████▎     | 169/395 [10:34<13:20,  3.54s/it] 43%|████▎     | 170/395 [10:37<13:14,  3.53s/it]                                                 {'loss': 0.2791, 'learning_rate': 0.00012713417655957376, 'epoch': 0.43}
 43%|████▎     | 170/395 [10:37<13:14,  3.53s/it] 43%|████▎     | 171/395 [10:41<13:07,  3.52s/it]                                                 {'loss': 0.4678, 'learning_rate': 0.00012634378702355313, 'epoch': 0.43}
 43%|████▎     | 171/395 [10:41<13:07,  3.52s/it] 44%|████▎     | 172/395 [10:44<13:18,  3.58s/it]                                                 {'loss': 0.2911, 'learning_rate': 0.00012555162502130433, 'epoch': 0.44}
 44%|████▎     | 172/395 [10:44<13:18,  3.58s/it] 44%|████▍     | 173/395 [10:48<13:15,  3.58s/it]                                                 {'loss': 0.6115, 'learning_rate': 0.00012475774385117786, 'epoch': 0.44}
 44%|████▍     | 173/395 [10:48<13:15,  3.58s/it] 44%|████▍     | 174/395 [10:52<13:30,  3.67s/it]                                                 {'loss': 0.29, 'learning_rate': 0.00012396219692719363, 'epoch': 0.44}
 44%|████▍     | 174/395 [10:52<13:30,  3.67s/it] 44%|████▍     | 175/395 [10:55<13:28,  3.68s/it]                                                 {'loss': 0.2235, 'learning_rate': 0.000123165037775447, 'epoch': 0.44}
 44%|████▍     | 175/395 [10:55<13:28,  3.68s/it] 45%|████▍     | 176/395 [10:59<13:22,  3.66s/it]                                                 {'loss': 0.4114, 'learning_rate': 0.00012236632003050736, 'epoch': 0.45}
 45%|████▍     | 176/395 [10:59<13:22,  3.66s/it] 45%|████▍     | 177/395 [11:03<13:13,  3.64s/it]                                                 {'loss': 0.4223, 'learning_rate': 0.00012156609743180969, 'epoch': 0.45}
 45%|████▍     | 177/395 [11:03<13:13,  3.64s/it] 45%|████▌     | 178/395 [11:06<13:02,  3.61s/it]                                                 {'loss': 0.3017, 'learning_rate': 0.0001207644238200387, 'epoch': 0.45}
 45%|████▌     | 178/395 [11:06<13:02,  3.61s/it] 45%|████▌     | 179/395 [11:10<12:59,  3.61s/it]                                                 {'loss': 0.2666, 'learning_rate': 0.00011996135313350636, 'epoch': 0.45}
 45%|████▌     | 179/395 [11:10<12:59,  3.61s/it] 46%|████▌     | 180/395 [11:13<12:51,  3.59s/it]                                                 {'loss': 0.0862, 'learning_rate': 0.0001191569394045228, 'epoch': 0.46}
 46%|████▌     | 180/395 [11:13<12:51,  3.59s/it] 46%|████▌     | 181/395 [11:17<12:50,  3.60s/it]                                                 {'loss': 0.6916, 'learning_rate': 0.00011835123675576092, 'epoch': 0.46}
 46%|████▌     | 181/395 [11:17<12:50,  3.60s/it] 46%|████▌     | 182/395 [11:21<12:50,  3.62s/it]                                                 {'loss': 0.0356, 'learning_rate': 0.0001175442993966149, 'epoch': 0.46}
 46%|████▌     | 182/395 [11:21<12:50,  3.62s/it] 46%|████▋     | 183/395 [11:24<12:42,  3.60s/it]                                                 {'loss': 0.46, 'learning_rate': 0.00011673618161955288, 'epoch': 0.46}
 46%|████▋     | 183/395 [11:24<12:42,  3.60s/it] 47%|████▋     | 184/395 [11:28<12:31,  3.56s/it]                                                 {'loss': 0.6097, 'learning_rate': 0.00011592693779646405, 'epoch': 0.47}
 47%|████▋     | 184/395 [11:28<12:31,  3.56s/it] 47%|████▋     | 185/395 [11:31<12:25,  3.55s/it]                                                 {'loss': 0.4885, 'learning_rate': 0.00011511662237500032, 'epoch': 0.47}
 47%|████▋     | 185/395 [11:31<12:25,  3.55s/it] 47%|████▋     | 186/395 [11:35<12:19,  3.54s/it]                                                 {'loss': 0.8504, 'learning_rate': 0.00011430528987491305, 'epoch': 0.47}
 47%|████▋     | 186/395 [11:35<12:19,  3.54s/it] 47%|████▋     | 187/395 [11:38<12:20,  3.56s/it]                                                 {'loss': 0.2633, 'learning_rate': 0.00011349299488438485, 'epoch': 0.47}
 47%|████▋     | 187/395 [11:38<12:20,  3.56s/it] 48%|████▊     | 188/395 [11:42<12:20,  3.58s/it]                                                 {'loss': 0.4594, 'learning_rate': 0.00011267979205635675, 'epoch': 0.48}
 48%|████▊     | 188/395 [11:42<12:20,  3.58s/it] 48%|████▊     | 189/395 [11:46<12:22,  3.60s/it]                                                 {'loss': 0.183, 'learning_rate': 0.00011186573610485098, 'epoch': 0.48}
 48%|████▊     | 189/395 [11:46<12:22,  3.60s/it] 48%|████▊     | 190/395 [11:49<12:26,  3.64s/it]                                                 {'loss': 0.258, 'learning_rate': 0.00011105088180128976, 'epoch': 0.48}
 48%|████▊     | 190/395 [11:49<12:26,  3.64s/it] 48%|████▊     | 191/395 [11:53<12:17,  3.62s/it]                                                 {'loss': 0.4401, 'learning_rate': 0.00011023528397081011, 'epoch': 0.48}
 48%|████▊     | 191/395 [11:53<12:17,  3.62s/it] 49%|████▊     | 192/395 [11:56<12:08,  3.59s/it]                                                 {'loss': 0.4722, 'learning_rate': 0.0001094189974885752, 'epoch': 0.49}
 49%|████▊     | 192/395 [11:56<12:08,  3.59s/it] 49%|████▉     | 193/395 [12:00<12:01,  3.57s/it]                                                 {'loss': 0.3568, 'learning_rate': 0.00010860207727608214, 'epoch': 0.49}
 49%|████▉     | 193/395 [12:00<12:01,  3.57s/it] 49%|████▉     | 194/395 [12:04<11:58,  3.57s/it]                                                 {'loss': 0.5373, 'learning_rate': 0.00010778457829746666, 'epoch': 0.49}
 49%|████▉     | 194/395 [12:04<11:58,  3.57s/it] 49%|████▉     | 195/395 [12:07<11:53,  3.57s/it]                                                 {'loss': 0.4926, 'learning_rate': 0.00010696655555580524, 'epoch': 0.49}
 49%|████▉     | 195/395 [12:07<11:53,  3.57s/it] 50%|████▉     | 196/395 [12:11<11:49,  3.57s/it]                                                 {'loss': 0.4393, 'learning_rate': 0.00010614806408941422, 'epoch': 0.5}
 50%|████▉     | 196/395 [12:11<11:49,  3.57s/it] 50%|████▉     | 197/395 [12:14<11:44,  3.56s/it]                                                 {'loss': 0.612, 'learning_rate': 0.00010532915896814672, 'epoch': 0.5}
 50%|████▉     | 197/395 [12:14<11:44,  3.56s/it] 50%|█████     | 198/395 [12:18<11:53,  3.62s/it]                                                 {'loss': 0.7252, 'learning_rate': 0.00010450989528968746, 'epoch': 0.5}
 50%|█████     | 198/395 [12:18<11:53,  3.62s/it] 50%|█████     | 199/395 [12:22<11:52,  3.63s/it]                                                 {'loss': 0.4817, 'learning_rate': 0.00010369032817584561, 'epoch': 0.5}
 50%|█████     | 199/395 [12:22<11:52,  3.63s/it] 51%|█████     | 200/395 [12:25<11:42,  3.60s/it]                                                 {'loss': 0.4044, 'learning_rate': 0.00010287051276884618, 'epoch': 0.51}
 51%|█████     | 200/395 [12:25<11:42,  3.60s/it] 51%|█████     | 201/395 [12:29<11:38,  3.60s/it]                                                 {'loss': 0.5259, 'learning_rate': 0.00010205050422761988, 'epoch': 0.51}
 51%|█████     | 201/395 [12:29<11:38,  3.60s/it] 51%|█████     | 202/395 [12:32<11:29,  3.57s/it]                                                 {'loss': 0.5738, 'learning_rate': 0.00010123035772409184, 'epoch': 0.51}
 51%|█████     | 202/395 [12:32<11:29,  3.57s/it] 51%|█████▏    | 203/395 [12:36<11:31,  3.60s/it]                                                 {'loss': 0.2884, 'learning_rate': 0.0001004101284394696, 'epoch': 0.51}
 51%|█████▏    | 203/395 [12:36<11:31,  3.60s/it] 52%|█████▏    | 204/395 [12:40<11:29,  3.61s/it]                                                 {'loss': 0.1583, 'learning_rate': 9.958987156053045e-05, 'epoch': 0.52}
 52%|█████▏    | 204/395 [12:40<11:29,  3.61s/it] 52%|█████▏    | 205/395 [12:43<11:22,  3.59s/it]                                                 {'loss': 0.5238, 'learning_rate': 9.87696422759082e-05, 'epoch': 0.52}
 52%|█████▏    | 205/395 [12:43<11:22,  3.59s/it] 52%|█████▏    | 206/395 [12:47<11:15,  3.58s/it]                                                 {'loss': 0.2768, 'learning_rate': 9.794949577238013e-05, 'epoch': 0.52}
 52%|█████▏    | 206/395 [12:47<11:15,  3.58s/it] 52%|█████▏    | 207/395 [12:50<11:21,  3.62s/it]                                                 {'loss': 0.1782, 'learning_rate': 9.712948723115383e-05, 'epoch': 0.52}
 52%|█████▏    | 207/395 [12:50<11:21,  3.62s/it] 53%|█████▎    | 208/395 [12:54<11:15,  3.61s/it]                                                 {'loss': 0.6193, 'learning_rate': 9.630967182415442e-05, 'epoch': 0.53}
 53%|█████▎    | 208/395 [12:54<11:15,  3.61s/it] 53%|█████▎    | 209/395 [12:58<11:08,  3.59s/it]                                                 {'loss': 0.3163, 'learning_rate': 9.549010471031256e-05, 'epoch': 0.53}
 53%|█████▎    | 209/395 [12:58<11:08,  3.59s/it] 53%|█████▎    | 210/395 [13:01<11:07,  3.61s/it]                                                 {'loss': 0.501, 'learning_rate': 9.467084103185329e-05, 'epoch': 0.53}
 53%|█████▎    | 210/395 [13:01<11:07,  3.61s/it] 53%|█████▎    | 211/395 [13:05<10:58,  3.58s/it]                                                 {'loss': 0.8444, 'learning_rate': 9.385193591058579e-05, 'epoch': 0.53}
 53%|█████▎    | 211/395 [13:05<10:58,  3.58s/it] 54%|█████▎    | 212/395 [13:08<10:51,  3.56s/it]                                                 {'loss': 0.5763, 'learning_rate': 9.303344444419476e-05, 'epoch': 0.54}
 54%|█████▎    | 212/395 [13:08<10:51,  3.56s/it] 54%|█████▍    | 213/395 [13:12<10:48,  3.56s/it]                                                 {'loss': 0.2901, 'learning_rate': 9.221542170253339e-05, 'epoch': 0.54}
 54%|█████▍    | 213/395 [13:12<10:48,  3.56s/it] 54%|█████▍    | 214/395 [13:15<10:46,  3.57s/it]                                                 {'loss': 0.1926, 'learning_rate': 9.139792272391791e-05, 'epoch': 0.54}
 54%|█████▍    | 214/395 [13:15<10:46,  3.57s/it] 54%|█████▍    | 215/395 [13:19<10:49,  3.61s/it]                                                 {'loss': 0.3103, 'learning_rate': 9.058100251142483e-05, 'epoch': 0.54}
 54%|█████▍    | 215/395 [13:19<10:49,  3.61s/it] 55%|█████▍    | 216/395 [13:23<10:43,  3.60s/it]                                                 {'loss': 0.4278, 'learning_rate': 8.976471602918991e-05, 'epoch': 0.55}
 55%|█████▍    | 216/395 [13:23<10:43,  3.60s/it] 55%|█████▍    | 217/395 [13:26<10:38,  3.59s/it]                                                 {'loss': 0.3137, 'learning_rate': 8.894911819871026e-05, 'epoch': 0.55}
 55%|█████▍    | 217/395 [13:26<10:38,  3.59s/it] 55%|█████▌    | 218/395 [13:30<10:34,  3.58s/it]                                                 {'loss': 0.2649, 'learning_rate': 8.813426389514903e-05, 'epoch': 0.55}
 55%|█████▌    | 218/395 [13:30<10:34,  3.58s/it] 55%|█████▌    | 219/395 [13:34<10:40,  3.64s/it]                                                 {'loss': 0.3666, 'learning_rate': 8.732020794364326e-05, 'epoch': 0.55}
 55%|█████▌    | 219/395 [13:34<10:40,  3.64s/it] 56%|█████▌    | 220/395 [13:37<10:34,  3.63s/it]                                                 {'loss': 0.4916, 'learning_rate': 8.650700511561514e-05, 'epoch': 0.56}
 56%|█████▌    | 220/395 [13:37<10:34,  3.63s/it] 56%|█████▌    | 221/395 [13:41<10:25,  3.59s/it]                                                 {'loss': 0.0976, 'learning_rate': 8.5694710125087e-05, 'epoch': 0.56}
 56%|█████▌    | 221/395 [13:41<10:25,  3.59s/it] 56%|█████▌    | 222/395 [13:44<10:19,  3.58s/it]                                                 {'loss': 2.1124, 'learning_rate': 8.488337762499972e-05, 'epoch': 0.56}
 56%|█████▌    | 222/395 [13:44<10:19,  3.58s/it] 56%|█████▋    | 223/395 [13:48<10:15,  3.58s/it]                                                 {'loss': 0.4041, 'learning_rate': 8.407306220353596e-05, 'epoch': 0.56}
 56%|█████▋    | 223/395 [13:48<10:15,  3.58s/it] 57%|█████▋    | 224/395 [13:52<10:20,  3.63s/it]                                                 {'loss': 0.2148, 'learning_rate': 8.326381838044713e-05, 'epoch': 0.57}
 57%|█████▋    | 224/395 [13:52<10:20,  3.63s/it] 57%|█████▋    | 225/395 [13:55<10:09,  3.59s/it]                                                 {'loss': 0.2796, 'learning_rate': 8.245570060338512e-05, 'epoch': 0.57}
 57%|█████▋    | 225/395 [13:55<10:09,  3.59s/it] 57%|█████▋    | 226/395 [13:59<10:08,  3.60s/it]                                                 {'loss': 0.2014, 'learning_rate': 8.164876324423909e-05, 'epoch': 0.57}
 57%|█████▋    | 226/395 [13:59<10:08,  3.60s/it] 57%|█████▋    | 227/395 [14:02<10:05,  3.61s/it]                                                 {'loss': 0.4001, 'learning_rate': 8.084306059547722e-05, 'epoch': 0.57}
 57%|█████▋    | 227/395 [14:02<10:05,  3.61s/it] 58%|█████▊    | 228/395 [14:06<09:54,  3.56s/it]                                                 {'loss': 0.5136, 'learning_rate': 8.003864686649366e-05, 'epoch': 0.58}
 58%|█████▊    | 228/395 [14:06<09:54,  3.56s/it] 58%|█████▊    | 229/395 [14:09<09:54,  3.58s/it]                                                 {'loss': 0.3065, 'learning_rate': 7.923557617996131e-05, 'epoch': 0.58}
 58%|█████▊    | 229/395 [14:09<09:54,  3.58s/it] 58%|█████▊    | 230/395 [14:13<09:50,  3.58s/it]                                                 {'loss': 0.3684, 'learning_rate': 7.843390256819034e-05, 'epoch': 0.58}
 58%|█████▊    | 230/395 [14:13<09:50,  3.58s/it] 58%|█████▊    | 231/395 [14:17<09:49,  3.60s/it]                                                 {'loss': 0.5746, 'learning_rate': 7.763367996949267e-05, 'epoch': 0.58}
 58%|█████▊    | 231/395 [14:17<09:49,  3.60s/it] 59%|█████▊    | 232/395 [14:20<09:52,  3.63s/it]                                                 {'loss': 0.281, 'learning_rate': 7.683496222455304e-05, 'epoch': 0.59}
 59%|█████▊    | 232/395 [14:20<09:52,  3.63s/it] 59%|█████▉    | 233/395 [14:24<09:50,  3.65s/it]                                                 {'loss': 0.3884, 'learning_rate': 7.603780307280639e-05, 'epoch': 0.59}
 59%|█████▉    | 233/395 [14:24<09:50,  3.65s/it] 59%|█████▉    | 234/395 [14:27<09:41,  3.61s/it]                                                 {'loss': 0.3493, 'learning_rate': 7.524225614882216e-05, 'epoch': 0.59}
 59%|█████▉    | 234/395 [14:27<09:41,  3.61s/it] 59%|█████▉    | 235/395 [14:31<09:35,  3.59s/it]                                                 {'loss': 0.5128, 'learning_rate': 7.44483749786957e-05, 'epoch': 0.59}
 59%|█████▉    | 235/395 [14:31<09:35,  3.59s/it] 60%|█████▉    | 236/395 [14:35<09:29,  3.58s/it]                                                 {'loss': 0.5319, 'learning_rate': 7.365621297644686e-05, 'epoch': 0.6}
 60%|█████▉    | 236/395 [14:35<09:29,  3.58s/it] 60%|██████    | 237/395 [14:38<09:22,  3.56s/it]                                                 {'loss': 0.1392, 'learning_rate': 7.286582344042625e-05, 'epoch': 0.6}
 60%|██████    | 237/395 [14:38<09:22,  3.56s/it] 60%|██████    | 238/395 [14:42<09:21,  3.58s/it]                                                 {'loss': 0.4564, 'learning_rate': 7.207725954972913e-05, 'epoch': 0.6}
 60%|██████    | 238/395 [14:42<09:21,  3.58s/it] 61%|██████    | 239/395 [14:45<09:17,  3.57s/it]                                                 {'loss': 0.4744, 'learning_rate': 7.129057436061739e-05, 'epoch': 0.61}
 61%|██████    | 239/395 [14:45<09:17,  3.57s/it] 61%|██████    | 240/395 [14:49<09:23,  3.63s/it]                                                 {'loss': 0.3384, 'learning_rate': 7.050582080294996e-05, 'epoch': 0.61}
 61%|██████    | 240/395 [14:49<09:23,  3.63s/it] 61%|██████    | 241/395 [14:53<09:22,  3.65s/it]                                                 {'loss': 0.4693, 'learning_rate': 6.972305167662145e-05, 'epoch': 0.61}
 61%|██████    | 241/395 [14:53<09:22,  3.65s/it] 61%|██████▏   | 242/395 [14:56<09:12,  3.61s/it]                                                 {'loss': 0.4256, 'learning_rate': 6.89423196480097e-05, 'epoch': 0.61}
 61%|██████▏   | 242/395 [14:56<09:12,  3.61s/it] 62%|██████▏   | 243/395 [15:00<09:01,  3.56s/it]                                                 {'loss': 0.3144, 'learning_rate': 6.816367724643224e-05, 'epoch': 0.62}
 62%|██████▏   | 243/395 [15:00<09:01,  3.56s/it] 62%|██████▏   | 244/395 [15:03<08:57,  3.56s/it]                                                 {'loss': 0.3307, 'learning_rate': 6.738717686061206e-05, 'epoch': 0.62}
 62%|██████▏   | 244/395 [15:03<08:57,  3.56s/it] 62%|██████▏   | 245/395 [15:07<08:53,  3.56s/it]                                                 {'loss': 0.135, 'learning_rate': 6.661287073515275e-05, 'epoch': 0.62}
 62%|██████▏   | 245/395 [15:07<08:53,  3.56s/it] 62%|██████▏   | 246/395 [15:10<08:51,  3.57s/it]                                                 {'loss': 0.3776, 'learning_rate': 6.58408109670234e-05, 'epoch': 0.62}
 62%|██████▏   | 246/395 [15:10<08:51,  3.57s/it] 63%|██████▎   | 247/395 [15:14<08:45,  3.55s/it]                                                 {'loss': 0.5198, 'learning_rate': 6.507104950205336e-05, 'epoch': 0.63}
 63%|██████▎   | 247/395 [15:14<08:45,  3.55s/it] 63%|██████▎   | 248/395 [15:18<08:43,  3.56s/it]                                                 {'loss': 0.264, 'learning_rate': 6.430363813143716e-05, 'epoch': 0.63}
 63%|██████▎   | 248/395 [15:18<08:43,  3.56s/it] 63%|██████▎   | 249/395 [15:21<08:42,  3.58s/it]                                                 {'loss': 0.4534, 'learning_rate': 6.35386284882501e-05, 'epoch': 0.63}
 63%|██████▎   | 249/395 [15:21<08:42,  3.58s/it] 63%|██████▎   | 250/395 [15:25<08:47,  3.64s/it]                                                 {'loss': 0.2184, 'learning_rate': 6.277607204397408e-05, 'epoch': 0.63}
 63%|██████▎   | 250/395 [15:25<08:47,  3.64s/it] 64%|██████▎   | 251/395 [15:28<08:40,  3.61s/it]                                                 {'loss': 0.242, 'learning_rate': 6.201602010503454e-05, 'epoch': 0.64}
 64%|██████▎   | 251/395 [15:28<08:40,  3.61s/it] 64%|██████▍   | 252/395 [15:32<08:36,  3.61s/it]                                                 {'loss': 0.3224, 'learning_rate': 6.125852380934841e-05, 'epoch': 0.64}
 64%|██████▍   | 252/395 [15:32<08:36,  3.61s/it] 64%|██████▍   | 253/395 [15:36<08:35,  3.63s/it]                                                 {'loss': 0.3332, 'learning_rate': 6.0503634122883556e-05, 'epoch': 0.64}
 64%|██████▍   | 253/395 [15:36<08:35,  3.63s/it] 64%|██████▍   | 254/395 [15:39<08:29,  3.61s/it]                                                 {'loss': 0.3695, 'learning_rate': 5.975140183622958e-05, 'epoch': 0.64}
 64%|██████▍   | 254/395 [15:39<08:29,  3.61s/it] 65%|██████▍   | 255/395 [15:43<08:25,  3.61s/it]                                                 {'loss': 0.6305, 'learning_rate': 5.900187756118055e-05, 'epoch': 0.65}
 65%|██████▍   | 255/395 [15:43<08:25,  3.61s/it] 65%|██████▍   | 256/395 [15:46<08:19,  3.59s/it]                                                 {'loss': 0.6971, 'learning_rate': 5.8255111727329717e-05, 'epoch': 0.65}
 65%|██████▍   | 256/395 [15:46<08:19,  3.59s/it] 65%|██████▌   | 257/395 [15:50<08:18,  3.62s/it]                                                 {'loss': 0.2798, 'learning_rate': 5.751115457867653e-05, 'epoch': 0.65}
 65%|██████▌   | 257/395 [15:50<08:18,  3.62s/it] 65%|██████▌   | 258/395 [15:54<08:20,  3.65s/it]                                                 {'loss': 0.3894, 'learning_rate': 5.6770056170246175e-05, 'epoch': 0.65}
 65%|██████▌   | 258/395 [15:54<08:20,  3.65s/it] 66%|██████▌   | 259/395 [15:57<08:11,  3.62s/it]                                                 {'loss': 0.3127, 'learning_rate': 5.6031866364721554e-05, 'epoch': 0.66}
 66%|██████▌   | 259/395 [15:57<08:11,  3.62s/it] 66%|██████▌   | 260/395 [16:01<07:59,  3.55s/it]                                                 {'loss': 0.307, 'learning_rate': 5.529663482908864e-05, 'epoch': 0.66}
 66%|██████▌   | 260/395 [16:01<07:59,  3.55s/it] 66%|██████▌   | 261/395 [16:04<07:56,  3.56s/it]                                                 {'loss': 0.4095, 'learning_rate': 5.4564411031294695e-05, 'epoch': 0.66}
 66%|██████▌   | 261/395 [16:04<07:56,  3.56s/it] 66%|██████▋   | 262/395 [16:08<07:52,  3.56s/it]                                                 {'loss': 0.373, 'learning_rate': 5.383524423691994e-05, 'epoch': 0.66}
 66%|██████▋   | 262/395 [16:08<07:52,  3.56s/it] 67%|██████▋   | 263/395 [16:11<07:48,  3.55s/it]                                                 {'loss': 0.4146, 'learning_rate': 5.310918350586291e-05, 'epoch': 0.67}
 67%|██████▋   | 263/395 [16:11<07:48,  3.55s/it] 67%|██████▋   | 264/395 [16:15<07:41,  3.52s/it]                                                 {'loss': 0.6141, 'learning_rate': 5.2386277689039565e-05, 'epoch': 0.67}
 67%|██████▋   | 264/395 [16:15<07:41,  3.52s/it] 67%|██████▋   | 265/395 [16:19<07:45,  3.58s/it]                                                 {'loss': 0.2494, 'learning_rate': 5.1666575425096396e-05, 'epoch': 0.67}
 67%|██████▋   | 265/395 [16:19<07:45,  3.58s/it] 67%|██████▋   | 266/395 [16:22<07:35,  3.53s/it]                                                 {'loss': 1.7882, 'learning_rate': 5.095012513713815e-05, 'epoch': 0.67}
 67%|██████▋   | 266/395 [16:22<07:35,  3.53s/it] 68%|██████▊   | 267/395 [16:26<07:36,  3.57s/it]                                                 {'loss': 0.1889, 'learning_rate': 5.023697502946969e-05, 'epoch': 0.68}
 68%|██████▊   | 267/395 [16:26<07:36,  3.57s/it] 68%|██████▊   | 268/395 [16:29<07:28,  3.53s/it]                                                 {'loss': 0.7404, 'learning_rate': 4.9527173084352544e-05, 'epoch': 0.68}
 68%|██████▊   | 268/395 [16:29<07:28,  3.53s/it] 68%|██████▊   | 269/395 [16:33<07:23,  3.52s/it]                                                 {'loss': 0.5791, 'learning_rate': 4.882076705877689e-05, 'epoch': 0.68}
 68%|██████▊   | 269/395 [16:33<07:23,  3.52s/it] 68%|██████▊   | 270/395 [16:36<07:18,  3.51s/it]                                                 {'loss': 0.3238, 'learning_rate': 4.811780448124812e-05, 'epoch': 0.68}
 68%|██████▊   | 270/395 [16:36<07:18,  3.51s/it] 69%|██████▊   | 271/395 [16:40<07:17,  3.53s/it]                                                 {'loss': 0.0985, 'learning_rate': 4.741833264858909e-05, 'epoch': 0.69}
 69%|██████▊   | 271/395 [16:40<07:17,  3.53s/it] 69%|██████▉   | 272/395 [16:43<07:15,  3.54s/it]                                                 {'loss': 0.4493, 'learning_rate': 4.6722398622757935e-05, 'epoch': 0.69}
 69%|██████▉   | 272/395 [16:43<07:15,  3.54s/it] 69%|██████▉   | 273/395 [16:47<07:13,  3.55s/it]                                                 {'loss': 0.0888, 'learning_rate': 4.603004922768148e-05, 'epoch': 0.69}
 69%|██████▉   | 273/395 [16:47<07:13,  3.55s/it] 69%|██████▉   | 274/395 [16:50<07:12,  3.57s/it]                                                 {'loss': 0.4055, 'learning_rate': 4.5341331046105064e-05, 'epoch': 0.69}
 69%|██████▉   | 274/395 [16:50<07:12,  3.57s/it] 70%|██████▉   | 275/395 [16:54<07:09,  3.58s/it]                                                 {'loss': 0.5418, 'learning_rate': 4.465629041645819e-05, 'epoch': 0.7}
 70%|██████▉   | 275/395 [16:54<07:09,  3.58s/it] 70%|██████▉   | 276/395 [16:58<07:11,  3.63s/it]                                                 {'loss': 0.1009, 'learning_rate': 4.397497342973676e-05, 'epoch': 0.7}
 70%|██████▉   | 276/395 [16:58<07:11,  3.63s/it] 70%|███████   | 277/395 [17:01<06:59,  3.55s/it]                                                 {'loss': 0.4552, 'learning_rate': 4.3297425926402115e-05, 'epoch': 0.7}
 70%|███████   | 277/395 [17:01<06:59,  3.55s/it] 70%|███████   | 278/395 [17:05<06:55,  3.55s/it]                                                 {'loss': 0.5211, 'learning_rate': 4.2623693493296644e-05, 'epoch': 0.7}
 70%|███████   | 278/395 [17:05<06:55,  3.55s/it] 71%|███████   | 279/395 [17:08<06:53,  3.57s/it]                                                 {'loss': 0.2676, 'learning_rate': 4.1953821460576715e-05, 'epoch': 0.71}
 71%|███████   | 279/395 [17:08<06:53,  3.57s/it] 71%|███████   | 280/395 [17:12<06:50,  3.57s/it]                                                 {'loss': 0.8417, 'learning_rate': 4.1287854898662705e-05, 'epoch': 0.71}
 71%|███████   | 280/395 [17:12<06:50,  3.57s/it] 71%|███████   | 281/395 [17:15<06:44,  3.55s/it]                                                 {'loss': 0.4501, 'learning_rate': 4.0625838615206566e-05, 'epoch': 0.71}
 71%|███████   | 281/395 [17:15<06:44,  3.55s/it] 71%|███████▏  | 282/395 [17:19<06:41,  3.55s/it]                                                 {'loss': 0.3311, 'learning_rate': 3.996781715207706e-05, 'epoch': 0.71}
 71%|███████▏  | 282/395 [17:19<06:41,  3.55s/it] 72%|███████▏  | 283/395 [17:23<06:40,  3.57s/it]                                                 {'loss': 0.1553, 'learning_rate': 3.9313834782362926e-05, 'epoch': 0.72}
 72%|███████▏  | 283/395 [17:23<06:40,  3.57s/it] 72%|███████▏  | 284/395 [17:26<06:41,  3.62s/it]                                                 {'loss': 0.3994, 'learning_rate': 3.866393550739416e-05, 'epoch': 0.72}
 72%|███████▏  | 284/395 [17:26<06:41,  3.62s/it] 72%|███████▏  | 285/395 [17:30<06:35,  3.59s/it]                                                 {'loss': 0.3386, 'learning_rate': 3.801816305378124e-05, 'epoch': 0.72}
 72%|███████▏  | 285/395 [17:30<06:35,  3.59s/it] 72%|███████▏  | 286/395 [17:33<06:29,  3.58s/it]                                                 {'loss': 0.4696, 'learning_rate': 3.737656087047347e-05, 'epoch': 0.72}
 72%|███████▏  | 286/395 [17:33<06:29,  3.58s/it] 73%|███████▎  | 287/395 [17:37<06:26,  3.58s/it]                                                 {'loss': 0.6185, 'learning_rate': 3.673917212583538e-05, 'epoch': 0.73}
 73%|███████▎  | 287/395 [17:37<06:26,  3.58s/it] 73%|███████▎  | 288/395 [17:41<06:23,  3.59s/it]                                                 {'loss': 0.3432, 'learning_rate': 3.610603970474239e-05, 'epoch': 0.73}
 73%|███████▎  | 288/395 [17:41<06:23,  3.59s/it] 73%|███████▎  | 289/395 [17:44<06:19,  3.58s/it]                                                 {'loss': 0.3383, 'learning_rate': 3.547720620569539e-05, 'epoch': 0.73}
 73%|███████▎  | 289/395 [17:44<06:19,  3.58s/it] 73%|███████▎  | 290/395 [17:48<06:16,  3.58s/it]                                                 {'loss': 0.3252, 'learning_rate': 3.485271393795453e-05, 'epoch': 0.73}
 73%|███████▎  | 290/395 [17:48<06:16,  3.58s/it] 74%|███████▎  | 291/395 [17:51<06:13,  3.59s/it]                                                 {'loss': 0.2885, 'learning_rate': 3.4232604918692754e-05, 'epoch': 0.74}
 74%|███████▎  | 291/395 [17:51<06:13,  3.59s/it] 74%|███████▍  | 292/395 [17:55<06:07,  3.57s/it]                                                 {'loss': 0.6503, 'learning_rate': 3.361692087016863e-05, 'epoch': 0.74}
 74%|███████▍  | 292/395 [17:55<06:07,  3.57s/it] 74%|███████▍  | 293/395 [17:59<06:08,  3.62s/it]                                                 {'loss': 0.3906, 'learning_rate': 3.300570321691934e-05, 'epoch': 0.74}
 74%|███████▍  | 293/395 [17:59<06:08,  3.62s/it] 74%|███████▍  | 294/395 [18:02<06:06,  3.62s/it]                                                 {'loss': 0.4041, 'learning_rate': 3.2398993082973294e-05, 'epoch': 0.74}
 74%|███████▍  | 294/395 [18:02<06:06,  3.62s/it] 75%|███████▍  | 295/395 [18:06<05:59,  3.59s/it]                                                 {'loss': 0.4703, 'learning_rate': 3.179683128908352e-05, 'epoch': 0.75}
 75%|███████▍  | 295/395 [18:06<05:59,  3.59s/it] 75%|███████▍  | 296/395 [18:09<05:55,  3.59s/it]                                                 {'loss': 0.3658, 'learning_rate': 3.1199258349980966e-05, 'epoch': 0.75}
 75%|███████▍  | 296/395 [18:09<05:55,  3.59s/it] 75%|███████▌  | 297/395 [18:13<05:49,  3.57s/it]                                                 {'loss': 0.3998, 'learning_rate': 3.0606314471648643e-05, 'epoch': 0.75}
 75%|███████▌  | 297/395 [18:13<05:49,  3.57s/it] 75%|███████▌  | 298/395 [18:16<05:46,  3.57s/it]                                                 {'loss': 0.2361, 'learning_rate': 3.0018039548616494e-05, 'epoch': 0.75}
 75%|███████▌  | 298/395 [18:16<05:46,  3.57s/it] 76%|███████▌  | 299/395 [18:20<05:43,  3.58s/it]                                                 {'loss': 0.4401, 'learning_rate': 2.943447316127712e-05, 'epoch': 0.76}
 76%|███████▌  | 299/395 [18:20<05:43,  3.58s/it] 76%|███████▌  | 300/395 [18:24<05:39,  3.57s/it]                                                 {'loss': 0.4157, 'learning_rate': 2.8855654573222825e-05, 'epoch': 0.76}
 76%|███████▌  | 300/395 [18:24<05:39,  3.57s/it] 76%|███████▌  | 301/395 [18:27<05:36,  3.58s/it]                                                 {'loss': 0.4376, 'learning_rate': 2.8281622728603864e-05, 'epoch': 0.76}
 76%|███████▌  | 301/395 [18:27<05:36,  3.58s/it] 76%|███████▋  | 302/395 [18:31<05:40,  3.66s/it]                                                 {'loss': 0.8448, 'learning_rate': 2.7712416249508177e-05, 'epoch': 0.76}
 76%|███████▋  | 302/395 [18:31<05:40,  3.66s/it] 77%|███████▋  | 303/395 [18:35<05:33,  3.62s/it]                                                 {'loss': 0.6689, 'learning_rate': 2.714807343336273e-05, 'epoch': 0.77}
 77%|███████▋  | 303/395 [18:35<05:33,  3.62s/it] 77%|███████▋  | 304/395 [18:38<05:28,  3.61s/it]                                                 {'loss': 0.2602, 'learning_rate': 2.6588632250356948e-05, 'epoch': 0.77}
 77%|███████▋  | 304/395 [18:38<05:28,  3.61s/it] 77%|███████▋  | 305/395 [18:42<05:22,  3.59s/it]                                                 {'loss': 0.2679, 'learning_rate': 2.6034130340887895e-05, 'epoch': 0.77}
 77%|███████▋  | 305/395 [18:42<05:22,  3.59s/it] 77%|███████▋  | 306/395 [18:45<05:17,  3.57s/it]                                                 {'loss': 0.2132, 'learning_rate': 2.5484605013027784e-05, 'epoch': 0.77}
 77%|███████▋  | 306/395 [18:45<05:17,  3.57s/it] 78%|███████▊  | 307/395 [18:49<05:14,  3.58s/it]                                                 {'loss': 0.1525, 'learning_rate': 2.4940093240013717e-05, 'epoch': 0.78}
 78%|███████▊  | 307/395 [18:49<05:14,  3.58s/it] 78%|███████▊  | 308/395 [18:52<05:04,  3.50s/it]                                                 {'loss': 0.209, 'learning_rate': 2.4400631657760186e-05, 'epoch': 0.78}
 78%|███████▊  | 308/395 [18:52<05:04,  3.50s/it] 78%|███████▊  | 309/395 [18:56<05:03,  3.53s/it]                                                 {'loss': 0.2881, 'learning_rate': 2.3866256562394083e-05, 'epoch': 0.78}
 78%|███████▊  | 309/395 [18:56<05:03,  3.53s/it] 78%|███████▊  | 310/395 [18:59<05:06,  3.60s/it]                                                 {'loss': 0.856, 'learning_rate': 2.333700390781256e-05, 'epoch': 0.78}
 78%|███████▊  | 310/395 [18:59<05:06,  3.60s/it] 79%|███████▊  | 311/395 [19:03<04:59,  3.56s/it]                                                 {'loss': 0.3615, 'learning_rate': 2.2812909303264085e-05, 'epoch': 0.79}
 79%|███████▊  | 311/395 [19:03<04:59,  3.56s/it] 79%|███████▉  | 312/395 [19:06<04:55,  3.55s/it]                                                 {'loss': 0.4514, 'learning_rate': 2.2294008010952382e-05, 'epoch': 0.79}
 79%|███████▉  | 312/395 [19:06<04:55,  3.55s/it] 79%|███████▉  | 313/395 [19:10<04:52,  3.56s/it]                                                 {'loss': 0.4713, 'learning_rate': 2.1780334943664162e-05, 'epoch': 0.79}
 79%|███████▉  | 313/395 [19:10<04:52,  3.56s/it] 79%|███████▉  | 314/395 [19:14<04:48,  3.56s/it]                                                 {'loss': 0.3066, 'learning_rate': 2.127192466241994e-05, 'epoch': 0.79}
 79%|███████▉  | 314/395 [19:14<04:48,  3.56s/it] 80%|███████▉  | 315/395 [19:17<04:45,  3.57s/it]                                                 {'loss': 0.5683, 'learning_rate': 2.07688113741488e-05, 'epoch': 0.8}
 80%|███████▉  | 315/395 [19:17<04:45,  3.57s/it] 80%|████████  | 316/395 [19:21<04:41,  3.56s/it]                                                 {'loss': 0.5282, 'learning_rate': 2.0271028929386738e-05, 'epoch': 0.8}
 80%|████████  | 316/395 [19:21<04:41,  3.56s/it] 80%|████████  | 317/395 [19:24<04:35,  3.54s/it]                                                 {'loss': 0.149, 'learning_rate': 1.977861081999931e-05, 'epoch': 0.8}
 80%|████████  | 317/395 [19:24<04:35,  3.54s/it] 81%|████████  | 318/395 [19:28<04:33,  3.55s/it]                                                 {'loss': 0.2361, 'learning_rate': 1.92915901769281e-05, 'epoch': 0.81}
 81%|████████  | 318/395 [19:28<04:33,  3.55s/it] 81%|████████  | 319/395 [19:32<04:33,  3.60s/it]                                                 {'loss': 0.4159, 'learning_rate': 1.880999976796164e-05, 'epoch': 0.81}
 81%|████████  | 319/395 [19:32<04:33,  3.60s/it] 81%|████████  | 320/395 [19:35<04:29,  3.60s/it]                                                 {'loss': 0.6322, 'learning_rate': 1.8333871995530726e-05, 'epoch': 0.81}
 81%|████████  | 320/395 [19:35<04:29,  3.60s/it] 81%|████████▏ | 321/395 [19:39<04:22,  3.55s/it]                                                 {'loss': 0.255, 'learning_rate': 1.786323889452828e-05, 'epoch': 0.81}
 81%|████████▏ | 321/395 [19:39<04:22,  3.55s/it] 82%|████████▏ | 322/395 [19:42<04:18,  3.55s/it]                                                 {'loss': 0.2336, 'learning_rate': 1.739813213015401e-05, 'epoch': 0.82}
 82%|████████▏ | 322/395 [19:42<04:18,  3.55s/it] 82%|████████▏ | 323/395 [19:46<04:17,  3.58s/it]                                                 {'loss': 0.2891, 'learning_rate': 1.693858299578396e-05, 'epoch': 0.82}
 82%|████████▏ | 323/395 [19:46<04:17,  3.58s/it] 82%|████████▏ | 324/395 [19:49<04:12,  3.56s/it]                                                 {'loss': 0.3923, 'learning_rate': 1.6484622410864835e-05, 'epoch': 0.82}
 82%|████████▏ | 324/395 [19:49<04:12,  3.56s/it] 82%|████████▏ | 325/395 [19:53<04:10,  3.57s/it]                                                 {'loss': 0.2373, 'learning_rate': 1.6036280918833924e-05, 'epoch': 0.82}
 82%|████████▏ | 325/395 [19:53<04:10,  3.57s/it] 83%|████████▎ | 326/395 [19:56<04:06,  3.57s/it]                                                 {'loss': 0.2332, 'learning_rate': 1.5593588685063896e-05, 'epoch': 0.83}
 83%|████████▎ | 326/395 [19:56<04:06,  3.57s/it] 83%|████████▎ | 327/395 [20:00<04:02,  3.57s/it]                                                 {'loss': 0.4347, 'learning_rate': 1.515657549483328e-05, 'epoch': 0.83}
 83%|████████▎ | 327/395 [20:00<04:02,  3.57s/it] 83%|████████▎ | 328/395 [20:04<04:02,  3.62s/it]                                                 {'loss': 0.1756, 'learning_rate': 1.4725270751322452e-05, 'epoch': 0.83}
 83%|████████▎ | 328/395 [20:04<04:02,  3.62s/it] 83%|████████▎ | 329/395 [20:07<03:55,  3.57s/it]                                                 {'loss': 0.462, 'learning_rate': 1.4299703473635218e-05, 'epoch': 0.83}
 83%|████████▎ | 329/395 [20:07<03:55,  3.57s/it] 84%|████████▎ | 330/395 [20:11<03:50,  3.54s/it]                                                 {'loss': 0.1139, 'learning_rate': 1.3879902294846536e-05, 'epoch': 0.84}
 84%|████████▎ | 330/395 [20:11<03:50,  3.54s/it] 84%|████████▍ | 331/395 [20:14<03:47,  3.56s/it]                                                 {'loss': 0.1925, 'learning_rate': 1.3465895460075873e-05, 'epoch': 0.84}
 84%|████████▍ | 331/395 [20:14<03:47,  3.56s/it] 84%|████████▍ | 332/395 [20:18<03:45,  3.57s/it]                                                 {'loss': 0.3601, 'learning_rate': 1.3057710824586899e-05, 'epoch': 0.84}
 84%|████████▍ | 332/395 [20:18<03:45,  3.57s/it] 84%|████████▍ | 333/395 [20:21<03:41,  3.58s/it]                                                 {'loss': 0.4757, 'learning_rate': 1.2655375851913232e-05, 'epoch': 0.84}
 84%|████████▍ | 333/395 [20:21<03:41,  3.58s/it] 85%|████████▍ | 334/395 [20:25<03:38,  3.59s/it]                                                 {'loss': 0.324, 'learning_rate': 1.22589176120107e-05, 'epoch': 0.85}
 85%|████████▍ | 334/395 [20:25<03:38,  3.59s/it] 85%|████████▍ | 335/395 [20:29<03:35,  3.58s/it]                                                 {'loss': 0.3754, 'learning_rate': 1.186836277943606e-05, 'epoch': 0.85}
 85%|████████▍ | 335/395 [20:29<03:35,  3.58s/it] 85%|████████▌ | 336/395 [20:32<03:34,  3.63s/it]                                                 {'loss': 0.4363, 'learning_rate': 1.1483737631552161e-05, 'epoch': 0.85}
 85%|████████▌ | 336/395 [20:32<03:34,  3.63s/it] 85%|████████▌ | 337/395 [20:36<03:28,  3.59s/it]                                                 {'loss': 0.1391, 'learning_rate': 1.1105068046760048e-05, 'epoch': 0.85}
 85%|████████▌ | 337/395 [20:36<03:28,  3.59s/it] 86%|████████▌ | 338/395 [20:40<03:26,  3.62s/it]                                                 {'loss': 0.3739, 'learning_rate': 1.0732379502757717e-05, 'epoch': 0.86}
 86%|████████▌ | 338/395 [20:40<03:26,  3.62s/it] 86%|████████▌ | 339/395 [20:43<03:18,  3.54s/it]                                                 {'loss': 0.1966, 'learning_rate': 1.036569707482602e-05, 'epoch': 0.86}
 86%|████████▌ | 339/395 [20:43<03:18,  3.54s/it] 86%|████████▌ | 340/395 [20:47<03:15,  3.56s/it]                                                 {'loss': 0.5644, 'learning_rate': 1.0005045434141502e-05, 'epoch': 0.86}
 86%|████████▌ | 340/395 [20:47<03:15,  3.56s/it] 86%|████████▋ | 341/395 [20:50<03:12,  3.57s/it]                                                 {'loss': 0.4225, 'learning_rate': 9.6504488461164e-06, 'epoch': 0.86}
 86%|████████▋ | 341/395 [20:50<03:12,  3.57s/it] 87%|████████▋ | 342/395 [20:54<03:09,  3.58s/it]                                                 {'loss': 0.4723, 'learning_rate': 9.301931168766164e-06, 'epoch': 0.87}
 87%|████████▋ | 342/395 [20:54<03:09,  3.58s/it] 87%|████████▋ | 343/395 [20:57<03:05,  3.56s/it]                                                 {'loss': 0.3429, 'learning_rate': 8.959515851104117e-06, 'epoch': 0.87}
 87%|████████▋ | 343/395 [20:57<03:05,  3.56s/it] 87%|████████▋ | 344/395 [21:01<03:00,  3.54s/it]                                                 {'loss': 0.2572, 'learning_rate': 8.623225931563805e-06, 'epoch': 0.87}
 87%|████████▋ | 344/395 [21:01<03:00,  3.54s/it] 87%|████████▋ | 345/395 [21:04<02:59,  3.59s/it]                                                 {'loss': 0.254, 'learning_rate': 8.293084036448895e-06, 'epoch': 0.87}
 87%|████████▋ | 345/395 [21:04<02:59,  3.59s/it] 88%|████████▊ | 346/395 [21:08<02:54,  3.56s/it]                                                 {'loss': 0.3241, 'learning_rate': 7.96911237841088e-06, 'epoch': 0.88}
 88%|████████▊ | 346/395 [21:08<02:54,  3.56s/it] 88%|████████▊ | 347/395 [21:11<02:50,  3.54s/it]                                                 {'loss': 0.2061, 'learning_rate': 7.651332754954477e-06, 'epoch': 0.88}
 88%|████████▊ | 347/395 [21:11<02:50,  3.54s/it] 88%|████████▊ | 348/395 [21:15<02:47,  3.56s/it]                                                 {'loss': 0.1955, 'learning_rate': 7.3397665469711495e-06, 'epoch': 0.88}
 88%|████████▊ | 348/395 [21:15<02:47,  3.56s/it] 88%|████████▊ | 349/395 [21:19<02:44,  3.57s/it]                                                 {'loss': 0.3613, 'learning_rate': 7.034434717300509e-06, 'epoch': 0.88}
 88%|████████▊ | 349/395 [21:19<02:44,  3.57s/it] 89%|████████▊ | 350/395 [21:22<02:40,  3.57s/it]                                                 {'loss': 0.4132, 'learning_rate': 6.735357809319809e-06, 'epoch': 0.89}
 89%|████████▊ | 350/395 [21:22<02:40,  3.57s/it] 89%|████████▉ | 351/395 [21:26<02:36,  3.56s/it]                                                 {'loss': 0.5221, 'learning_rate': 6.442555945561901e-06, 'epoch': 0.89}
 89%|████████▉ | 351/395 [21:26<02:36,  3.56s/it] 89%|████████▉ | 352/395 [21:29<02:33,  3.56s/it]                                                 {'loss': 0.19, 'learning_rate': 6.156048826361238e-06, 'epoch': 0.89}
 89%|████████▉ | 352/395 [21:29<02:33,  3.56s/it] 89%|████████▉ | 353/395 [21:33<02:29,  3.56s/it]                                                 {'loss': 0.3805, 'learning_rate': 5.8758557285284125e-06, 'epoch': 0.89}
 89%|████████▉ | 353/395 [21:33<02:29,  3.56s/it] 90%|████████▉ | 354/395 [21:37<02:28,  3.63s/it]                                                 {'loss': 0.205, 'learning_rate': 5.6019955040531925e-06, 'epoch': 0.9}
 90%|████████▉ | 354/395 [21:37<02:28,  3.63s/it] 90%|████████▉ | 355/395 [21:40<02:24,  3.62s/it]                                                 {'loss': 0.1769, 'learning_rate': 5.334486578836118e-06, 'epoch': 0.9}
 90%|████████▉ | 355/395 [21:40<02:24,  3.62s/it] 90%|█████████ | 356/395 [21:44<02:23,  3.68s/it]                                                 {'loss': 0.6771, 'learning_rate': 5.073346951448699e-06, 'epoch': 0.9}
 90%|█████████ | 356/395 [21:44<02:23,  3.68s/it] 90%|█████████ | 357/395 [21:48<02:18,  3.65s/it]                                                 {'loss': 0.1745, 'learning_rate': 4.818594191922576e-06, 'epoch': 0.9}
 90%|█████████ | 357/395 [21:48<02:18,  3.65s/it] 91%|█████████ | 358/395 [21:51<02:13,  3.60s/it]                                                 {'loss': 0.3014, 'learning_rate': 4.5702454405672e-06, 'epoch': 0.91}
 91%|█████████ | 358/395 [21:51<02:13,  3.60s/it] 91%|█████████ | 359/395 [21:55<02:08,  3.58s/it]                                                 {'loss': 0.197, 'learning_rate': 4.328317406816751e-06, 'epoch': 0.91}
 91%|█████████ | 359/395 [21:55<02:08,  3.58s/it] 91%|█████████ | 360/395 [21:58<02:04,  3.55s/it]                                                 {'loss': 0.2032, 'learning_rate': 4.092826368105795e-06, 'epoch': 0.91}
 91%|█████████ | 360/395 [21:58<02:04,  3.55s/it] 91%|█████████▏| 361/395 [22:02<01:59,  3.52s/it]                                                 {'loss': 0.084, 'learning_rate': 3.863788168774118e-06, 'epoch': 0.91}
 91%|█████████▏| 361/395 [22:02<01:59,  3.52s/it] 92%|█████████▏| 362/395 [22:05<01:57,  3.57s/it]                                                 {'loss': 0.2911, 'learning_rate': 3.6412182190007082e-06, 'epoch': 0.92}
 92%|█████████▏| 362/395 [22:05<01:57,  3.57s/it] 92%|█████████▏| 363/395 [22:09<01:54,  3.58s/it]                                                 {'loss': 0.4576, 'learning_rate': 3.425131493766931e-06, 'epoch': 0.92}
 92%|█████████▏| 363/395 [22:09<01:54,  3.58s/it] 92%|█████████▏| 364/395 [22:13<01:51,  3.58s/it]                                                 {'loss': 0.3521, 'learning_rate': 3.2155425318489584e-06, 'epoch': 0.92}
 92%|█████████▏| 364/395 [22:13<01:51,  3.58s/it] 92%|█████████▏| 365/395 [22:16<01:47,  3.59s/it]                                                 {'loss': 0.2978, 'learning_rate': 3.012465434839529e-06, 'epoch': 0.92}
 92%|█████████▏| 365/395 [22:16<01:47,  3.59s/it] 93%|█████████▎| 366/395 [22:20<01:43,  3.58s/it]                                                 {'loss': 0.2254, 'learning_rate': 2.8159138661992824e-06, 'epoch': 0.93}
 93%|█████████▎| 366/395 [22:20<01:43,  3.58s/it] 93%|█████████▎| 367/395 [22:23<01:40,  3.58s/it]                                                 {'loss': 0.2251, 'learning_rate': 2.6259010503373206e-06, 'epoch': 0.93}
 93%|█████████▎| 367/395 [22:23<01:40,  3.58s/it] 93%|█████████▎| 368/395 [22:27<01:36,  3.59s/it]                                                 {'loss': 0.2428, 'learning_rate': 2.4424397717215387e-06, 'epoch': 0.93}
 93%|█████████▎| 368/395 [22:27<01:36,  3.59s/it] 93%|█████████▎| 369/395 [22:30<01:33,  3.59s/it]                                                 {'loss': 0.1474, 'learning_rate': 2.2655423740183924e-06, 'epoch': 0.93}
 93%|█████████▎| 369/395 [22:30<01:33,  3.59s/it] 94%|█████████▎| 370/395 [22:34<01:30,  3.61s/it]                                                 {'loss': 0.228, 'learning_rate': 2.0952207592624505e-06, 'epoch': 0.94}
 94%|█████████▎| 370/395 [22:34<01:30,  3.61s/it] 94%|█████████▍| 371/395 [22:38<01:27,  3.64s/it]                                                 {'loss': 0.24, 'learning_rate': 1.9314863870555255e-06, 'epoch': 0.94}
 94%|█████████▍| 371/395 [22:38<01:27,  3.64s/it] 94%|█████████▍| 372/395 [22:41<01:23,  3.61s/it]                                                 {'loss': 0.3987, 'learning_rate': 1.7743502737957106e-06, 'epoch': 0.94}
 94%|█████████▍| 372/395 [22:41<01:23,  3.61s/it] 94%|█████████▍| 373/395 [22:45<01:19,  3.60s/it]                                                 {'loss': 0.3718, 'learning_rate': 1.6238229919361858e-06, 'epoch': 0.94}
 94%|█████████▍| 373/395 [22:45<01:19,  3.60s/it] 95%|█████████▍| 374/395 [22:49<01:15,  3.60s/it]                                                 {'loss': 0.4856, 'learning_rate': 1.4799146692737741e-06, 'epoch': 0.95}
 95%|█████████▍| 374/395 [22:49<01:15,  3.60s/it] 95%|█████████▍| 375/395 [22:52<01:11,  3.60s/it]                                                 {'loss': 0.2254, 'learning_rate': 1.3426349882676325e-06, 'epoch': 0.95}
 95%|█████████▍| 375/395 [22:52<01:11,  3.60s/it] 95%|█████████▌| 376/395 [22:56<01:08,  3.58s/it]                                                 {'loss': 0.4693, 'learning_rate': 1.211993185387772e-06, 'epoch': 0.95}
 95%|█████████▌| 376/395 [22:56<01:08,  3.58s/it] 95%|█████████▌| 377/395 [22:59<01:04,  3.59s/it]                                                 {'loss': 1.7132, 'learning_rate': 1.0879980504935772e-06, 'epoch': 0.95}
 95%|█████████▌| 377/395 [22:59<01:04,  3.59s/it] 96%|█████████▌| 378/395 [23:03<01:00,  3.58s/it]                                                 {'loss': 0.6169, 'learning_rate': 9.706579262424131e-07, 'epoch': 0.96}
 96%|█████████▌| 378/395 [23:03<01:00,  3.58s/it] 96%|█████████▌| 379/395 [23:06<00:57,  3.60s/it]                                                 {'loss': 0.8429, 'learning_rate': 8.599807075283406e-07, 'epoch': 0.96}
 96%|█████████▌| 379/395 [23:06<00:57,  3.60s/it] 96%|█████████▌| 380/395 [23:10<00:53,  3.56s/it]                                                 {'loss': 0.5783, 'learning_rate': 7.559738409508855e-07, 'epoch': 0.96}
 96%|█████████▌| 380/395 [23:10<00:53,  3.56s/it] 96%|█████████▋| 381/395 [23:14<00:49,  3.56s/it]                                                 {'loss': 0.3141, 'learning_rate': 6.586443243140838e-07, 'epoch': 0.96}
 96%|█████████▋| 381/395 [23:14<00:49,  3.56s/it] 97%|█████████▋| 382/395 [23:17<00:45,  3.53s/it]                                                 {'loss': 0.1964, 'learning_rate': 5.679987061555703e-07, 'epoch': 0.97}
 97%|█████████▋| 382/395 [23:17<00:45,  3.53s/it] 97%|█████████▋| 383/395 [23:21<00:42,  3.57s/it]                                                 {'loss': 0.3207, 'learning_rate': 4.840430853060518e-07, 'epoch': 0.97}
 97%|█████████▋| 383/395 [23:21<00:42,  3.57s/it] 97%|█████████▋| 384/395 [23:24<00:39,  3.57s/it]                                                 {'loss': 0.2165, 'learning_rate': 4.0678311047890327e-07, 'epoch': 0.97}
 97%|█████████▋| 384/395 [23:24<00:39,  3.57s/it] 97%|█████████▋| 385/395 [23:28<00:35,  3.54s/it]                                                 {'loss': 0.1333, 'learning_rate': 3.362239798901712e-07, 'epoch': 0.97}
 97%|█████████▋| 385/395 [23:28<00:35,  3.54s/it] 98%|█████████▊| 386/395 [23:31<00:32,  3.56s/it]                                                 {'loss': 0.7596, 'learning_rate': 2.723704409087979e-07, 'epoch': 0.98}
 98%|█████████▊| 386/395 [23:31<00:32,  3.56s/it] 98%|█████████▊| 387/395 [23:33<00:25,  3.15s/it]                                                 {'loss': 0.4081, 'learning_rate': 2.1522678973718847e-07, 'epoch': 0.98}
 98%|█████████▊| 387/395 [23:33<00:25,  3.15s/it] 98%|█████████▊| 388/395 [23:36<00:19,  2.86s/it]                                                 {'loss': 0.306, 'learning_rate': 1.6479687112217479e-07, 'epoch': 0.98}
 98%|█████████▊| 388/395 [23:36<00:19,  2.86s/it] 98%|█████████▊| 389/395 [23:38<00:16,  2.71s/it]                                                 {'loss': 0.325, 'learning_rate': 1.2108407809635624e-07, 'epoch': 0.98}
 98%|█████████▊| 389/395 [23:38<00:16,  2.71s/it] 99%|█████████▊| 390/395 [23:40<00:12,  2.55s/it]                                                 {'loss': 0.143, 'learning_rate': 8.40913517497377e-08, 'epoch': 0.99}
 99%|█████████▊| 390/395 [23:40<00:12,  2.55s/it] 99%|█████████▉| 391/395 [23:42<00:09,  2.43s/it]                                                 {'loss': 0.4242, 'learning_rate': 5.382118103193223e-08, 'epoch': 0.99}
 99%|█████████▉| 391/395 [23:42<00:09,  2.43s/it] 99%|█████████▉| 392/395 [23:45<00:07,  2.35s/it]                                                 {'loss': 0.3366, 'learning_rate': 3.027560258465067e-08, 'epoch': 0.99}
 99%|█████████▉| 392/395 [23:45<00:07,  2.35s/it] 99%|█████████▉| 393/395 [23:47<00:04,  2.30s/it]                                                 {'loss': 0.3934, 'learning_rate': 1.345620060465569e-08, 'epoch': 0.99}
 99%|█████████▉| 393/395 [23:47<00:04,  2.30s/it]100%|█████████▉| 394/395 [23:49<00:02,  2.26s/it]                                                 {'loss': 0.2123, 'learning_rate': 3.3641067372358612e-09, 'epoch': 1.0}
100%|█████████▉| 394/395 [23:49<00:02,  2.26s/it]100%|██████████| 395/395 [23:51<00:00,  2.37s/it]                                                 {'loss': 0.1928, 'learning_rate': 0.0, 'epoch': 1.0}
100%|██████████| 395/395 [23:51<00:00,  2.37s/it]                                                 {'train_runtime': 1438.0626, 'train_samples_per_second': 2.195, 'train_steps_per_second': 0.275, 'train_loss': 0.8474235021049463, 'epoch': 1.0}
100%|██████████| 395/395 [23:51<00:00,  2.37s/it]100%|██████████| 395/395 [23:51<00:00,  3.63s/it]
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
[2024-07-10 11:07:47,631] [INFO] [launch.py:347:main] Process 1191623 exits successfully.
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.036 MB of 0.079 MB uploaded (0.002 MB deduped)wandb: / 0.080 MB of 0.080 MB uploaded (0.002 MB deduped)wandb: 
wandb: Run history:
wandb:                    train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:              train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            train/learning_rate ▃███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:                     train/loss █▅▃▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                    train/epoch 1.0
wandb:              train/global_step 395
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.1928
wandb:               train/total_flos 872293294080.0
wandb:               train/train_loss 0.84742
wandb:            train/train_runtime 1438.0626
wandb: train/train_samples_per_second 2.195
wandb:   train/train_steps_per_second 0.275
wandb: 
wandb: 🚀 View run llava1.5_c_lora_floodnet at: https://wandb.ai/ankxanity19/CPT_VLM/runs/3nriek0x
wandb: ⭐️ View project at: https://wandb.ai/ankxanity19/CPT_VLM
wandb: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240710_104345-3nriek0x/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[2024-07-10 11:07:56,641] [INFO] [launch.py:347:main] Process 1191622 exits successfully.
