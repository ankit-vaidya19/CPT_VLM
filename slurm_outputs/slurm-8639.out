[2024-07-10 11:08:01,849] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-10 11:08:04,217] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1: setting --include=localhost:0,1
[2024-07-10 11:08:04,218] [INFO] [runner.py:571:main] cmd = /home/avaidya7/.conda/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /home/avaidya7/CPT_VLM/llava/train/train_mem.py --lora_enable True --nola_enable True --nola_num_basis 512 --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed /home/avaidya7/CPT_VLM/scripts/zero3.json --model_name_or_path /home/avaidya7/checkpoints/llava-v1.5-7b-tv100_nola_lora_merged --version v1 --data_path /home/avaidya7/FloodNet_train.json --image_folder /home/avaidya7/Sat_data/FloodNet/Images/Train_Image --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /home/avaidya7/checkpoints/llava-v1.5-7b-c_floodnet_nola --num_train_epochs 1 --per_device_train_batch_size 4 --per_device_eval_batch_size 2 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name llava1.5_c_nola_floodnet
[2024-07-10 11:08:06,428] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-10 11:08:07,822] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2024-07-10 11:08:07,822] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-07-10 11:08:07,822] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-07-10 11:08:07,822] [INFO] [launch.py:163:main] dist_world_size=2
[2024-07-10 11:08:07,822] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2024-07-10 11:08:12,161] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-10 11:08:12,161] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-10 11:08:14,685] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-10 11:08:14,685] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-10 11:08:14,685] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
[2024-07-10 11:08:17,904] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 295, num_elems = 6.76B
Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:00<00:00,  2.80it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:06<00:13,  6.51s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:12<00:07,  7.28s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:13<00:06,  6.58s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  6.22s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.81s/it]
Some weights of the model checkpoint at /home/avaidya7/checkpoints/llava-v1.5-7b-tv100_nola_lora_merged were not used when initializing LlavaLlamaForCausalLM: ['model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight']
- This IS expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.74s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:17<00:00,  5.96s/it]
Some weights of the model checkpoint at /home/avaidya7/checkpoints/llava-v1.5-7b-tv100_nola_lora_merged were not used when initializing LlavaLlamaForCausalLM: ['model.vision_tower.vision_tower.vision_model.embeddings.class_embedding', 'model.vision_tower.vision_tower.vision_model.embeddings.patch_embedding.weight', 'model.vision_tower.vision_tower.vision_model.embeddings.position_embedding.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_tower.vision_tower.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_tower.vision_tower.vision_model.post_layernorm.bias', 'model.vision_tower.vision_tower.vision_model.post_layernorm.weight', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.bias', 'model.vision_tower.vision_tower.vision_model.pre_layrnorm.weight']
- This IS expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing LlavaLlamaForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Adding LoRA adapters...
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
[2024-07-10 11:10:47,724] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 686, num_elems = 7.06B
Formatting inputs...Skip in lazy mode
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations
  warnings.warn(
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
Parameter Offload: Total persistent parameters: 828416 in 760 params
wandb: Currently logged in as: ankxanity19. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /home/avaidya7/CPT_VLM/wandb/run-20240710_111056-765vh334
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run llava1.5_c_nola_floodnet
wandb: ⭐️ View project at https://wandb.ai/ankxanity19/CPT_VLM
wandb: 🚀 View run at https://wandb.ai/ankxanity19/CPT_VLM/runs/765vh334
  0%|          | 0/395 [00:00<?, ?it/s]/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
  0%|          | 1/395 [00:36<3:56:56, 36.08s/it]                                                 {'loss': 1.1632, 'learning_rate': 1.6666666666666667e-05, 'epoch': 0.0}
  0%|          | 1/395 [00:36<3:56:56, 36.08s/it]  1%|          | 2/395 [00:52<2:39:25, 24.34s/it]                                                 {'loss': 1.2635, 'learning_rate': 3.3333333333333335e-05, 'epoch': 0.01}
  1%|          | 2/395 [00:52<2:39:25, 24.34s/it]  1%|          | 3/395 [01:05<2:06:05, 19.30s/it]                                                 {'loss': 1.1136, 'learning_rate': 5e-05, 'epoch': 0.01}
  1%|          | 3/395 [01:05<2:06:05, 19.30s/it]  1%|          | 4/395 [01:19<1:51:54, 17.17s/it]                                                 {'loss': 1.1446, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.01}
  1%|          | 4/395 [01:19<1:51:54, 17.17s/it]  1%|▏         | 5/395 [01:33<1:44:17, 16.04s/it]                                                 {'loss': 0.9747, 'learning_rate': 8.333333333333334e-05, 'epoch': 0.01}
  1%|▏         | 5/395 [01:33<1:44:17, 16.04s/it]  2%|▏         | 6/395 [01:47<1:39:05, 15.29s/it]                                                 {'loss': 1.1772, 'learning_rate': 0.0001, 'epoch': 0.02}
  2%|▏         | 6/395 [01:47<1:39:05, 15.29s/it]  2%|▏         | 7/395 [02:01<1:36:19, 14.90s/it]                                                 {'loss': 0.9419, 'learning_rate': 0.00011666666666666668, 'epoch': 0.02}
  2%|▏         | 7/395 [02:01<1:36:19, 14.90s/it]  2%|▏         | 8/395 [02:15<1:34:04, 14.58s/it]                                                 {'loss': 1.0786, 'learning_rate': 0.00013333333333333334, 'epoch': 0.02}
  2%|▏         | 8/395 [02:15<1:34:04, 14.58s/it]  2%|▏         | 9/395 [02:29<1:32:44, 14.41s/it]                                                 {'loss': 0.8364, 'learning_rate': 0.00015000000000000001, 'epoch': 0.02}
  2%|▏         | 9/395 [02:29<1:32:44, 14.41s/it]  3%|▎         | 10/395 [02:42<1:30:49, 14.16s/it]                                                  {'loss': 0.9122, 'learning_rate': 0.0001666666666666667, 'epoch': 0.03}
  3%|▎         | 10/395 [02:42<1:30:49, 14.16s/it]  3%|▎         | 11/395 [02:56<1:29:53, 14.05s/it]                                                  {'loss': 0.5812, 'learning_rate': 0.00018333333333333334, 'epoch': 0.03}
  3%|▎         | 11/395 [02:56<1:29:53, 14.05s/it]  3%|▎         | 12/395 [03:10<1:29:38, 14.04s/it]                                                  {'loss': 0.6379, 'learning_rate': 0.0002, 'epoch': 0.03}
  3%|▎         | 12/395 [03:10<1:29:38, 14.04s/it]  3%|▎         | 13/395 [03:24<1:28:37, 13.92s/it]                                                  {'loss': 0.7158, 'learning_rate': 0.0001999966358932628, 'epoch': 0.03}
  3%|▎         | 13/395 [03:24<1:28:37, 13.92s/it]  4%|▎         | 14/395 [03:38<1:28:33, 13.95s/it]                                                  {'loss': 0.4992, 'learning_rate': 0.00019998654379939533, 'epoch': 0.04}
  4%|▎         | 14/395 [03:38<1:28:33, 13.95s/it]  4%|▍         | 15/395 [03:52<1:28:10, 13.92s/it]                                                  {'loss': 0.4709, 'learning_rate': 0.00019996972439741538, 'epoch': 0.04}
  4%|▍         | 15/395 [03:52<1:28:10, 13.92s/it]  4%|▍         | 16/395 [04:06<1:27:40, 13.88s/it]                                                  {'loss': 0.6395, 'learning_rate': 0.0001999461788189681, 'epoch': 0.04}
  4%|▍         | 16/395 [04:06<1:27:40, 13.88s/it]  4%|▍         | 17/395 [04:19<1:27:17, 13.86s/it]                                                  {'loss': 0.5024, 'learning_rate': 0.00019991590864825028, 'epoch': 0.04}
  4%|▍         | 17/395 [04:19<1:27:17, 13.86s/it]  5%|▍         | 18/395 [04:33<1:26:41, 13.80s/it]                                                  {'loss': 0.5892, 'learning_rate': 0.00019987891592190366, 'epoch': 0.05}
  5%|▍         | 18/395 [04:33<1:26:41, 13.80s/it]  5%|▍         | 19/395 [04:47<1:26:02, 13.73s/it]                                                  {'loss': 0.4344, 'learning_rate': 0.00019983520312887785, 'epoch': 0.05}
  5%|▍         | 19/395 [04:47<1:26:02, 13.73s/it]  5%|▌         | 20/395 [05:00<1:26:11, 13.79s/it]                                                  {'loss': 0.366, 'learning_rate': 0.00019978477321026282, 'epoch': 0.05}
  5%|▌         | 20/395 [05:00<1:26:11, 13.79s/it]  5%|▌         | 21/395 [05:14<1:25:15, 13.68s/it]                                                  {'loss': 0.5177, 'learning_rate': 0.0001997276295590912, 'epoch': 0.05}
  5%|▌         | 21/395 [05:14<1:25:15, 13.68s/it]  6%|▌         | 22/395 [05:28<1:25:33, 13.76s/it]                                                  {'loss': 0.5778, 'learning_rate': 0.00019966377602010984, 'epoch': 0.06}
  6%|▌         | 22/395 [05:28<1:25:33, 13.76s/it]  6%|▌         | 23/395 [05:42<1:25:52, 13.85s/it]                                                  {'loss': 0.4038, 'learning_rate': 0.0001995932168895211, 'epoch': 0.06}
  6%|▌         | 23/395 [05:42<1:25:52, 13.85s/it]  6%|▌         | 24/395 [05:56<1:25:22, 13.81s/it]                                                  {'loss': 0.8277, 'learning_rate': 0.00019951595691469396, 'epoch': 0.06}
  6%|▌         | 24/395 [05:56<1:25:22, 13.81s/it]  6%|▋         | 25/395 [06:09<1:25:08, 13.81s/it]                                                  {'loss': 0.523, 'learning_rate': 0.00019943200129384444, 'epoch': 0.06}
  6%|▋         | 25/395 [06:09<1:25:08, 13.81s/it]  7%|▋         | 26/395 [06:23<1:25:03, 13.83s/it]                                                  {'loss': 0.2796, 'learning_rate': 0.0001993413556756859, 'epoch': 0.07}
  7%|▋         | 26/395 [06:23<1:25:03, 13.83s/it]  7%|▋         | 27/395 [06:37<1:24:17, 13.74s/it]                                                  {'loss': 0.3803, 'learning_rate': 0.0001992440261590491, 'epoch': 0.07}
  7%|▋         | 27/395 [06:37<1:24:17, 13.74s/it]  7%|▋         | 28/395 [06:51<1:24:13, 13.77s/it]                                                  {'loss': 0.3845, 'learning_rate': 0.00019914001929247167, 'epoch': 0.07}
  7%|▋         | 28/395 [06:51<1:24:13, 13.77s/it]  7%|▋         | 29/395 [07:05<1:24:25, 13.84s/it]                                                  {'loss': 0.5754, 'learning_rate': 0.0001990293420737576, 'epoch': 0.07}
  7%|▋         | 29/395 [07:05<1:24:25, 13.84s/it]  8%|▊         | 30/395 [07:19<1:24:21, 13.87s/it]                                                  {'loss': 0.2106, 'learning_rate': 0.00019891200194950643, 'epoch': 0.08}
  8%|▊         | 30/395 [07:19<1:24:21, 13.87s/it]  8%|▊         | 31/395 [07:33<1:24:19, 13.90s/it]                                                  {'loss': 0.2808, 'learning_rate': 0.00019878800681461222, 'epoch': 0.08}
  8%|▊         | 31/395 [07:33<1:24:19, 13.90s/it]  8%|▊         | 32/395 [07:47<1:24:21, 13.94s/it]                                                  {'loss': 0.4458, 'learning_rate': 0.00019865736501173238, 'epoch': 0.08}
  8%|▊         | 32/395 [07:47<1:24:21, 13.94s/it]  8%|▊         | 33/395 [08:00<1:23:18, 13.81s/it]                                                  {'loss': 0.3439, 'learning_rate': 0.00019852008533072625, 'epoch': 0.08}
  8%|▊         | 33/395 [08:00<1:23:18, 13.81s/it]  9%|▊         | 34/395 [08:14<1:23:21, 13.85s/it]                                                  {'loss': 0.2782, 'learning_rate': 0.00019837617700806383, 'epoch': 0.09}
  9%|▊         | 34/395 [08:14<1:23:21, 13.85s/it]  9%|▉         | 35/395 [08:28<1:23:03, 13.84s/it]                                                  {'loss': 0.1178, 'learning_rate': 0.00019822564972620427, 'epoch': 0.09}
  9%|▉         | 35/395 [08:28<1:23:03, 13.84s/it]  9%|▉         | 36/395 [08:42<1:22:45, 13.83s/it]                                                  {'loss': 0.2237, 'learning_rate': 0.0001980685136129445, 'epoch': 0.09}
  9%|▉         | 36/395 [08:42<1:22:45, 13.83s/it]  9%|▉         | 37/395 [08:55<1:22:06, 13.76s/it]                                                  {'loss': 0.2929, 'learning_rate': 0.00019790477924073755, 'epoch': 0.09}
  9%|▉         | 37/395 [08:55<1:22:06, 13.76s/it] 10%|▉         | 38/395 [09:09<1:21:53, 13.76s/it]                                                  {'loss': 0.2514, 'learning_rate': 0.00019773445762598163, 'epoch': 0.1}
 10%|▉         | 38/395 [09:09<1:21:53, 13.76s/it] 10%|▉         | 39/395 [09:23<1:22:07, 13.84s/it]                                                  {'loss': 0.2359, 'learning_rate': 0.00019755756022827846, 'epoch': 0.1}
 10%|▉         | 39/395 [09:23<1:22:07, 13.84s/it] 10%|█         | 40/395 [09:37<1:21:56, 13.85s/it]                                                  {'loss': 0.6781, 'learning_rate': 0.00019737409894966267, 'epoch': 0.1}
 10%|█         | 40/395 [09:37<1:21:56, 13.85s/it] 10%|█         | 41/395 [09:50<1:21:01, 13.73s/it]                                                  {'loss': 0.283, 'learning_rate': 0.00019718408613380074, 'epoch': 0.1}
 10%|█         | 41/395 [09:50<1:21:01, 13.73s/it] 11%|█         | 42/395 [10:04<1:20:57, 13.76s/it]                                                  {'loss': 0.1774, 'learning_rate': 0.00019698753456516048, 'epoch': 0.11}
 11%|█         | 42/395 [10:04<1:20:57, 13.76s/it] 11%|█         | 43/395 [10:18<1:20:34, 13.73s/it]                                                  {'loss': 0.6284, 'learning_rate': 0.00019678445746815107, 'epoch': 0.11}
 11%|█         | 43/395 [10:18<1:20:34, 13.73s/it] 11%|█         | 44/395 [10:32<1:20:15, 13.72s/it]                                                  {'loss': 0.186, 'learning_rate': 0.00019657486850623306, 'epoch': 0.11}
 11%|█         | 44/395 [10:32<1:20:15, 13.72s/it] 11%|█▏        | 45/395 [10:45<1:20:04, 13.73s/it]                                                  {'loss': 0.4397, 'learning_rate': 0.00019635878178099928, 'epoch': 0.11}
 11%|█▏        | 45/395 [10:45<1:20:04, 13.73s/it] 12%|█▏        | 46/395 [10:59<1:19:35, 13.68s/it]                                                  {'loss': 0.191, 'learning_rate': 0.0001961362118312259, 'epoch': 0.12}
 12%|█▏        | 46/395 [10:59<1:19:35, 13.68s/it] 12%|█▏        | 47/395 [11:13<1:20:15, 13.84s/it]                                                  {'loss': 0.1434, 'learning_rate': 0.0001959071736318942, 'epoch': 0.12}
 12%|█▏        | 47/395 [11:13<1:20:15, 13.84s/it] 12%|█▏        | 48/395 [11:27<1:20:07, 13.86s/it]                                                  {'loss': 0.5707, 'learning_rate': 0.00019567168259318325, 'epoch': 0.12}
 12%|█▏        | 48/395 [11:27<1:20:07, 13.86s/it] 12%|█▏        | 49/395 [11:41<1:20:11, 13.91s/it]                                                  {'loss': 0.4868, 'learning_rate': 0.00019542975455943281, 'epoch': 0.12}
 12%|█▏        | 49/395 [11:41<1:20:11, 13.91s/it] 13%|█▎        | 50/395 [11:55<1:20:00, 13.92s/it]                                                  {'loss': 0.3337, 'learning_rate': 0.00019518140580807744, 'epoch': 0.13}
 13%|█▎        | 50/395 [11:55<1:20:00, 13.92s/it] 13%|█▎        | 51/395 [12:09<1:19:38, 13.89s/it]                                                  {'loss': 0.224, 'learning_rate': 0.00019492665304855132, 'epoch': 0.13}
 13%|█▎        | 51/395 [12:09<1:19:38, 13.89s/it] 13%|█▎        | 52/395 [12:23<1:19:57, 13.99s/it]                                                  {'loss': 0.3423, 'learning_rate': 0.0001946655134211639, 'epoch': 0.13}
 13%|█▎        | 52/395 [12:23<1:19:57, 13.99s/it] 13%|█▎        | 53/395 [12:37<1:19:12, 13.90s/it]                                                  {'loss': 0.1881, 'learning_rate': 0.0001943980044959468, 'epoch': 0.13}
 13%|█▎        | 53/395 [12:37<1:19:12, 13.90s/it] 14%|█▎        | 54/395 [12:51<1:19:23, 13.97s/it]                                                  {'loss': 0.3044, 'learning_rate': 0.0001941241442714716, 'epoch': 0.14}
 14%|█▎        | 54/395 [12:51<1:19:23, 13.97s/it] 14%|█▍        | 55/395 [13:05<1:19:06, 13.96s/it]                                                  {'loss': 0.3148, 'learning_rate': 0.0001938439511736388, 'epoch': 0.14}
 14%|█▍        | 55/395 [13:05<1:19:06, 13.96s/it] 14%|█▍        | 56/395 [13:18<1:17:40, 13.75s/it]                                                  {'loss': 0.327, 'learning_rate': 0.0001935574440544381, 'epoch': 0.14}
 14%|█▍        | 56/395 [13:18<1:17:40, 13.75s/it] 14%|█▍        | 57/395 [13:32<1:17:27, 13.75s/it]                                                  {'loss': 0.3944, 'learning_rate': 0.0001932646421906802, 'epoch': 0.14}
 14%|█▍        | 57/395 [13:32<1:17:27, 13.75s/it] 15%|█▍        | 58/395 [13:46<1:17:40, 13.83s/it]                                                  {'loss': 0.2839, 'learning_rate': 0.00019296556528269952, 'epoch': 0.15}
 15%|█▍        | 58/395 [13:46<1:17:40, 13.83s/it] 15%|█▍        | 59/395 [14:00<1:17:33, 13.85s/it]                                                  {'loss': 0.2152, 'learning_rate': 0.00019266023345302887, 'epoch': 0.15}
 15%|█▍        | 59/395 [14:00<1:17:33, 13.85s/it] 15%|█▌        | 60/395 [14:14<1:17:25, 13.87s/it]                                                  {'loss': 0.2217, 'learning_rate': 0.00019234866724504555, 'epoch': 0.15}
 15%|█▌        | 60/395 [14:14<1:17:25, 13.87s/it] 15%|█▌        | 61/395 [14:28<1:17:30, 13.92s/it]                                                  {'loss': 0.336, 'learning_rate': 0.00019203088762158915, 'epoch': 0.15}
 15%|█▌        | 61/395 [14:28<1:17:30, 13.92s/it] 16%|█▌        | 62/395 [14:42<1:17:26, 13.95s/it]                                                  {'loss': 0.3402, 'learning_rate': 0.00019170691596355114, 'epoch': 0.16}
 16%|█▌        | 62/395 [14:42<1:17:26, 13.95s/it] 16%|█▌        | 63/395 [14:56<1:17:45, 14.05s/it]                                                  {'loss': 0.2385, 'learning_rate': 0.00019137677406843619, 'epoch': 0.16}
 16%|█▌        | 63/395 [14:56<1:17:45, 14.05s/it] 16%|█▌        | 64/395 [15:10<1:17:00, 13.96s/it]                                                  {'loss': 0.6554, 'learning_rate': 0.00019104048414889587, 'epoch': 0.16}
 16%|█▌        | 64/395 [15:10<1:17:00, 13.96s/it] 16%|█▋        | 65/395 [15:24<1:16:51, 13.97s/it]                                                  {'loss': 0.3665, 'learning_rate': 0.00019069806883123387, 'epoch': 0.16}
 16%|█▋        | 65/395 [15:24<1:16:51, 13.97s/it] 17%|█▋        | 66/395 [15:38<1:16:26, 13.94s/it]                                                  {'loss': 0.3235, 'learning_rate': 0.00019034955115388363, 'epoch': 0.17}
 17%|█▋        | 66/395 [15:38<1:16:26, 13.94s/it] 17%|█▋        | 67/395 [15:52<1:16:08, 13.93s/it]                                                  {'loss': 0.2027, 'learning_rate': 0.00018999495456585854, 'epoch': 0.17}
 17%|█▋        | 67/395 [15:52<1:16:08, 13.93s/it] 17%|█▋        | 68/395 [16:05<1:15:49, 13.91s/it]                                                  {'loss': 0.2935, 'learning_rate': 0.00018963430292517398, 'epoch': 0.17}
 17%|█▋        | 68/395 [16:05<1:15:49, 13.91s/it] 17%|█▋        | 69/395 [16:20<1:15:56, 13.98s/it]                                                  {'loss': 0.1973, 'learning_rate': 0.00018926762049724228, 'epoch': 0.17}
 17%|█▋        | 69/395 [16:20<1:15:56, 13.98s/it] 18%|█▊        | 70/395 [16:34<1:15:45, 13.99s/it]                                                  {'loss': 0.4493, 'learning_rate': 0.00018889493195323997, 'epoch': 0.18}
 18%|█▊        | 70/395 [16:34<1:15:45, 13.99s/it] 18%|█▊        | 71/395 [16:47<1:15:13, 13.93s/it]                                                  {'loss': 0.5291, 'learning_rate': 0.00018851626236844786, 'epoch': 0.18}
 18%|█▊        | 71/395 [16:47<1:15:13, 13.93s/it] 18%|█▊        | 72/395 [17:01<1:15:17, 13.99s/it]                                                  {'loss': 0.4906, 'learning_rate': 0.00018813163722056396, 'epoch': 0.18}
 18%|█▊        | 72/395 [17:01<1:15:17, 13.99s/it] 18%|█▊        | 73/395 [17:15<1:14:34, 13.90s/it]                                                  {'loss': 0.0523, 'learning_rate': 0.0001877410823879893, 'epoch': 0.18}
 18%|█▊        | 73/395 [17:15<1:14:34, 13.90s/it] 19%|█▊        | 74/395 [17:29<1:14:33, 13.94s/it]                                                  {'loss': 0.1434, 'learning_rate': 0.0001873446241480868, 'epoch': 0.19}
 19%|█▊        | 74/395 [17:29<1:14:33, 13.94s/it] 19%|█▉        | 75/395 [17:43<1:14:27, 13.96s/it]                                                  {'loss': 0.1195, 'learning_rate': 0.00018694228917541313, 'epoch': 0.19}
 19%|█▉        | 75/395 [17:43<1:14:27, 13.96s/it] 19%|█▉        | 76/395 [17:57<1:14:12, 13.96s/it]                                                  {'loss': 0.4727, 'learning_rate': 0.00018653410453992413, 'epoch': 0.19}
 19%|█▉        | 76/395 [17:57<1:14:12, 13.96s/it] 19%|█▉        | 77/395 [18:11<1:13:54, 13.94s/it]                                                  {'loss': 0.399, 'learning_rate': 0.0001861200977051535, 'epoch': 0.19}
 19%|█▉        | 77/395 [18:11<1:13:54, 13.94s/it] 20%|█▉        | 78/395 [18:25<1:13:44, 13.96s/it]                                                  {'loss': 0.3775, 'learning_rate': 0.0001857002965263648, 'epoch': 0.2}
 20%|█▉        | 78/395 [18:25<1:13:44, 13.96s/it] 20%|██        | 79/395 [18:39<1:13:14, 13.91s/it]                                                  {'loss': 0.4561, 'learning_rate': 0.00018527472924867756, 'epoch': 0.2}
 20%|██        | 79/395 [18:39<1:13:14, 13.91s/it] 20%|██        | 80/395 [18:53<1:12:51, 13.88s/it]                                                  {'loss': 0.2967, 'learning_rate': 0.00018484342450516671, 'epoch': 0.2}
 20%|██        | 80/395 [18:53<1:12:51, 13.88s/it] 21%|██        | 81/395 [19:07<1:12:42, 13.89s/it]                                                  {'loss': 0.276, 'learning_rate': 0.0001844064113149361, 'epoch': 0.21}
 21%|██        | 81/395 [19:07<1:12:42, 13.89s/it] 21%|██        | 82/395 [19:20<1:11:11, 13.65s/it]                                                  {'loss': 0.4183, 'learning_rate': 0.0001839637190811661, 'epoch': 0.21}
 21%|██        | 82/395 [19:20<1:11:11, 13.65s/it] 21%|██        | 83/395 [19:34<1:11:35, 13.77s/it]                                                  {'loss': 0.361, 'learning_rate': 0.00018351537758913518, 'epoch': 0.21}
 21%|██        | 83/395 [19:34<1:11:35, 13.77s/it] 21%|██▏       | 84/395 [19:48<1:11:26, 13.78s/it]                                                  {'loss': 0.207, 'learning_rate': 0.00018306141700421606, 'epoch': 0.21}
 21%|██▏       | 84/395 [19:48<1:11:26, 13.78s/it] 22%|██▏       | 85/395 [20:01<1:11:20, 13.81s/it]                                                  {'loss': 0.2683, 'learning_rate': 0.000182601867869846, 'epoch': 0.22}
 22%|██▏       | 85/395 [20:01<1:11:20, 13.81s/it] 22%|██▏       | 86/395 [20:15<1:11:14, 13.83s/it]                                                  {'loss': 0.294, 'learning_rate': 0.00018213676110547176, 'epoch': 0.22}
 22%|██▏       | 86/395 [20:15<1:11:14, 13.83s/it] 22%|██▏       | 87/395 [20:29<1:10:55, 13.82s/it]                                                  {'loss': 0.4336, 'learning_rate': 0.0001816661280044693, 'epoch': 0.22}
 22%|██▏       | 87/395 [20:29<1:10:55, 13.82s/it] 22%|██▏       | 88/395 [20:43<1:10:50, 13.84s/it]                                                  {'loss': 0.1339, 'learning_rate': 0.00018119000023203837, 'epoch': 0.22}
 22%|██▏       | 88/395 [20:43<1:10:50, 13.84s/it] 23%|██▎       | 89/395 [20:57<1:10:42, 13.86s/it]                                                  {'loss': 0.2719, 'learning_rate': 0.0001807084098230719, 'epoch': 0.23}
 23%|██▎       | 89/395 [20:57<1:10:42, 13.86s/it] 23%|██▎       | 90/395 [21:10<1:09:31, 13.68s/it]                                                  {'loss': 0.2675, 'learning_rate': 0.0001802213891800007, 'epoch': 0.23}
 23%|██▎       | 90/395 [21:10<1:09:31, 13.68s/it] 23%|██▎       | 91/395 [21:24<1:09:28, 13.71s/it]                                                  {'loss': 0.3106, 'learning_rate': 0.00017972897107061328, 'epoch': 0.23}
 23%|██▎       | 91/395 [21:24<1:09:28, 13.71s/it] 23%|██▎       | 92/395 [21:38<1:09:25, 13.75s/it]                                                  {'loss': 0.2846, 'learning_rate': 0.00017923118862585123, 'epoch': 0.23}
 23%|██▎       | 92/395 [21:38<1:09:25, 13.75s/it] 24%|██▎       | 93/395 [21:52<1:09:30, 13.81s/it]                                                  {'loss': 0.3043, 'learning_rate': 0.00017872807533758007, 'epoch': 0.24}
 24%|██▎       | 93/395 [21:52<1:09:30, 13.81s/it] 24%|██▍       | 94/395 [22:06<1:09:24, 13.83s/it]                                                  {'loss': 0.3538, 'learning_rate': 0.00017821966505633587, 'epoch': 0.24}
 24%|██▍       | 94/395 [22:06<1:09:24, 13.83s/it] 24%|██▍       | 95/395 [22:19<1:08:48, 13.76s/it]                                                  {'loss': 0.2246, 'learning_rate': 0.00017770599198904763, 'epoch': 0.24}
 24%|██▍       | 95/395 [22:19<1:08:48, 13.76s/it] 24%|██▍       | 96/395 [22:32<1:07:51, 13.62s/it]                                                  {'loss': 0.1585, 'learning_rate': 0.00017718709069673594, 'epoch': 0.24}
 24%|██▍       | 96/395 [22:32<1:07:51, 13.62s/it] 25%|██▍       | 97/395 [22:47<1:08:18, 13.75s/it]                                                  {'loss': 0.4353, 'learning_rate': 0.00017666299609218745, 'epoch': 0.25}
 25%|██▍       | 97/395 [22:47<1:08:18, 13.75s/it] 25%|██▍       | 98/395 [23:00<1:08:08, 13.77s/it]                                                  {'loss': 0.3203, 'learning_rate': 0.00017613374343760594, 'epoch': 0.25}
 25%|██▍       | 98/395 [23:00<1:08:08, 13.77s/it] 25%|██▌       | 99/395 [23:14<1:08:10, 13.82s/it]                                                  {'loss': 0.3688, 'learning_rate': 0.00017559936834223982, 'epoch': 0.25}
 25%|██▌       | 99/395 [23:14<1:08:10, 13.82s/it] 25%|██▌       | 100/395 [23:28<1:08:05, 13.85s/it]                                                   {'loss': 0.1704, 'learning_rate': 0.0001750599067599863, 'epoch': 0.25}
 25%|██▌       | 100/395 [23:28<1:08:05, 13.85s/it] 26%|██▌       | 101/395 [23:42<1:07:45, 13.83s/it]                                                   {'loss': 0.3642, 'learning_rate': 0.00017451539498697225, 'epoch': 0.26}
 26%|██▌       | 101/395 [23:42<1:07:45, 13.83s/it] 26%|██▌       | 102/395 [23:56<1:07:22, 13.80s/it]                                                   {'loss': 0.3572, 'learning_rate': 0.0001739658696591121, 'epoch': 0.26}
 26%|██▌       | 102/395 [23:56<1:07:22, 13.80s/it] 26%|██▌       | 103/395 [24:09<1:06:33, 13.68s/it]                                                   {'loss': 0.3544, 'learning_rate': 0.00017341136774964307, 'epoch': 0.26}
 26%|██▌       | 103/395 [24:09<1:06:33, 13.68s/it] 26%|██▋       | 104/395 [24:23<1:06:46, 13.77s/it]                                                   {'loss': 0.3502, 'learning_rate': 0.0001728519265666373, 'epoch': 0.26}
 26%|██▋       | 104/395 [24:23<1:06:46, 13.77s/it] 27%|██▋       | 105/395 [24:37<1:06:45, 13.81s/it]                                                   {'loss': 0.2899, 'learning_rate': 0.00017228758375049185, 'epoch': 0.27}
 27%|██▋       | 105/395 [24:37<1:06:45, 13.81s/it] 27%|██▋       | 106/395 [24:51<1:06:52, 13.88s/it]                                                   {'loss': 0.5609, 'learning_rate': 0.00017171837727139613, 'epoch': 0.27}
 27%|██▋       | 106/395 [24:51<1:06:52, 13.88s/it] 27%|██▋       | 107/395 [25:05<1:06:41, 13.89s/it]                                                   {'loss': 0.2536, 'learning_rate': 0.0001711443454267772, 'epoch': 0.27}
 27%|██▋       | 107/395 [25:05<1:06:41, 13.89s/it] 27%|██▋       | 108/395 [25:18<1:05:09, 13.62s/it]                                                   {'loss': 0.4919, 'learning_rate': 0.00017056552683872292, 'epoch': 0.27}
 27%|██▋       | 108/395 [25:18<1:05:09, 13.62s/it] 28%|██▊       | 109/395 [25:32<1:05:11, 13.68s/it]                                                   {'loss': 0.4355, 'learning_rate': 0.00016998196045138353, 'epoch': 0.28}
 28%|██▊       | 109/395 [25:32<1:05:11, 13.68s/it] 28%|██▊       | 110/395 [25:46<1:05:19, 13.75s/it]                                                   {'loss': 0.2461, 'learning_rate': 0.00016939368552835137, 'epoch': 0.28}
 28%|██▊       | 110/395 [25:46<1:05:19, 13.75s/it] 28%|██▊       | 111/395 [26:00<1:05:24, 13.82s/it]                                                   {'loss': 0.3802, 'learning_rate': 0.00016880074165001905, 'epoch': 0.28}
 28%|██▊       | 111/395 [26:00<1:05:24, 13.82s/it] 28%|██▊       | 112/395 [26:14<1:05:20, 13.85s/it]                                                   {'loss': 0.1902, 'learning_rate': 0.0001682031687109165, 'epoch': 0.28}
 28%|██▊       | 112/395 [26:14<1:05:20, 13.85s/it] 29%|██▊       | 113/395 [26:27<1:05:10, 13.87s/it]                                                   {'loss': 0.254, 'learning_rate': 0.00016760100691702674, 'epoch': 0.29}
 29%|██▊       | 113/395 [26:27<1:05:10, 13.87s/it] 29%|██▉       | 114/395 [26:41<1:04:34, 13.79s/it]                                                   {'loss': 0.211, 'learning_rate': 0.0001669942967830807, 'epoch': 0.29}
 29%|██▉       | 114/395 [26:41<1:04:34, 13.79s/it] 29%|██▉       | 115/395 [26:54<1:03:16, 13.56s/it]                                                   {'loss': 0.4442, 'learning_rate': 0.00016638307912983136, 'epoch': 0.29}
 29%|██▉       | 115/395 [26:54<1:03:16, 13.56s/it] 29%|██▉       | 116/395 [27:08<1:03:26, 13.64s/it]                                                   {'loss': 0.263, 'learning_rate': 0.00016576739508130726, 'epoch': 0.29}
 29%|██▉       | 116/395 [27:08<1:03:26, 13.64s/it] 30%|██▉       | 117/395 [27:22<1:03:07, 13.62s/it]                                                   {'loss': 0.2561, 'learning_rate': 0.0001651472860620455, 'epoch': 0.3}
 30%|██▉       | 117/395 [27:22<1:03:07, 13.62s/it] 30%|██▉       | 118/395 [27:35<1:03:06, 13.67s/it]                                                   {'loss': 0.1852, 'learning_rate': 0.00016452279379430463, 'epoch': 0.3}
 30%|██▉       | 118/395 [27:35<1:03:06, 13.67s/it] 30%|███       | 119/395 [27:49<1:03:15, 13.75s/it]                                                   {'loss': 0.8789, 'learning_rate': 0.00016389396029525762, 'epoch': 0.3}
 30%|███       | 119/395 [27:49<1:03:15, 13.75s/it] 30%|███       | 120/395 [28:03<1:03:00, 13.75s/it]                                                   {'loss': 0.4341, 'learning_rate': 0.0001632608278741646, 'epoch': 0.3}
 30%|███       | 120/395 [28:03<1:03:00, 13.75s/it] 31%|███       | 121/395 [28:17<1:03:19, 13.87s/it]                                                   {'loss': 0.3179, 'learning_rate': 0.00016262343912952656, 'epoch': 0.31}
 31%|███       | 121/395 [28:17<1:03:19, 13.87s/it] 31%|███       | 122/395 [28:31<1:02:58, 13.84s/it]                                                   {'loss': 0.4601, 'learning_rate': 0.0001619818369462188, 'epoch': 0.31}
 31%|███       | 122/395 [28:31<1:02:58, 13.84s/it] 31%|███       | 123/395 [28:45<1:03:06, 13.92s/it]                                                   {'loss': 0.4324, 'learning_rate': 0.0001613360644926059, 'epoch': 0.31}
 31%|███       | 123/395 [28:45<1:03:06, 13.92s/it] 31%|███▏      | 124/395 [28:59<1:02:42, 13.88s/it]                                                   {'loss': 0.1047, 'learning_rate': 0.00016068616521763707, 'epoch': 0.31}
 31%|███▏      | 124/395 [28:59<1:02:42, 13.88s/it] 32%|███▏      | 125/395 [29:13<1:02:24, 13.87s/it]                                                   {'loss': 0.4986, 'learning_rate': 0.00016003218284792298, 'epoch': 0.32}
 32%|███▏      | 125/395 [29:13<1:02:24, 13.87s/it] 32%|███▏      | 126/395 [29:27<1:02:17, 13.89s/it]                                                   {'loss': 0.1246, 'learning_rate': 0.00015937416138479344, 'epoch': 0.32}
 32%|███▏      | 126/395 [29:27<1:02:17, 13.89s/it] 32%|███▏      | 127/395 [29:41<1:02:32, 14.00s/it]                                                   {'loss': 0.328, 'learning_rate': 0.0001587121451013373, 'epoch': 0.32}
 32%|███▏      | 127/395 [29:41<1:02:32, 14.00s/it] 32%|███▏      | 128/395 [29:55<1:02:17, 14.00s/it]                                                   {'loss': 0.2385, 'learning_rate': 0.0001580461785394233, 'epoch': 0.32}
 32%|███▏      | 128/395 [29:55<1:02:17, 14.00s/it] 33%|███▎      | 129/395 [30:09<1:01:46, 13.93s/it]                                                   {'loss': 0.113, 'learning_rate': 0.00015737630650670335, 'epoch': 0.33}
 33%|███▎      | 129/395 [30:09<1:01:46, 13.93s/it] 33%|███▎      | 130/395 [30:23<1:01:47, 13.99s/it]                                                   {'loss': 0.5018, 'learning_rate': 0.00015670257407359792, 'epoch': 0.33}
 33%|███▎      | 130/395 [30:23<1:01:47, 13.99s/it] 33%|███▎      | 131/395 [30:37<1:01:25, 13.96s/it]                                                   {'loss': 0.3252, 'learning_rate': 0.00015602502657026328, 'epoch': 0.33}
 33%|███▎      | 131/395 [30:37<1:01:25, 13.96s/it] 33%|███▎      | 132/395 [30:51<1:01:16, 13.98s/it]                                                   {'loss': 0.4137, 'learning_rate': 0.00015534370958354186, 'epoch': 0.33}
 33%|███▎      | 132/395 [30:51<1:01:16, 13.98s/it] 34%|███▎      | 133/395 [31:05<1:01:01, 13.97s/it]                                                   {'loss': 0.4665, 'learning_rate': 0.00015465866895389495, 'epoch': 0.34}
 34%|███▎      | 133/395 [31:05<1:01:01, 13.97s/it] 34%|███▍      | 134/395 [31:19<1:00:49, 13.98s/it]                                                   {'loss': 0.3564, 'learning_rate': 0.00015396995077231854, 'epoch': 0.34}
 34%|███▍      | 134/395 [31:19<1:00:49, 13.98s/it] 34%|███▍      | 135/395 [31:32<1:00:27, 13.95s/it]                                                   {'loss': 0.5112, 'learning_rate': 0.00015327760137724212, 'epoch': 0.34}
 34%|███▍      | 135/395 [31:33<1:00:27, 13.95s/it] 34%|███▍      | 136/395 [31:46<59:57, 13.89s/it]                                                   {'loss': 0.2308, 'learning_rate': 0.00015258166735141092, 'epoch': 0.34}
 34%|███▍      | 136/395 [31:46<59:57, 13.89s/it] 35%|███▍      | 137/395 [32:00<59:44, 13.90s/it]                                                 {'loss': 0.3653, 'learning_rate': 0.0001518821955187519, 'epoch': 0.35}
 35%|███▍      | 137/395 [32:00<59:44, 13.90s/it] 35%|███▍      | 138/395 [32:14<59:28, 13.88s/it]                                                 {'loss': 0.4521, 'learning_rate': 0.00015117923294122312, 'epoch': 0.35}
 35%|███▍      | 138/395 [32:14<59:28, 13.88s/it] 35%|███▌      | 139/395 [32:28<59:13, 13.88s/it]                                                 {'loss': 0.4634, 'learning_rate': 0.0001504728269156475, 'epoch': 0.35}
 35%|███▌      | 139/395 [32:28<59:13, 13.88s/it] 35%|███▌      | 140/395 [32:42<58:52, 13.85s/it]                                                 {'loss': 0.1196, 'learning_rate': 0.00014976302497053036, 'epoch': 0.35}
 35%|███▌      | 140/395 [32:42<58:52, 13.85s/it] 36%|███▌      | 141/395 [32:56<58:47, 13.89s/it]                                                 {'loss': 0.3188, 'learning_rate': 0.00014904987486286184, 'epoch': 0.36}
 36%|███▌      | 141/395 [32:56<58:47, 13.89s/it] 36%|███▌      | 142/395 [33:09<58:14, 13.81s/it]                                                 {'loss': 0.3763, 'learning_rate': 0.00014833342457490361, 'epoch': 0.36}
 36%|███▌      | 142/395 [33:09<58:14, 13.81s/it] 36%|███▌      | 143/395 [33:23<58:15, 13.87s/it]                                                 {'loss': 0.1403, 'learning_rate': 0.00014761372231096047, 'epoch': 0.36}
 36%|███▌      | 143/395 [33:23<58:15, 13.87s/it] 36%|███▋      | 144/395 [33:37<57:33, 13.76s/it]                                                 {'loss': 0.1949, 'learning_rate': 0.00014689081649413708, 'epoch': 0.36}
 36%|███▋      | 144/395 [33:37<57:33, 13.76s/it] 37%|███▋      | 145/395 [33:51<57:30, 13.80s/it]                                                 {'loss': 0.5082, 'learning_rate': 0.00014616475576308005, 'epoch': 0.37}
 37%|███▋      | 145/395 [33:51<57:30, 13.80s/it] 37%|███▋      | 146/395 [34:04<56:52, 13.71s/it]                                                 {'loss': 0.2688, 'learning_rate': 0.00014543558896870531, 'epoch': 0.37}
 37%|███▋      | 146/395 [34:04<56:52, 13.71s/it] 37%|███▋      | 147/395 [34:17<56:11, 13.59s/it]                                                 {'loss': 0.2686, 'learning_rate': 0.0001447033651709114, 'epoch': 0.37}
 37%|███▋      | 147/395 [34:18<56:11, 13.59s/it] 37%|███▋      | 148/395 [34:31<55:39, 13.52s/it]                                                 {'loss': 0.2653, 'learning_rate': 0.0001439681336352785, 'epoch': 0.37}
 37%|███▋      | 148/395 [34:31<55:39, 13.52s/it] 38%|███▊      | 149/395 [34:44<55:30, 13.54s/it]                                                 {'loss': 0.249, 'learning_rate': 0.00014322994382975386, 'epoch': 0.38}
 38%|███▊      | 149/395 [34:44<55:30, 13.54s/it] 38%|███▊      | 150/395 [34:59<55:56, 13.70s/it]                                                 {'loss': 0.9325, 'learning_rate': 0.0001424888454213235, 'epoch': 0.38}
 38%|███▊      | 150/395 [34:59<55:56, 13.70s/it] 38%|███▊      | 151/395 [35:12<55:44, 13.71s/it]                                                 {'loss': 0.3674, 'learning_rate': 0.0001417448882726703, 'epoch': 0.38}
 38%|███▊      | 151/395 [35:12<55:44, 13.71s/it] 38%|███▊      | 152/395 [35:26<55:28, 13.70s/it]                                                 {'loss': 0.3537, 'learning_rate': 0.00014099812243881948, 'epoch': 0.38}
 38%|███▊      | 152/395 [35:26<55:28, 13.70s/it] 39%|███▊      | 153/395 [35:40<55:27, 13.75s/it]                                                 {'loss': 0.3081, 'learning_rate': 0.00014024859816377046, 'epoch': 0.39}
 39%|███▊      | 153/395 [35:40<55:27, 13.75s/it] 39%|███▉      | 154/395 [35:54<55:19, 13.77s/it]                                                 {'loss': 0.2904, 'learning_rate': 0.00013949636587711644, 'epoch': 0.39}
 39%|███▉      | 154/395 [35:54<55:19, 13.77s/it] 39%|███▉      | 155/395 [36:07<55:08, 13.79s/it]                                                 {'loss': 0.6634, 'learning_rate': 0.0001387414761906516, 'epoch': 0.39}
 39%|███▉      | 155/395 [36:07<55:08, 13.79s/it] 39%|███▉      | 156/395 [36:21<55:00, 13.81s/it]                                                 {'loss': 0.1866, 'learning_rate': 0.00013798397989496549, 'epoch': 0.39}
 39%|███▉      | 156/395 [36:21<55:00, 13.81s/it] 40%|███▉      | 157/395 [36:35<54:59, 13.86s/it]                                                 {'loss': 0.6075, 'learning_rate': 0.00013722392795602594, 'epoch': 0.4}
 40%|███▉      | 157/395 [36:35<54:59, 13.86s/it] 40%|████      | 158/395 [36:49<54:08, 13.71s/it]                                                 {'loss': 0.3909, 'learning_rate': 0.0001364613715117499, 'epoch': 0.4}
 40%|████      | 158/395 [36:49<54:08, 13.71s/it] 40%|████      | 159/395 [37:03<54:15, 13.80s/it]                                                 {'loss': 0.2433, 'learning_rate': 0.00013569636186856288, 'epoch': 0.4}
 40%|████      | 159/395 [37:03<54:15, 13.80s/it] 41%|████      | 160/395 [37:17<54:10, 13.83s/it]                                                 {'loss': 0.3929, 'learning_rate': 0.0001349289504979467, 'epoch': 0.41}
 41%|████      | 160/395 [37:17<54:10, 13.83s/it] 41%|████      | 161/395 [37:30<53:54, 13.82s/it]                                                 {'loss': 0.6704, 'learning_rate': 0.0001341591890329766, 'epoch': 0.41}
 41%|████      | 161/395 [37:30<53:54, 13.82s/it] 41%|████      | 162/395 [37:44<53:48, 13.86s/it]                                                 {'loss': 0.0618, 'learning_rate': 0.00013338712926484725, 'epoch': 0.41}
 41%|████      | 162/395 [37:44<53:48, 13.86s/it] 41%|████▏     | 163/395 [37:58<52:57, 13.70s/it]                                                 {'loss': 0.3248, 'learning_rate': 0.00013261282313938795, 'epoch': 0.41}
 41%|████▏     | 163/395 [37:58<52:57, 13.70s/it] 42%|████▏     | 164/395 [38:12<53:00, 13.77s/it]                                                 {'loss': 0.1921, 'learning_rate': 0.00013183632275356777, 'epoch': 0.42}
 42%|████▏     | 164/395 [38:12<53:00, 13.77s/it] 42%|████▏     | 165/395 [38:26<53:08, 13.86s/it]                                                 {'loss': 0.3048, 'learning_rate': 0.00013105768035199034, 'epoch': 0.42}
 42%|████▏     | 165/395 [38:26<53:08, 13.86s/it] 42%|████▏     | 166/395 [38:39<52:51, 13.85s/it]                                                 {'loss': 0.8907, 'learning_rate': 0.0001302769483233786, 'epoch': 0.42}
 42%|████▏     | 166/395 [38:39<52:51, 13.85s/it] 42%|████▏     | 167/395 [38:53<52:13, 13.74s/it]                                                 {'loss': 0.3375, 'learning_rate': 0.00012949417919705007, 'epoch': 0.42}
 42%|████▏     | 167/395 [38:53<52:13, 13.74s/it] 43%|████▎     | 168/395 [39:07<52:14, 13.81s/it]                                                 {'loss': 0.1222, 'learning_rate': 0.00012870942563938264, 'epoch': 0.43}
 43%|████▎     | 168/395 [39:07<52:14, 13.81s/it] 43%|████▎     | 169/395 [39:21<52:07, 13.84s/it]                                                 {'loss': 0.3996, 'learning_rate': 0.0001279227404502709, 'epoch': 0.43}
 43%|████▎     | 169/395 [39:21<52:07, 13.84s/it] 43%|████▎     | 170/395 [39:35<52:05, 13.89s/it]                                                 {'loss': 0.2251, 'learning_rate': 0.00012713417655957376, 'epoch': 0.43}
 43%|████▎     | 170/395 [39:35<52:05, 13.89s/it] 43%|████▎     | 171/395 [39:49<51:46, 13.87s/it]                                                 {'loss': 0.389, 'learning_rate': 0.00012634378702355313, 'epoch': 0.43}
 43%|████▎     | 171/395 [39:49<51:46, 13.87s/it] 44%|████▎     | 172/395 [40:03<51:33, 13.87s/it]                                                 {'loss': 0.2052, 'learning_rate': 0.00012555162502130433, 'epoch': 0.44}
 44%|████▎     | 172/395 [40:03<51:33, 13.87s/it] 44%|████▍     | 173/395 [40:16<51:15, 13.85s/it]                                                 {'loss': 0.4725, 'learning_rate': 0.00012475774385117786, 'epoch': 0.44}
 44%|████▍     | 173/395 [40:16<51:15, 13.85s/it] 44%|████▍     | 174/395 [40:30<51:09, 13.89s/it]                                                 {'loss': 0.1387, 'learning_rate': 0.00012396219692719363, 'epoch': 0.44}
 44%|████▍     | 174/395 [40:30<51:09, 13.89s/it] 44%|████▍     | 175/395 [40:44<50:45, 13.84s/it]                                                 {'loss': 0.206, 'learning_rate': 0.000123165037775447, 'epoch': 0.44}
 44%|████▍     | 175/395 [40:44<50:45, 13.84s/it] 45%|████▍     | 176/395 [40:58<50:21, 13.80s/it]                                                 {'loss': 0.3996, 'learning_rate': 0.00012236632003050736, 'epoch': 0.45}
 45%|████▍     | 176/395 [40:58<50:21, 13.80s/it] 45%|████▍     | 177/395 [41:11<50:06, 13.79s/it]                                                 {'loss': 0.2189, 'learning_rate': 0.00012156609743180969, 'epoch': 0.45}
 45%|████▍     | 177/395 [41:11<50:06, 13.79s/it] 45%|████▌     | 178/395 [41:25<49:42, 13.75s/it]                                                 {'loss': 0.3167, 'learning_rate': 0.0001207644238200387, 'epoch': 0.45}
 45%|████▌     | 178/395 [41:25<49:42, 13.75s/it] 45%|████▌     | 179/395 [41:39<49:37, 13.79s/it]                                                 {'loss': 0.25, 'learning_rate': 0.00011996135313350636, 'epoch': 0.45}
 45%|████▌     | 179/395 [41:39<49:37, 13.79s/it] 46%|████▌     | 180/395 [41:53<49:37, 13.85s/it]                                                 {'loss': 0.0644, 'learning_rate': 0.0001191569394045228, 'epoch': 0.46}
 46%|████▌     | 180/395 [41:53<49:37, 13.85s/it] 46%|████▌     | 181/395 [42:07<49:20, 13.83s/it]                                                 {'loss': 0.4544, 'learning_rate': 0.00011835123675576092, 'epoch': 0.46}
 46%|████▌     | 181/395 [42:07<49:20, 13.83s/it] 46%|████▌     | 182/395 [42:21<49:13, 13.87s/it]                                                 {'loss': 0.0499, 'learning_rate': 0.0001175442993966149, 'epoch': 0.46}
 46%|████▌     | 182/395 [42:21<49:13, 13.87s/it] 46%|████▋     | 183/395 [42:34<48:49, 13.82s/it]                                                 {'loss': 0.3705, 'learning_rate': 0.00011673618161955288, 'epoch': 0.46}
 46%|████▋     | 183/395 [42:34<48:49, 13.82s/it] 47%|████▋     | 184/395 [42:48<48:42, 13.85s/it]                                                 {'loss': 0.4236, 'learning_rate': 0.00011592693779646405, 'epoch': 0.47}
 47%|████▋     | 184/395 [42:48<48:42, 13.85s/it] 47%|████▋     | 185/395 [43:02<48:36, 13.89s/it]                                                 {'loss': 0.4448, 'learning_rate': 0.00011511662237500032, 'epoch': 0.47}
 47%|████▋     | 185/395 [43:02<48:36, 13.89s/it] 47%|████▋     | 186/395 [43:16<48:18, 13.87s/it]                                                 {'loss': 0.5979, 'learning_rate': 0.00011430528987491305, 'epoch': 0.47}
 47%|████▋     | 186/395 [43:16<48:18, 13.87s/it] 47%|████▋     | 187/395 [43:30<48:03, 13.86s/it]                                                 {'loss': 0.2376, 'learning_rate': 0.00011349299488438485, 'epoch': 0.47}
 47%|████▋     | 187/395 [43:30<48:03, 13.86s/it] 48%|████▊     | 188/395 [43:44<48:00, 13.92s/it]                                                 {'loss': 0.2934, 'learning_rate': 0.00011267979205635675, 'epoch': 0.48}
 48%|████▊     | 188/395 [43:44<48:00, 13.92s/it] 48%|████▊     | 189/395 [43:58<47:55, 13.96s/it]                                                 {'loss': 0.1726, 'learning_rate': 0.00011186573610485098, 'epoch': 0.48}
 48%|████▊     | 189/395 [43:58<47:55, 13.96s/it] 48%|████▊     | 190/395 [44:12<47:44, 13.97s/it]                                                 {'loss': 0.158, 'learning_rate': 0.00011105088180128976, 'epoch': 0.48}
 48%|████▊     | 190/395 [44:12<47:44, 13.97s/it] 48%|████▊     | 191/395 [44:26<47:23, 13.94s/it]                                                 {'loss': 0.3488, 'learning_rate': 0.00011023528397081011, 'epoch': 0.48}
 48%|████▊     | 191/395 [44:26<47:23, 13.94s/it] 49%|████▊     | 192/395 [44:40<46:56, 13.87s/it]                                                 {'loss': 0.4858, 'learning_rate': 0.0001094189974885752, 'epoch': 0.49}
 49%|████▊     | 192/395 [44:40<46:56, 13.87s/it] 49%|████▉     | 193/395 [44:54<46:47, 13.90s/it]                                                 {'loss': 0.2703, 'learning_rate': 0.00010860207727608214, 'epoch': 0.49}
 49%|████▉     | 193/395 [44:54<46:47, 13.90s/it] 49%|████▉     | 194/395 [45:08<46:43, 13.95s/it]                                                 {'loss': 0.4288, 'learning_rate': 0.00010778457829746666, 'epoch': 0.49}
 49%|████▉     | 194/395 [45:08<46:43, 13.95s/it] 49%|████▉     | 195/395 [45:22<46:24, 13.92s/it]                                                 {'loss': 0.4297, 'learning_rate': 0.00010696655555580524, 'epoch': 0.49}
 49%|████▉     | 195/395 [45:22<46:24, 13.92s/it] 50%|████▉     | 196/395 [45:35<46:02, 13.88s/it]                                                 {'loss': 0.3838, 'learning_rate': 0.00010614806408941422, 'epoch': 0.5}
 50%|████▉     | 196/395 [45:35<46:02, 13.88s/it] 50%|████▉     | 197/395 [45:49<45:40, 13.84s/it]                                                 {'loss': 0.5727, 'learning_rate': 0.00010532915896814672, 'epoch': 0.5}
 50%|████▉     | 197/395 [45:49<45:40, 13.84s/it] 50%|█████     | 198/395 [46:03<45:15, 13.78s/it]                                                 {'loss': 0.561, 'learning_rate': 0.00010450989528968746, 'epoch': 0.5}
 50%|█████     | 198/395 [46:03<45:15, 13.78s/it] 50%|█████     | 199/395 [46:17<45:06, 13.81s/it]                                                 {'loss': 0.3623, 'learning_rate': 0.00010369032817584561, 'epoch': 0.5}
 50%|█████     | 199/395 [46:17<45:06, 13.81s/it] 51%|█████     | 200/395 [46:31<44:59, 13.84s/it]                                                 {'loss': 0.415, 'learning_rate': 0.00010287051276884618, 'epoch': 0.51}
 51%|█████     | 200/395 [46:31<44:59, 13.84s/it] 51%|█████     | 201/395 [46:44<44:50, 13.87s/it]                                                 {'loss': 0.4128, 'learning_rate': 0.00010205050422761988, 'epoch': 0.51}
 51%|█████     | 201/395 [46:44<44:50, 13.87s/it] 51%|█████     | 202/395 [46:58<44:29, 13.83s/it]                                                 {'loss': 0.5008, 'learning_rate': 0.00010123035772409184, 'epoch': 0.51}
 51%|█████     | 202/395 [46:58<44:29, 13.83s/it] 51%|█████▏    | 203/395 [47:12<44:11, 13.81s/it]                                                 {'loss': 0.2756, 'learning_rate': 0.0001004101284394696, 'epoch': 0.51}
 51%|█████▏    | 203/395 [47:12<44:11, 13.81s/it] 52%|█████▏    | 204/395 [47:26<44:04, 13.85s/it]                                                 {'loss': 0.1089, 'learning_rate': 9.958987156053045e-05, 'epoch': 0.52}
 52%|█████▏    | 204/395 [47:26<44:04, 13.85s/it] 52%|█████▏    | 205/395 [47:40<43:37, 13.78s/it]                                                 {'loss': 0.4122, 'learning_rate': 9.87696422759082e-05, 'epoch': 0.52}
 52%|█████▏    | 205/395 [47:40<43:37, 13.78s/it] 52%|█████▏    | 206/395 [47:53<43:25, 13.79s/it]                                                 {'loss': 0.2196, 'learning_rate': 9.794949577238013e-05, 'epoch': 0.52}
 52%|█████▏    | 206/395 [47:53<43:25, 13.79s/it] 52%|█████▏    | 207/395 [48:07<43:19, 13.83s/it]                                                 {'loss': 0.2114, 'learning_rate': 9.712948723115383e-05, 'epoch': 0.52}
 52%|█████▏    | 207/395 [48:07<43:19, 13.83s/it] 53%|█████▎    | 208/395 [48:21<43:13, 13.87s/it]                                                 {'loss': 0.5263, 'learning_rate': 9.630967182415442e-05, 'epoch': 0.53}
 53%|█████▎    | 208/395 [48:21<43:13, 13.87s/it] 53%|█████▎    | 209/395 [48:35<43:01, 13.88s/it]                                                 {'loss': 0.2615, 'learning_rate': 9.549010471031256e-05, 'epoch': 0.53}
 53%|█████▎    | 209/395 [48:35<43:01, 13.88s/it] 53%|█████▎    | 210/395 [48:49<42:51, 13.90s/it]                                                 {'loss': 0.388, 'learning_rate': 9.467084103185329e-05, 'epoch': 0.53}
 53%|█████▎    | 210/395 [48:49<42:51, 13.90s/it] 53%|█████▎    | 211/395 [49:02<41:45, 13.62s/it]                                                 {'loss': 0.7127, 'learning_rate': 9.385193591058579e-05, 'epoch': 0.53}
 53%|█████▎    | 211/395 [49:02<41:45, 13.62s/it] 54%|█████▎    | 212/395 [49:16<41:42, 13.67s/it]                                                 {'loss': 0.4888, 'learning_rate': 9.303344444419476e-05, 'epoch': 0.54}
 54%|█████▎    | 212/395 [49:16<41:42, 13.67s/it] 54%|█████▍    | 213/395 [49:30<41:32, 13.69s/it]                                                 {'loss': 0.2499, 'learning_rate': 9.221542170253339e-05, 'epoch': 0.54}
 54%|█████▍    | 213/395 [49:30<41:32, 13.69s/it] 54%|█████▍    | 214/395 [49:44<41:33, 13.78s/it]                                                 {'loss': 0.1543, 'learning_rate': 9.139792272391791e-05, 'epoch': 0.54}
 54%|█████▍    | 214/395 [49:44<41:33, 13.78s/it] 54%|█████▍    | 215/395 [49:57<41:21, 13.79s/it]                                                 {'loss': 0.2467, 'learning_rate': 9.058100251142483e-05, 'epoch': 0.54}
 54%|█████▍    | 215/395 [49:57<41:21, 13.79s/it] 55%|█████▍    | 216/395 [50:11<41:18, 13.85s/it]                                                 {'loss': 0.2781, 'learning_rate': 8.976471602918991e-05, 'epoch': 0.55}
 55%|█████▍    | 216/395 [50:11<41:18, 13.85s/it] 55%|█████▍    | 217/395 [50:25<41:06, 13.86s/it]                                                 {'loss': 0.2475, 'learning_rate': 8.894911819871026e-05, 'epoch': 0.55}
 55%|█████▍    | 217/395 [50:25<41:06, 13.86s/it] 55%|█████▌    | 218/395 [50:39<40:40, 13.79s/it]                                                 {'loss': 0.3147, 'learning_rate': 8.813426389514903e-05, 'epoch': 0.55}
 55%|█████▌    | 218/395 [50:39<40:40, 13.79s/it] 55%|█████▌    | 219/395 [50:53<40:29, 13.80s/it]                                                 {'loss': 0.2601, 'learning_rate': 8.732020794364326e-05, 'epoch': 0.55}
 55%|█████▌    | 219/395 [50:53<40:29, 13.80s/it] 56%|█████▌    | 220/395 [51:06<39:50, 13.66s/it]                                                 {'loss': 0.4416, 'learning_rate': 8.650700511561514e-05, 'epoch': 0.56}
 56%|█████▌    | 220/395 [51:06<39:50, 13.66s/it] 56%|█████▌    | 221/395 [51:20<39:49, 13.73s/it]                                                 {'loss': 0.0988, 'learning_rate': 8.5694710125087e-05, 'epoch': 0.56}
 56%|█████▌    | 221/395 [51:20<39:49, 13.73s/it] 56%|█████▌    | 222/395 [51:34<39:51, 13.83s/it]                                                 {'loss': 0.6552, 'learning_rate': 8.488337762499972e-05, 'epoch': 0.56}
 56%|█████▌    | 222/395 [51:34<39:51, 13.83s/it] 56%|█████▋    | 223/395 [51:48<39:49, 13.89s/it]                                                 {'loss': 0.3621, 'learning_rate': 8.407306220353596e-05, 'epoch': 0.56}
 56%|█████▋    | 223/395 [51:48<39:49, 13.89s/it] 57%|█████▋    | 224/395 [52:02<39:30, 13.86s/it]                                                 {'loss': 0.1976, 'learning_rate': 8.326381838044713e-05, 'epoch': 0.57}
 57%|█████▋    | 224/395 [52:02<39:30, 13.86s/it] 57%|█████▋    | 225/395 [52:16<39:21, 13.89s/it]                                                 {'loss': 0.2765, 'learning_rate': 8.245570060338512e-05, 'epoch': 0.57}
 57%|█████▋    | 225/395 [52:16<39:21, 13.89s/it] 57%|█████▋    | 226/395 [52:30<39:09, 13.90s/it]                                                 {'loss': 0.2153, 'learning_rate': 8.164876324423909e-05, 'epoch': 0.57}
 57%|█████▋    | 226/395 [52:30<39:09, 13.90s/it] 57%|█████▋    | 227/395 [52:43<38:48, 13.86s/it]                                                 {'loss': 0.2774, 'learning_rate': 8.084306059547722e-05, 'epoch': 0.57}
 57%|█████▋    | 227/395 [52:43<38:48, 13.86s/it] 58%|█████▊    | 228/395 [52:57<38:30, 13.84s/it]                                                 {'loss': 0.5711, 'learning_rate': 8.003864686649366e-05, 'epoch': 0.58}
 58%|█████▊    | 228/395 [52:57<38:30, 13.84s/it] 58%|█████▊    | 229/395 [53:11<38:17, 13.84s/it]                                                 {'loss': 0.2523, 'learning_rate': 7.923557617996131e-05, 'epoch': 0.58}
 58%|█████▊    | 229/395 [53:11<38:17, 13.84s/it] 58%|█████▊    | 230/395 [53:25<38:16, 13.92s/it]                                                 {'loss': 0.2907, 'learning_rate': 7.843390256819034e-05, 'epoch': 0.58}
 58%|█████▊    | 230/395 [53:25<38:16, 13.92s/it] 58%|█████▊    | 231/395 [53:39<37:37, 13.76s/it]                                                 {'loss': 0.4757, 'learning_rate': 7.763367996949267e-05, 'epoch': 0.58}
 58%|█████▊    | 231/395 [53:39<37:37, 13.76s/it] 59%|█████▊    | 232/395 [53:53<37:32, 13.82s/it]                                                 {'loss': 0.3018, 'learning_rate': 7.683496222455304e-05, 'epoch': 0.59}
 59%|█████▊    | 232/395 [53:53<37:32, 13.82s/it] 59%|█████▉    | 233/395 [54:06<37:17, 13.81s/it]                                                 {'loss': 0.2565, 'learning_rate': 7.603780307280639e-05, 'epoch': 0.59}
 59%|█████▉    | 233/395 [54:06<37:17, 13.81s/it] 59%|█████▉    | 234/395 [54:20<37:00, 13.79s/it]                                                 {'loss': 0.281, 'learning_rate': 7.524225614882216e-05, 'epoch': 0.59}
 59%|█████▉    | 234/395 [54:20<37:00, 13.79s/it] 59%|█████▉    | 235/395 [54:34<36:47, 13.80s/it]                                                 {'loss': 0.4243, 'learning_rate': 7.44483749786957e-05, 'epoch': 0.59}
 59%|█████▉    | 235/395 [54:34<36:47, 13.80s/it] 60%|█████▉    | 236/395 [54:48<36:38, 13.83s/it]                                                 {'loss': 0.4516, 'learning_rate': 7.365621297644686e-05, 'epoch': 0.6}
 60%|█████▉    | 236/395 [54:48<36:38, 13.83s/it] 60%|██████    | 237/395 [55:02<36:22, 13.81s/it]                                                 {'loss': 0.1326, 'learning_rate': 7.286582344042625e-05, 'epoch': 0.6}
 60%|██████    | 237/395 [55:02<36:22, 13.81s/it] 60%|██████    | 238/395 [55:16<36:13, 13.85s/it]                                                 {'loss': 0.4655, 'learning_rate': 7.207725954972913e-05, 'epoch': 0.6}
 60%|██████    | 238/395 [55:16<36:13, 13.85s/it] 61%|██████    | 239/395 [55:29<35:36, 13.69s/it]                                                 {'loss': 0.4155, 'learning_rate': 7.129057436061739e-05, 'epoch': 0.61}
 61%|██████    | 239/395 [55:29<35:36, 13.69s/it] 61%|██████    | 240/395 [55:42<35:01, 13.56s/it]                                                 {'loss': 0.2397, 'learning_rate': 7.050582080294996e-05, 'epoch': 0.61}
 61%|██████    | 240/395 [55:42<35:01, 13.56s/it] 61%|██████    | 241/395 [55:56<35:04, 13.67s/it]                                                 {'loss': 0.4249, 'learning_rate': 6.972305167662145e-05, 'epoch': 0.61}
 61%|██████    | 241/395 [55:56<35:04, 13.67s/it] 61%|██████▏   | 242/395 [56:09<34:28, 13.52s/it]                                                 {'loss': 0.3755, 'learning_rate': 6.89423196480097e-05, 'epoch': 0.61}
 61%|██████▏   | 242/395 [56:09<34:28, 13.52s/it] 62%|██████▏   | 243/395 [56:23<34:35, 13.65s/it]                                                 {'loss': 0.3106, 'learning_rate': 6.816367724643224e-05, 'epoch': 0.62}
 62%|██████▏   | 243/395 [56:23<34:35, 13.65s/it] 62%|██████▏   | 244/395 [56:37<34:25, 13.68s/it]                                                 {'loss': 0.2718, 'learning_rate': 6.738717686061206e-05, 'epoch': 0.62}
 62%|██████▏   | 244/395 [56:37<34:25, 13.68s/it] 62%|██████▏   | 245/395 [56:51<34:29, 13.79s/it]                                                 {'loss': 0.1065, 'learning_rate': 6.661287073515275e-05, 'epoch': 0.62}
 62%|██████▏   | 245/395 [56:51<34:29, 13.79s/it] 62%|██████▏   | 246/395 [57:04<33:56, 13.67s/it]                                                 {'loss': 0.3413, 'learning_rate': 6.58408109670234e-05, 'epoch': 0.62}
 62%|██████▏   | 246/395 [57:04<33:56, 13.67s/it] 63%|██████▎   | 247/395 [57:18<33:57, 13.77s/it]                                                 {'loss': 0.5235, 'learning_rate': 6.507104950205336e-05, 'epoch': 0.63}
 63%|██████▎   | 247/395 [57:18<33:57, 13.77s/it] 63%|██████▎   | 248/395 [57:32<33:50, 13.81s/it]                                                 {'loss': 0.1899, 'learning_rate': 6.430363813143716e-05, 'epoch': 0.63}
 63%|██████▎   | 248/395 [57:32<33:50, 13.81s/it] 63%|██████▎   | 249/395 [57:46<33:43, 13.86s/it]                                                 {'loss': 0.4667, 'learning_rate': 6.35386284882501e-05, 'epoch': 0.63}
 63%|██████▎   | 249/395 [57:46<33:43, 13.86s/it] 63%|██████▎   | 250/395 [58:00<33:26, 13.84s/it]                                                 {'loss': 0.1959, 'learning_rate': 6.277607204397408e-05, 'epoch': 0.63}
 63%|██████▎   | 250/395 [58:00<33:26, 13.84s/it] 64%|██████▎   | 251/395 [58:14<33:17, 13.87s/it]                                                 {'loss': 0.2399, 'learning_rate': 6.201602010503454e-05, 'epoch': 0.64}
 64%|██████▎   | 251/395 [58:14<33:17, 13.87s/it] 64%|██████▍   | 252/395 [58:28<33:10, 13.92s/it]                                                 {'loss': 0.3223, 'learning_rate': 6.125852380934841e-05, 'epoch': 0.64}
 64%|██████▍   | 252/395 [58:28<33:10, 13.92s/it] 64%|██████▍   | 253/395 [58:42<32:59, 13.94s/it]                                                 {'loss': 0.3553, 'learning_rate': 6.0503634122883556e-05, 'epoch': 0.64}
 64%|██████▍   | 253/395 [58:42<32:59, 13.94s/it] 64%|██████▍   | 254/395 [58:55<32:22, 13.78s/it]                                                 {'loss': 0.352, 'learning_rate': 5.975140183622958e-05, 'epoch': 0.64}
 64%|██████▍   | 254/395 [58:55<32:22, 13.78s/it] 65%|██████▍   | 255/395 [59:09<32:04, 13.74s/it]                                                 {'loss': 0.4715, 'learning_rate': 5.900187756118055e-05, 'epoch': 0.65}
 65%|██████▍   | 255/395 [59:09<32:04, 13.74s/it] 65%|██████▍   | 256/395 [59:23<31:52, 13.76s/it]                                                 {'loss': 0.5889, 'learning_rate': 5.8255111727329717e-05, 'epoch': 0.65}
 65%|██████▍   | 256/395 [59:23<31:52, 13.76s/it] 65%|██████▌   | 257/395 [59:37<31:50, 13.84s/it]                                                 {'loss': 0.2911, 'learning_rate': 5.751115457867653e-05, 'epoch': 0.65}
 65%|██████▌   | 257/395 [59:37<31:50, 13.84s/it] 65%|██████▌   | 258/395 [59:51<31:36, 13.85s/it]                                                 {'loss': 0.349, 'learning_rate': 5.6770056170246175e-05, 'epoch': 0.65}
 65%|██████▌   | 258/395 [59:51<31:36, 13.85s/it] 66%|██████▌   | 259/395 [1:00:04<31:06, 13.72s/it]                                                   {'loss': 0.3257, 'learning_rate': 5.6031866364721554e-05, 'epoch': 0.66}
 66%|██████▌   | 259/395 [1:00:04<31:06, 13.72s/it] 66%|██████▌   | 260/395 [1:00:18<31:02, 13.80s/it]                                                   {'loss': 0.1758, 'learning_rate': 5.529663482908864e-05, 'epoch': 0.66}
 66%|██████▌   | 260/395 [1:00:18<31:02, 13.80s/it] 66%|██████▌   | 261/395 [1:00:32<30:52, 13.82s/it]                                                   {'loss': 0.4548, 'learning_rate': 5.4564411031294695e-05, 'epoch': 0.66}
 66%|██████▌   | 261/395 [1:00:32<30:52, 13.82s/it] 66%|██████▋   | 262/395 [1:00:46<30:42, 13.85s/it]                                                   {'loss': 0.4049, 'learning_rate': 5.383524423691994e-05, 'epoch': 0.66}
 66%|██████▋   | 262/395 [1:00:46<30:42, 13.85s/it] 67%|██████▋   | 263/395 [1:00:59<30:13, 13.74s/it]                                                   {'loss': 0.4527, 'learning_rate': 5.310918350586291e-05, 'epoch': 0.67}
 67%|██████▋   | 263/395 [1:00:59<30:13, 13.74s/it] 67%|██████▋   | 264/395 [1:01:13<29:50, 13.67s/it]                                                   {'loss': 0.5403, 'learning_rate': 5.2386277689039565e-05, 'epoch': 0.67}
 67%|██████▋   | 264/395 [1:01:13<29:50, 13.67s/it] 67%|██████▋   | 265/395 [1:01:26<29:26, 13.59s/it]                                                   {'loss': 0.1573, 'learning_rate': 5.1666575425096396e-05, 'epoch': 0.67}
 67%|██████▋   | 265/395 [1:01:26<29:26, 13.59s/it] 67%|██████▋   | 266/395 [1:01:40<29:08, 13.55s/it]                                                   {'loss': 0.7309, 'learning_rate': 5.095012513713815e-05, 'epoch': 0.67}
 67%|██████▋   | 266/395 [1:01:40<29:08, 13.55s/it] 68%|██████▊   | 267/395 [1:01:54<29:05, 13.64s/it]                                                   {'loss': 0.1423, 'learning_rate': 5.023697502946969e-05, 'epoch': 0.68}
 68%|██████▊   | 267/395 [1:01:54<29:05, 13.64s/it] 68%|██████▊   | 268/395 [1:02:07<28:55, 13.67s/it]                                                   {'loss': 0.7206, 'learning_rate': 4.9527173084352544e-05, 'epoch': 0.68}
 68%|██████▊   | 268/395 [1:02:07<28:55, 13.67s/it] 68%|██████▊   | 269/395 [1:02:21<28:50, 13.73s/it]                                                   {'loss': 0.4384, 'learning_rate': 4.882076705877689e-05, 'epoch': 0.68}
 68%|██████▊   | 269/395 [1:02:21<28:50, 13.73s/it] 68%|██████▊   | 270/395 [1:02:35<28:40, 13.76s/it]                                                   {'loss': 0.252, 'learning_rate': 4.811780448124812e-05, 'epoch': 0.68}
 68%|██████▊   | 270/395 [1:02:35<28:40, 13.76s/it] 69%|██████▊   | 271/395 [1:02:49<28:32, 13.81s/it]                                                   {'loss': 0.0514, 'learning_rate': 4.741833264858909e-05, 'epoch': 0.69}
 69%|██████▊   | 271/395 [1:02:49<28:32, 13.81s/it] 69%|██████▉   | 272/395 [1:03:03<28:24, 13.86s/it]                                                   {'loss': 0.3713, 'learning_rate': 4.6722398622757935e-05, 'epoch': 0.69}
 69%|██████▉   | 272/395 [1:03:03<28:24, 13.86s/it] 69%|██████▉   | 273/395 [1:03:17<28:01, 13.79s/it]                                                   {'loss': 0.0981, 'learning_rate': 4.603004922768148e-05, 'epoch': 0.69}
 69%|██████▉   | 273/395 [1:03:17<28:01, 13.79s/it] 69%|██████▉   | 274/395 [1:03:30<27:47, 13.78s/it]                                                   {'loss': 0.3888, 'learning_rate': 4.5341331046105064e-05, 'epoch': 0.69}
 69%|██████▉   | 274/395 [1:03:30<27:47, 13.78s/it] 70%|██████▉   | 275/395 [1:03:44<27:38, 13.82s/it]                                                   {'loss': 0.4176, 'learning_rate': 4.465629041645819e-05, 'epoch': 0.7}
 70%|██████▉   | 275/395 [1:03:44<27:38, 13.82s/it] 70%|██████▉   | 276/395 [1:03:58<27:25, 13.83s/it]                                                   {'loss': 0.1225, 'learning_rate': 4.397497342973676e-05, 'epoch': 0.7}
 70%|██████▉   | 276/395 [1:03:58<27:25, 13.83s/it] 70%|███████   | 277/395 [1:04:12<27:16, 13.87s/it]                                                   {'loss': 0.2787, 'learning_rate': 4.3297425926402115e-05, 'epoch': 0.7}
 70%|███████   | 277/395 [1:04:12<27:16, 13.87s/it] 70%|███████   | 278/395 [1:04:25<26:43, 13.71s/it]                                                   {'loss': 0.4937, 'learning_rate': 4.2623693493296644e-05, 'epoch': 0.7}
 70%|███████   | 278/395 [1:04:25<26:43, 13.71s/it] 71%|███████   | 279/395 [1:04:39<26:43, 13.82s/it]                                                   {'loss': 0.2499, 'learning_rate': 4.1953821460576715e-05, 'epoch': 0.71}
 71%|███████   | 279/395 [1:04:39<26:43, 13.82s/it] 71%|███████   | 280/395 [1:04:53<26:29, 13.82s/it]                                                   {'loss': 0.6381, 'learning_rate': 4.1287854898662705e-05, 'epoch': 0.71}
 71%|███████   | 280/395 [1:04:53<26:29, 13.82s/it] 71%|███████   | 281/395 [1:05:07<26:01, 13.69s/it]                                                   {'loss': 0.4001, 'learning_rate': 4.0625838615206566e-05, 'epoch': 0.71}
 71%|███████   | 281/395 [1:05:07<26:01, 13.69s/it] 71%|███████▏  | 282/395 [1:05:20<25:47, 13.70s/it]                                                   {'loss': 0.2988, 'learning_rate': 3.996781715207706e-05, 'epoch': 0.71}
 71%|███████▏  | 282/395 [1:05:20<25:47, 13.70s/it] 72%|███████▏  | 283/395 [1:05:34<25:45, 13.80s/it]                                                   {'loss': 0.1646, 'learning_rate': 3.9313834782362926e-05, 'epoch': 0.72}
 72%|███████▏  | 283/395 [1:05:34<25:45, 13.80s/it] 72%|███████▏  | 284/395 [1:05:48<25:34, 13.82s/it]                                                   {'loss': 0.3011, 'learning_rate': 3.866393550739416e-05, 'epoch': 0.72}
 72%|███████▏  | 284/395 [1:05:48<25:34, 13.82s/it] 72%|███████▏  | 285/395 [1:06:02<25:23, 13.85s/it]                                                   {'loss': 0.2838, 'learning_rate': 3.801816305378124e-05, 'epoch': 0.72}
 72%|███████▏  | 285/395 [1:06:02<25:23, 13.85s/it] 72%|███████▏  | 286/395 [1:06:16<25:15, 13.90s/it]                                                   {'loss': 0.4607, 'learning_rate': 3.737656087047347e-05, 'epoch': 0.72}
 72%|███████▏  | 286/395 [1:06:16<25:15, 13.90s/it] 73%|███████▎  | 287/395 [1:06:30<24:55, 13.84s/it]                                                   {'loss': 0.5544, 'learning_rate': 3.673917212583538e-05, 'epoch': 0.73}
 73%|███████▎  | 287/395 [1:06:30<24:55, 13.84s/it] 73%|███████▎  | 288/395 [1:06:44<24:59, 14.01s/it]                                                   {'loss': 0.3138, 'learning_rate': 3.610603970474239e-05, 'epoch': 0.73}
 73%|███████▎  | 288/395 [1:06:44<24:59, 14.01s/it] 73%|███████▎  | 289/395 [1:06:58<24:42, 13.98s/it]                                                   {'loss': 0.3744, 'learning_rate': 3.547720620569539e-05, 'epoch': 0.73}
 73%|███████▎  | 289/395 [1:06:58<24:42, 13.98s/it] 73%|███████▎  | 290/395 [1:07:12<24:17, 13.88s/it]                                                   {'loss': 0.2551, 'learning_rate': 3.485271393795453e-05, 'epoch': 0.73}
 73%|███████▎  | 290/395 [1:07:12<24:17, 13.88s/it] 74%|███████▎  | 291/395 [1:07:26<23:57, 13.83s/it]                                                   {'loss': 0.2923, 'learning_rate': 3.4232604918692754e-05, 'epoch': 0.74}
 74%|███████▎  | 291/395 [1:07:26<23:57, 13.83s/it] 74%|███████▍  | 292/395 [1:07:40<23:47, 13.86s/it]                                                   {'loss': 0.6597, 'learning_rate': 3.361692087016863e-05, 'epoch': 0.74}
 74%|███████▍  | 292/395 [1:07:40<23:47, 13.86s/it] 74%|███████▍  | 293/395 [1:07:54<23:36, 13.88s/it]                                                   {'loss': 0.3867, 'learning_rate': 3.300570321691934e-05, 'epoch': 0.74}
 74%|███████▍  | 293/395 [1:07:54<23:36, 13.88s/it] 74%|███████▍  | 294/395 [1:08:08<23:25, 13.92s/it]                                                   {'loss': 0.3127, 'learning_rate': 3.2398993082973294e-05, 'epoch': 0.74}
 74%|███████▍  | 294/395 [1:08:08<23:25, 13.92s/it] 75%|███████▍  | 295/395 [1:08:21<23:07, 13.87s/it]                                                   {'loss': 0.4239, 'learning_rate': 3.179683128908352e-05, 'epoch': 0.75}
 75%|███████▍  | 295/395 [1:08:21<23:07, 13.87s/it] 75%|███████▍  | 296/395 [1:08:34<22:31, 13.65s/it]                                                   {'loss': 0.2394, 'learning_rate': 3.1199258349980966e-05, 'epoch': 0.75}
 75%|███████▍  | 296/395 [1:08:34<22:31, 13.65s/it] 75%|███████▌  | 297/395 [1:08:48<22:26, 13.74s/it]                                                   {'loss': 0.4528, 'learning_rate': 3.0606314471648643e-05, 'epoch': 0.75}
 75%|███████▌  | 297/395 [1:08:48<22:26, 13.74s/it] 75%|███████▌  | 298/395 [1:09:02<21:58, 13.60s/it]                                                   {'loss': 0.2439, 'learning_rate': 3.0018039548616494e-05, 'epoch': 0.75}
 75%|███████▌  | 298/395 [1:09:02<21:58, 13.60s/it] 76%|███████▌  | 299/395 [1:09:16<21:57, 13.73s/it]                                                   {'loss': 0.3532, 'learning_rate': 2.943447316127712e-05, 'epoch': 0.76}
 76%|███████▌  | 299/395 [1:09:16<21:57, 13.73s/it] 76%|███████▌  | 300/395 [1:09:29<21:45, 13.74s/it]                                                   {'loss': 0.4947, 'learning_rate': 2.8855654573222825e-05, 'epoch': 0.76}
 76%|███████▌  | 300/395 [1:09:29<21:45, 13.74s/it] 76%|███████▌  | 301/395 [1:09:43<21:35, 13.78s/it]                                                   {'loss': 0.4828, 'learning_rate': 2.8281622728603864e-05, 'epoch': 0.76}
 76%|███████▌  | 301/395 [1:09:43<21:35, 13.78s/it] 76%|███████▋  | 302/395 [1:09:57<21:09, 13.65s/it]                                                   {'loss': 0.7191, 'learning_rate': 2.7712416249508177e-05, 'epoch': 0.76}
 76%|███████▋  | 302/395 [1:09:57<21:09, 13.65s/it] 77%|███████▋  | 303/395 [1:10:10<20:57, 13.67s/it]                                                   {'loss': 0.5968, 'learning_rate': 2.714807343336273e-05, 'epoch': 0.77}
 77%|███████▋  | 303/395 [1:10:10<20:57, 13.67s/it] 77%|███████▋  | 304/395 [1:10:24<20:54, 13.78s/it]                                                   {'loss': 0.1656, 'learning_rate': 2.6588632250356948e-05, 'epoch': 0.77}
 77%|███████▋  | 304/395 [1:10:24<20:54, 13.78s/it] 77%|███████▋  | 305/395 [1:10:38<20:41, 13.79s/it]                                                   {'loss': 0.2119, 'learning_rate': 2.6034130340887895e-05, 'epoch': 0.77}
 77%|███████▋  | 305/395 [1:10:38<20:41, 13.79s/it] 77%|███████▋  | 306/395 [1:10:51<20:09, 13.59s/it]                                                   {'loss': 0.1985, 'learning_rate': 2.5484605013027784e-05, 'epoch': 0.77}
 77%|███████▋  | 306/395 [1:10:51<20:09, 13.59s/it] 78%|███████▊  | 307/395 [1:11:05<20:04, 13.69s/it]                                                   {'loss': 0.1733, 'learning_rate': 2.4940093240013717e-05, 'epoch': 0.78}
 78%|███████▊  | 307/395 [1:11:05<20:04, 13.69s/it] 78%|███████▊  | 308/395 [1:11:19<19:57, 13.77s/it]                                                   {'loss': 0.2365, 'learning_rate': 2.4400631657760186e-05, 'epoch': 0.78}
 78%|███████▊  | 308/395 [1:11:19<19:57, 13.77s/it] 78%|███████▊  | 309/395 [1:11:33<19:44, 13.77s/it]                                                   {'loss': 0.3288, 'learning_rate': 2.3866256562394083e-05, 'epoch': 0.78}
 78%|███████▊  | 309/395 [1:11:33<19:44, 13.77s/it] 78%|███████▊  | 310/395 [1:11:47<19:35, 13.83s/it]                                                   {'loss': 0.6802, 'learning_rate': 2.333700390781256e-05, 'epoch': 0.78}
 78%|███████▊  | 310/395 [1:11:47<19:35, 13.83s/it] 79%|███████▊  | 311/395 [1:12:01<19:20, 13.81s/it]                                                   {'loss': 0.394, 'learning_rate': 2.2812909303264085e-05, 'epoch': 0.79}
 79%|███████▊  | 311/395 [1:12:01<19:20, 13.81s/it] 79%|███████▉  | 312/395 [1:12:15<19:11, 13.87s/it]                                                   {'loss': 0.3816, 'learning_rate': 2.2294008010952382e-05, 'epoch': 0.79}
 79%|███████▉  | 312/395 [1:12:15<19:11, 13.87s/it] 79%|███████▉  | 313/395 [1:12:29<18:57, 13.87s/it]                                                   {'loss': 0.4679, 'learning_rate': 2.1780334943664162e-05, 'epoch': 0.79}
 79%|███████▉  | 313/395 [1:12:29<18:57, 13.87s/it] 79%|███████▉  | 314/395 [1:12:42<18:42, 13.86s/it]                                                   {'loss': 0.342, 'learning_rate': 2.127192466241994e-05, 'epoch': 0.79}
 79%|███████▉  | 314/395 [1:12:42<18:42, 13.86s/it] 80%|███████▉  | 315/395 [1:12:56<18:28, 13.86s/it]                                                   {'loss': 0.4927, 'learning_rate': 2.07688113741488e-05, 'epoch': 0.8}
 80%|███████▉  | 315/395 [1:12:56<18:28, 13.86s/it] 80%|████████  | 316/395 [1:13:10<18:14, 13.86s/it]                                                   {'loss': 0.5108, 'learning_rate': 2.0271028929386738e-05, 'epoch': 0.8}
 80%|████████  | 316/395 [1:13:10<18:14, 13.86s/it] 80%|████████  | 317/395 [1:13:24<18:01, 13.86s/it]                                                   {'loss': 0.167, 'learning_rate': 1.977861081999931e-05, 'epoch': 0.8}
 80%|████████  | 317/395 [1:13:24<18:01, 13.86s/it] 81%|████████  | 318/395 [1:13:38<17:49, 13.90s/it]                                                   {'loss': 0.1874, 'learning_rate': 1.92915901769281e-05, 'epoch': 0.81}
 81%|████████  | 318/395 [1:13:38<17:49, 13.90s/it] 81%|████████  | 319/395 [1:13:52<17:34, 13.88s/it]                                                   {'loss': 0.3601, 'learning_rate': 1.880999976796164e-05, 'epoch': 0.81}
 81%|████████  | 319/395 [1:13:52<17:34, 13.88s/it] 81%|████████  | 320/395 [1:14:05<17:06, 13.69s/it]                                                   {'loss': 0.5923, 'learning_rate': 1.8333871995530726e-05, 'epoch': 0.81}
 81%|████████  | 320/395 [1:14:05<17:06, 13.69s/it] 81%|████████▏ | 321/395 [1:14:19<16:56, 13.74s/it]                                                   {'loss': 0.2436, 'learning_rate': 1.786323889452828e-05, 'epoch': 0.81}
 81%|████████▏ | 321/395 [1:14:19<16:56, 13.74s/it] 82%|████████▏ | 322/395 [1:14:33<16:42, 13.73s/it]                                                   {'loss': 0.2429, 'learning_rate': 1.739813213015401e-05, 'epoch': 0.82}
 82%|████████▏ | 322/395 [1:14:33<16:42, 13.73s/it] 82%|████████▏ | 323/395 [1:14:47<16:32, 13.79s/it]                                                   {'loss': 0.285, 'learning_rate': 1.693858299578396e-05, 'epoch': 0.82}
 82%|████████▏ | 323/395 [1:14:47<16:32, 13.79s/it] 82%|████████▏ | 324/395 [1:15:01<16:22, 13.83s/it]                                                   {'loss': 0.3952, 'learning_rate': 1.6484622410864835e-05, 'epoch': 0.82}
 82%|████████▏ | 324/395 [1:15:01<16:22, 13.83s/it] 82%|████████▏ | 325/395 [1:15:14<15:56, 13.67s/it]                                                   {'loss': 0.2728, 'learning_rate': 1.6036280918833924e-05, 'epoch': 0.82}
 82%|████████▏ | 325/395 [1:15:14<15:56, 13.67s/it] 83%|████████▎ | 326/395 [1:15:27<15:41, 13.64s/it]                                                   {'loss': 0.206, 'learning_rate': 1.5593588685063896e-05, 'epoch': 0.83}
 83%|████████▎ | 326/395 [1:15:27<15:41, 13.64s/it] 83%|████████▎ | 327/395 [1:15:41<15:33, 13.73s/it]                                                   {'loss': 0.3964, 'learning_rate': 1.515657549483328e-05, 'epoch': 0.83}
 83%|████████▎ | 327/395 [1:15:41<15:33, 13.73s/it] 83%|████████▎ | 328/395 [1:15:55<15:23, 13.79s/it]                                                   {'loss': 0.1266, 'learning_rate': 1.4725270751322452e-05, 'epoch': 0.83}
 83%|████████▎ | 328/395 [1:15:55<15:23, 13.79s/it] 83%|████████▎ | 329/395 [1:16:09<15:08, 13.77s/it]                                                   {'loss': 0.3478, 'learning_rate': 1.4299703473635218e-05, 'epoch': 0.83}
 83%|████████▎ | 329/395 [1:16:09<15:08, 13.77s/it] 84%|████████▎ | 330/395 [1:16:23<14:55, 13.78s/it]                                                   {'loss': 0.1151, 'learning_rate': 1.3879902294846536e-05, 'epoch': 0.84}
 84%|████████▎ | 330/395 [1:16:23<14:55, 13.78s/it] 84%|████████▍ | 331/395 [1:16:37<14:46, 13.85s/it]                                                   {'loss': 0.1905, 'learning_rate': 1.3465895460075873e-05, 'epoch': 0.84}
 84%|████████▍ | 331/395 [1:16:37<14:46, 13.85s/it] 84%|████████▍ | 332/395 [1:16:51<14:31, 13.84s/it]                                                   {'loss': 0.3318, 'learning_rate': 1.3057710824586899e-05, 'epoch': 0.84}
 84%|████████▍ | 332/395 [1:16:51<14:31, 13.84s/it] 84%|████████▍ | 333/395 [1:17:04<14:18, 13.85s/it]                                                   {'loss': 0.47, 'learning_rate': 1.2655375851913232e-05, 'epoch': 0.84}
 84%|████████▍ | 333/395 [1:17:04<14:18, 13.85s/it] 85%|████████▍ | 334/395 [1:17:18<14:05, 13.86s/it]                                                   {'loss': 0.2726, 'learning_rate': 1.22589176120107e-05, 'epoch': 0.85}
 85%|████████▍ | 334/395 [1:17:18<14:05, 13.86s/it] 85%|████████▍ | 335/395 [1:17:32<13:51, 13.86s/it]                                                   {'loss': 0.3529, 'learning_rate': 1.186836277943606e-05, 'epoch': 0.85}
 85%|████████▍ | 335/395 [1:17:32<13:51, 13.86s/it] 85%|████████▌ | 336/395 [1:17:46<13:32, 13.77s/it]                                                   {'loss': 0.4217, 'learning_rate': 1.1483737631552161e-05, 'epoch': 0.85}
 85%|████████▌ | 336/395 [1:17:46<13:32, 13.77s/it] 85%|████████▌ | 337/395 [1:18:00<13:20, 13.80s/it]                                                   {'loss': 0.1238, 'learning_rate': 1.1105068046760048e-05, 'epoch': 0.85}
 85%|████████▌ | 337/395 [1:18:00<13:20, 13.80s/it] 86%|████████▌ | 338/395 [1:18:13<13:06, 13.79s/it]                                                   {'loss': 0.3557, 'learning_rate': 1.0732379502757717e-05, 'epoch': 0.86}
 86%|████████▌ | 338/395 [1:18:13<13:06, 13.79s/it] 86%|████████▌ | 339/395 [1:18:27<12:56, 13.87s/it]                                                   {'loss': 0.1761, 'learning_rate': 1.036569707482602e-05, 'epoch': 0.86}
 86%|████████▌ | 339/395 [1:18:27<12:56, 13.87s/it] 86%|████████▌ | 340/395 [1:18:41<12:40, 13.82s/it]                                                   {'loss': 0.6016, 'learning_rate': 1.0005045434141502e-05, 'epoch': 0.86}
 86%|████████▌ | 340/395 [1:18:41<12:40, 13.82s/it] 86%|████████▋ | 341/395 [1:18:55<12:30, 13.90s/it]                                                   {'loss': 0.4988, 'learning_rate': 9.6504488461164e-06, 'epoch': 0.86}
 86%|████████▋ | 341/395 [1:18:55<12:30, 13.90s/it] 87%|████████▋ | 342/395 [1:19:09<12:13, 13.83s/it]                                                   {'loss': 0.4, 'learning_rate': 9.301931168766164e-06, 'epoch': 0.87}
 87%|████████▋ | 342/395 [1:19:09<12:13, 13.83s/it] 87%|████████▋ | 343/395 [1:19:22<11:50, 13.67s/it]                                                   {'loss': 0.362, 'learning_rate': 8.959515851104117e-06, 'epoch': 0.87}
 87%|████████▋ | 343/395 [1:19:22<11:50, 13.67s/it] 87%|████████▋ | 344/395 [1:19:36<11:41, 13.75s/it]                                                   {'loss': 0.2592, 'learning_rate': 8.623225931563805e-06, 'epoch': 0.87}
 87%|████████▋ | 344/395 [1:19:36<11:41, 13.75s/it] 87%|████████▋ | 345/395 [1:19:50<11:29, 13.80s/it]                                                   {'loss': 0.1757, 'learning_rate': 8.293084036448895e-06, 'epoch': 0.87}
 87%|████████▋ | 345/395 [1:19:50<11:29, 13.80s/it] 88%|████████▊ | 346/395 [1:20:04<11:18, 13.85s/it]                                                   {'loss': 0.3019, 'learning_rate': 7.96911237841088e-06, 'epoch': 0.88}
 88%|████████▊ | 346/395 [1:20:04<11:18, 13.85s/it] 88%|████████▊ | 347/395 [1:20:18<11:05, 13.87s/it]                                                   {'loss': 0.2412, 'learning_rate': 7.651332754954477e-06, 'epoch': 0.88}
 88%|████████▊ | 347/395 [1:20:18<11:05, 13.87s/it] 88%|████████▊ | 348/395 [1:20:32<10:54, 13.93s/it]                                                   {'loss': 0.1788, 'learning_rate': 7.3397665469711495e-06, 'epoch': 0.88}
 88%|████████▊ | 348/395 [1:20:32<10:54, 13.93s/it] 88%|████████▊ | 349/395 [1:20:46<10:38, 13.87s/it]                                                   {'loss': 0.311, 'learning_rate': 7.034434717300509e-06, 'epoch': 0.88}
 88%|████████▊ | 349/395 [1:20:46<10:38, 13.87s/it] 89%|████████▊ | 350/395 [1:21:00<10:22, 13.84s/it]                                                   {'loss': 0.3597, 'learning_rate': 6.735357809319809e-06, 'epoch': 0.89}
 89%|████████▊ | 350/395 [1:21:00<10:22, 13.84s/it] 89%|████████▉ | 351/395 [1:21:13<10:05, 13.75s/it]                                                   {'loss': 0.5043, 'learning_rate': 6.442555945561901e-06, 'epoch': 0.89}
 89%|████████▉ | 351/395 [1:21:13<10:05, 13.75s/it] 89%|████████▉ | 352/395 [1:21:26<09:42, 13.54s/it]                                                   {'loss': 0.1731, 'learning_rate': 6.156048826361238e-06, 'epoch': 0.89}
 89%|████████▉ | 352/395 [1:21:26<09:42, 13.54s/it] 89%|████████▉ | 353/395 [1:21:39<09:25, 13.47s/it]                                                   {'loss': 0.3626, 'learning_rate': 5.8758557285284125e-06, 'epoch': 0.89}
 89%|████████▉ | 353/395 [1:21:39<09:25, 13.47s/it] 90%|████████▉ | 354/395 [1:21:53<09:18, 13.62s/it]                                                   {'loss': 0.1835, 'learning_rate': 5.6019955040531925e-06, 'epoch': 0.9}
 90%|████████▉ | 354/395 [1:21:53<09:18, 13.62s/it] 90%|████████▉ | 355/395 [1:22:07<09:02, 13.56s/it]                                                   {'loss': 0.1712, 'learning_rate': 5.334486578836118e-06, 'epoch': 0.9}
 90%|████████▉ | 355/395 [1:22:07<09:02, 13.56s/it] 90%|█████████ | 356/395 [1:22:21<08:51, 13.63s/it]                                                   {'loss': 0.5457, 'learning_rate': 5.073346951448699e-06, 'epoch': 0.9}
 90%|█████████ | 356/395 [1:22:21<08:51, 13.63s/it] 90%|█████████ | 357/395 [1:22:34<08:36, 13.60s/it]                                                   {'loss': 0.155, 'learning_rate': 4.818594191922576e-06, 'epoch': 0.9}
 90%|█████████ | 357/395 [1:22:34<08:36, 13.60s/it] 91%|█████████ | 358/395 [1:22:48<08:25, 13.66s/it]                                                   {'loss': 0.2825, 'learning_rate': 4.5702454405672e-06, 'epoch': 0.91}
 91%|█████████ | 358/395 [1:22:48<08:25, 13.66s/it] 91%|█████████ | 359/395 [1:23:02<08:13, 13.70s/it]                                                   {'loss': 0.1864, 'learning_rate': 4.328317406816751e-06, 'epoch': 0.91}
 91%|█████████ | 359/395 [1:23:02<08:13, 13.70s/it] 91%|█████████ | 360/395 [1:23:16<08:00, 13.72s/it]                                                   {'loss': 0.1939, 'learning_rate': 4.092826368105795e-06, 'epoch': 0.91}
 91%|█████████ | 360/395 [1:23:16<08:00, 13.72s/it] 91%|█████████▏| 361/395 [1:23:29<07:48, 13.79s/it]                                                   {'loss': 0.0616, 'learning_rate': 3.863788168774118e-06, 'epoch': 0.91}
 91%|█████████▏| 361/395 [1:23:29<07:48, 13.79s/it] 92%|█████████▏| 362/395 [1:23:43<07:36, 13.82s/it]                                                   {'loss': 0.2826, 'learning_rate': 3.6412182190007082e-06, 'epoch': 0.92}
 92%|█████████▏| 362/395 [1:23:43<07:36, 13.82s/it] 92%|█████████▏| 363/395 [1:23:57<07:22, 13.82s/it]                                                   {'loss': 0.4299, 'learning_rate': 3.425131493766931e-06, 'epoch': 0.92}
 92%|█████████▏| 363/395 [1:23:57<07:22, 13.82s/it] 92%|█████████▏| 364/395 [1:24:11<07:10, 13.89s/it]                                                   {'loss': 0.317, 'learning_rate': 3.2155425318489584e-06, 'epoch': 0.92}
 92%|█████████▏| 364/395 [1:24:11<07:10, 13.89s/it] 92%|█████████▏| 365/395 [1:24:24<06:49, 13.66s/it]                                                   {'loss': 0.3163, 'learning_rate': 3.012465434839529e-06, 'epoch': 0.92}
 92%|█████████▏| 365/395 [1:24:24<06:49, 13.66s/it] 93%|█████████▎| 366/395 [1:24:38<06:37, 13.71s/it]                                                   {'loss': 0.2601, 'learning_rate': 2.8159138661992824e-06, 'epoch': 0.93}
 93%|█████████▎| 366/395 [1:24:38<06:37, 13.71s/it] 93%|█████████▎| 367/395 [1:24:52<06:25, 13.77s/it]                                                   {'loss': 0.2326, 'learning_rate': 2.6259010503373206e-06, 'epoch': 0.93}
 93%|█████████▎| 367/395 [1:24:52<06:25, 13.77s/it] 93%|█████████▎| 368/395 [1:25:06<06:13, 13.82s/it]                                                   {'loss': 0.2092, 'learning_rate': 2.4424397717215387e-06, 'epoch': 0.93}
 93%|█████████▎| 368/395 [1:25:06<06:13, 13.82s/it] 93%|█████████▎| 369/395 [1:25:19<05:55, 13.66s/it]                                                   {'loss': 0.1292, 'learning_rate': 2.2655423740183924e-06, 'epoch': 0.93}
 93%|█████████▎| 369/395 [1:25:19<05:55, 13.66s/it] 94%|█████████▎| 370/395 [1:25:33<05:44, 13.77s/it]                                                   {'loss': 0.1802, 'learning_rate': 2.0952207592624505e-06, 'epoch': 0.94}
 94%|█████████▎| 370/395 [1:25:33<05:44, 13.77s/it] 94%|█████████▍| 371/395 [1:25:47<05:30, 13.78s/it]                                                   {'loss': 0.2201, 'learning_rate': 1.9314863870555255e-06, 'epoch': 0.94}
 94%|█████████▍| 371/395 [1:25:47<05:30, 13.78s/it] 94%|█████████▍| 372/395 [1:26:01<05:14, 13.67s/it]                                                   {'loss': 0.4669, 'learning_rate': 1.7743502737957106e-06, 'epoch': 0.94}
 94%|█████████▍| 372/395 [1:26:01<05:14, 13.67s/it] 94%|█████████▍| 373/395 [1:26:15<05:03, 13.78s/it]                                                   {'loss': 0.4238, 'learning_rate': 1.6238229919361858e-06, 'epoch': 0.94}
 94%|█████████▍| 373/395 [1:26:15<05:03, 13.78s/it] 95%|█████████▍| 374/395 [1:26:28<04:49, 13.79s/it]                                                   {'loss': 0.4379, 'learning_rate': 1.4799146692737741e-06, 'epoch': 0.95}
 95%|█████████▍| 374/395 [1:26:28<04:49, 13.79s/it] 95%|█████████▍| 375/395 [1:26:42<04:36, 13.82s/it]                                                   {'loss': 0.2118, 'learning_rate': 1.3426349882676325e-06, 'epoch': 0.95}
 95%|█████████▍| 375/395 [1:26:42<04:36, 13.82s/it] 95%|█████████▌| 376/395 [1:26:56<04:22, 13.83s/it]                                                   {'loss': 0.3356, 'learning_rate': 1.211993185387772e-06, 'epoch': 0.95}
 95%|█████████▌| 376/395 [1:26:56<04:22, 13.83s/it] 95%|█████████▌| 377/395 [1:27:10<04:07, 13.78s/it]                                                   {'loss': 0.7571, 'learning_rate': 1.0879980504935772e-06, 'epoch': 0.95}
 95%|█████████▌| 377/395 [1:27:10<04:07, 13.78s/it] 96%|█████████▌| 378/395 [1:27:24<03:54, 13.81s/it]                                                   {'loss': 0.4044, 'learning_rate': 9.706579262424131e-07, 'epoch': 0.96}
 96%|█████████▌| 378/395 [1:27:24<03:54, 13.81s/it] 96%|█████████▌| 379/395 [1:27:37<03:40, 13.79s/it]                                                   {'loss': 0.7837, 'learning_rate': 8.599807075283406e-07, 'epoch': 0.96}
 96%|█████████▌| 379/395 [1:27:37<03:40, 13.79s/it] 96%|█████████▌| 380/395 [1:27:51<03:27, 13.82s/it]                                                   {'loss': 0.564, 'learning_rate': 7.559738409508855e-07, 'epoch': 0.96}
 96%|█████████▌| 380/395 [1:27:51<03:27, 13.82s/it] 96%|█████████▋| 381/395 [1:28:05<03:13, 13.82s/it]                                                   {'loss': 0.3136, 'learning_rate': 6.586443243140838e-07, 'epoch': 0.96}
 96%|█████████▋| 381/395 [1:28:05<03:13, 13.82s/it] 97%|█████████▋| 382/395 [1:28:19<03:00, 13.88s/it]                                                   {'loss': 0.217, 'learning_rate': 5.679987061555703e-07, 'epoch': 0.97}
 97%|█████████▋| 382/395 [1:28:19<03:00, 13.88s/it] 97%|█████████▋| 383/395 [1:28:33<02:46, 13.87s/it]                                                   {'loss': 0.3296, 'learning_rate': 4.840430853060518e-07, 'epoch': 0.97}
 97%|█████████▋| 383/395 [1:28:33<02:46, 13.87s/it] 97%|█████████▋| 384/395 [1:28:47<02:31, 13.80s/it]                                                   {'loss': 0.214, 'learning_rate': 4.0678311047890327e-07, 'epoch': 0.97}
 97%|█████████▋| 384/395 [1:28:47<02:31, 13.80s/it] 97%|█████████▋| 385/395 [1:29:00<02:17, 13.74s/it]                                                   {'loss': 0.1154, 'learning_rate': 3.362239798901712e-07, 'epoch': 0.97}
 97%|█████████▋| 385/395 [1:29:00<02:17, 13.74s/it] 98%|█████████▊| 386/395 [1:29:14<02:03, 13.68s/it]                                                   {'loss': 0.6891, 'learning_rate': 2.723704409087979e-07, 'epoch': 0.98}
 98%|█████████▊| 386/395 [1:29:14<02:03, 13.68s/it] 98%|█████████▊| 387/395 [1:29:26<01:46, 13.33s/it]                                                   {'loss': 0.3782, 'learning_rate': 2.1522678973718847e-07, 'epoch': 0.98}
 98%|█████████▊| 387/395 [1:29:26<01:46, 13.33s/it] 98%|█████████▊| 388/395 [1:29:39<01:31, 13.08s/it]                                                   {'loss': 0.3378, 'learning_rate': 1.6479687112217479e-07, 'epoch': 0.98}
 98%|█████████▊| 388/395 [1:29:39<01:31, 13.08s/it] 98%|█████████▊| 389/395 [1:29:51<01:17, 12.93s/it]                                                   {'loss': 0.3145, 'learning_rate': 1.2108407809635624e-07, 'epoch': 0.98}
 98%|█████████▊| 389/395 [1:29:51<01:17, 12.93s/it] 99%|█████████▊| 390/395 [1:30:04<01:03, 12.74s/it]                                                   {'loss': 0.1591, 'learning_rate': 8.40913517497377e-08, 'epoch': 0.99}
 99%|█████████▊| 390/395 [1:30:04<01:03, 12.74s/it] 99%|█████████▉| 391/395 [1:30:16<00:50, 12.69s/it]                                                   {'loss': 0.3701, 'learning_rate': 5.382118103193223e-08, 'epoch': 0.99}
 99%|█████████▉| 391/395 [1:30:16<00:50, 12.69s/it] 99%|█████████▉| 392/395 [1:30:29<00:37, 12.65s/it]                                                   {'loss': 0.303, 'learning_rate': 3.027560258465067e-08, 'epoch': 0.99}
 99%|█████████▉| 392/395 [1:30:29<00:37, 12.65s/it] 99%|█████████▉| 393/395 [1:30:41<00:25, 12.62s/it]                                                   {'loss': 0.4319, 'learning_rate': 1.345620060465569e-08, 'epoch': 0.99}
 99%|█████████▉| 393/395 [1:30:41<00:25, 12.62s/it]100%|█████████▉| 394/395 [1:30:54<00:12, 12.61s/it]                                                   {'loss': 0.1972, 'learning_rate': 3.3641067372358612e-09, 'epoch': 1.0}
100%|█████████▉| 394/395 [1:30:54<00:12, 12.61s/it]100%|██████████| 395/395 [1:31:07<00:00, 12.71s/it]                                                   {'loss': 0.1923, 'learning_rate': 0.0, 'epoch': 1.0}
100%|██████████| 395/395 [1:31:07<00:00, 12.71s/it]                                                   {'train_runtime': 5473.3785, 'train_samples_per_second': 0.577, 'train_steps_per_second': 0.072, 'train_loss': 0.3632502247921274, 'epoch': 1.0}
100%|██████████| 395/395 [1:31:07<00:00, 12.71s/it]100%|██████████| 395/395 [1:31:07<00:00, 13.84s/it]
Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.
Non-default generation parameters: {'max_length': 4096}
[2024-07-10 12:42:18,541] [INFO] [launch.py:347:main] Process 1198746 exits successfully.
wandb: - 0.010 MB of 0.010 MB uploadedwandb: \ 0.010 MB of 0.010 MB uploadedwandb: | 0.010 MB of 0.051 MB uploadedwandb: / 0.053 MB of 0.053 MB uploadedwandb: 
wandb: Run history:
wandb:                    train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:              train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:            train/learning_rate ▃███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:                     train/loss █▅▃▃▅▃▂▄▃▂▂▂▃▃▁▃▃▂▁▄▃▄▃▃▂▃▂▃▅▅▄▃▄▃▄▂▂▄▄▃
wandb:               train/total_flos ▁
wandb:               train/train_loss ▁
wandb:            train/train_runtime ▁
wandb: train/train_samples_per_second ▁
wandb:   train/train_steps_per_second ▁
wandb: 
wandb: Run summary:
wandb:                    train/epoch 1.0
wandb:              train/global_step 395
wandb:            train/learning_rate 0.0
wandb:                     train/loss 0.1923
wandb:               train/total_flos 1206299615232.0
wandb:               train/train_loss 0.36325
wandb:            train/train_runtime 5473.3785
wandb: train/train_samples_per_second 0.577
wandb:   train/train_steps_per_second 0.072
wandb: 
wandb: 🚀 View run llava1.5_c_nola_floodnet at: https://wandb.ai/ankxanity19/CPT_VLM/runs/765vh334
wandb: ⭐️ View project at: https://wandb.ai/ankxanity19/CPT_VLM
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240710_111056-765vh334/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[2024-07-10 12:42:27,551] [INFO] [launch.py:347:main] Process 1198745 exits successfully.
