[2024-07-09 13:42:18,532] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-09 13:42:21,970] [WARNING] [runner.py:202:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
Detected CUDA_VISIBLE_DEVICES=0,1: setting --include=localhost:0,1
[2024-07-09 13:42:21,971] [INFO] [runner.py:571:main] cmd = /home/avaidya7/.conda/envs/llava/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMV19 --master_addr=127.0.0.1 --master_port=29500 --enable_each_rank_log=None /home/avaidya7/CPT_VLM/llava/train/train_mem.py --lora_enable True --nola_enable False --nola_num_basis 512 --lora_r 128 --lora_alpha 256 --mm_projector_lr 2e-5 --deepspeed /home/avaidya7/CPT_VLM/scripts/zero3.json --model_name_or_path /home/avaidya7/checkpoints/llava-v1.5-7b-tv100_lora --version v1 --data_path /home/avaidya7/FloodNet_train.json --image_folder /home/avaidya7/Sat_data/FloodNet/Images/Train_Image --vision_tower openai/clip-vit-large-patch14-336 --mm_projector_type mlp2x_gelu --mm_vision_select_layer -2 --mm_use_im_start_end False --mm_use_im_patch_token False --image_aspect_ratio pad --group_by_modality_length True --bf16 True --output_dir /home/avaidya7/checkpoints/llava-v1.5-7b-tv100_lora_floodnet --num_train_epochs 1 --per_device_train_batch_size 4 --per_device_eval_batch_size 2 --gradient_accumulation_steps 1 --evaluation_strategy no --save_strategy steps --save_steps 50000 --save_total_limit 1 --learning_rate 2e-4 --weight_decay 0. --warmup_ratio 0.03 --lr_scheduler_type cosine --logging_steps 1 --tf32 True --model_max_length 2048 --gradient_checkpointing True --dataloader_num_workers 4 --lazy_preprocess True --report_to wandb --run_name llava1.5_lora_tv100_floodnet
[2024-07-09 13:42:24,307] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-09 13:42:26,099] [INFO] [launch.py:145:main] WORLD INFO DICT: {'localhost': [0, 1]}
[2024-07-09 13:42:26,099] [INFO] [launch.py:151:main] nnodes=1, num_local_procs=2, node_rank=0
[2024-07-09 13:42:26,099] [INFO] [launch.py:162:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2024-07-09 13:42:26,099] [INFO] [launch.py:163:main] dist_world_size=2
[2024-07-09 13:42:26,099] [INFO] [launch.py:165:main] Setting CUDA_VISIBLE_DEVICES=0,1
[2024-07-09 13:42:30,713] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-09 13:42:30,713] [INFO] [real_accelerator.py:161:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-07-09 13:42:33,679] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-09 13:42:33,679] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-07-09 13:42:33,680] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
You are using a model of type llava to instantiate a model of type llava_llama. This is not supported for all configurations of models and can yield errors.
[2024-07-09 13:42:37,268] [INFO] [partition_parameters.py:348:__exit__] finished initializing model - num_params = 295, num_elems = 6.76B
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  return self.fget.__get__(instance, owner)()
Loading checkpoint shards:  50%|█████     | 1/2 [00:18<00:18, 18.87s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:20<00:20, 20.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 11.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.92s/it]
Traceback (most recent call last):
  File "/home/avaidya7/CPT_VLM/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation=None)
  File "/home/avaidya7/CPT_VLM/llava/train/train.py", line 935, in train
    model = LlavaLlamaForCausalLM.from_pretrained(
  File "/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3931, in from_pretrained
    model.load_adapter(
  File "/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/transformers/integrations/peft.py", line 180, in load_adapter
    peft_config = PeftConfig.from_pretrained(
  File "/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/peft/config.py", line 151, in from_pretrained
    return cls.from_peft_type(**kwargs)
  File "/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/peft/config.py", line 118, in from_peft_type
    return config_cls(**kwargs)
TypeError: LoraConfig.__init__() got an unexpected keyword argument 'nola_num_basis'
Loading checkpoint shards: 100%|██████████| 2/2 [00:26<00:00, 11.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:26<00:00, 13.15s/it]
Traceback (most recent call last):
  File "/home/avaidya7/CPT_VLM/llava/train/train_mem.py", line 4, in <module>
    train(attn_implementation=None)
  File "/home/avaidya7/CPT_VLM/llava/train/train.py", line 935, in train
    model = LlavaLlamaForCausalLM.from_pretrained(
  File "/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/transformers/modeling_utils.py", line 3931, in from_pretrained
    model.load_adapter(
  File "/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/transformers/integrations/peft.py", line 180, in load_adapter
    peft_config = PeftConfig.from_pretrained(
  File "/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/peft/config.py", line 151, in from_pretrained
    return cls.from_peft_type(**kwargs)
  File "/home/avaidya7/.conda/envs/llava/lib/python3.10/site-packages/peft/config.py", line 118, in from_peft_type
    return config_cls(**kwargs)
TypeError: LoraConfig.__init__() got an unexpected keyword argument 'nola_num_basis'
[2024-07-09 13:43:05,141] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 901366
[2024-07-09 13:43:05,141] [INFO] [launch.py:315:sigkill_handler] Killing subprocess 901367
[2024-07-09 13:43:05,178] [ERROR] [launch.py:321:sigkill_handler] ['/home/avaidya7/.conda/envs/llava/bin/python', '-u', '/home/avaidya7/CPT_VLM/llava/train/train_mem.py', '--local_rank=1', '--lora_enable', 'True', '--nola_enable', 'False', '--nola_num_basis', '512', '--lora_r', '128', '--lora_alpha', '256', '--mm_projector_lr', '2e-5', '--deepspeed', '/home/avaidya7/CPT_VLM/scripts/zero3.json', '--model_name_or_path', '/home/avaidya7/checkpoints/llava-v1.5-7b-tv100_lora', '--version', 'v1', '--data_path', '/home/avaidya7/FloodNet_train.json', '--image_folder', '/home/avaidya7/Sat_data/FloodNet/Images/Train_Image', '--vision_tower', 'openai/clip-vit-large-patch14-336', '--mm_projector_type', 'mlp2x_gelu', '--mm_vision_select_layer', '-2', '--mm_use_im_start_end', 'False', '--mm_use_im_patch_token', 'False', '--image_aspect_ratio', 'pad', '--group_by_modality_length', 'True', '--bf16', 'True', '--output_dir', '/home/avaidya7/checkpoints/llava-v1.5-7b-tv100_lora_floodnet', '--num_train_epochs', '1', '--per_device_train_batch_size', '4', '--per_device_eval_batch_size', '2', '--gradient_accumulation_steps', '1', '--evaluation_strategy', 'no', '--save_strategy', 'steps', '--save_steps', '50000', '--save_total_limit', '1', '--learning_rate', '2e-4', '--weight_decay', '0.', '--warmup_ratio', '0.03', '--lr_scheduler_type', 'cosine', '--logging_steps', '1', '--tf32', 'True', '--model_max_length', '2048', '--gradient_checkpointing', 'True', '--dataloader_num_workers', '4', '--lazy_preprocess', 'True', '--report_to', 'wandb', '--run_name', 'llava1.5_lora_tv100_floodnet'] exits with return code = 1
